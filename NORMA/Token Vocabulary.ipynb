{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>azərbaycan respublikası mülki məcəlləsinin təs...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>azərbaycan respublikasının mülki məcəlləsi təs...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>azərbaycan respublikasının mülki məcəlləsi qüv...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>azərbaycan respublikasının mülki məcəlləsinə d...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>azərbaycan ssr mülki mülki prosessual məcəlləl...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  unique_word_count\n",
       "0  azərbaycan respublikası mülki məcəlləsinin təs...                 13\n",
       "1  azərbaycan respublikasının mülki məcəlləsi təs...                  6\n",
       "2  azərbaycan respublikasının mülki məcəlləsi qüv...                 24\n",
       "3  azərbaycan respublikasının mülki məcəlləsinə d...                 35\n",
       "4  azərbaycan ssr mülki mülki prosessual məcəlləl...                 17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>unique_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500188</th>\n",
       "      <td>maddədə istehsal məişət tullantıları sözləri t...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500189</th>\n",
       "      <td>maddədə istehsal məişət tullantıları sözləri t...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500190</th>\n",
       "      <td>maddədə sənaye məişət tullantılarının sözləri ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500191</th>\n",
       "      <td>qiymətli metallar qiymətli daşlar azərbaycan r...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500192</th>\n",
       "      <td>yaşıllıqların mühafizəsi azərbaycan respublika...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  unique_word_count\n",
       "1500188  maddədə istehsal məişət tullantıları sözləri t...                  8\n",
       "1500189  maddədə istehsal məişət tullantıları sözləri t...                  9\n",
       "1500190  maddədə sənaye məişət tullantılarının sözləri ...                  9\n",
       "1500191  qiymətli metallar qiymətli daşlar azərbaycan r...                 19\n",
       "1500192  yaşıllıqların mühafizəsi azərbaycan respublika...                 18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500193 entries, 0 to 1500192\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count    Dtype \n",
      "---  ------             --------------    ----- \n",
      " 0   text               1500193 non-null  object\n",
      " 1   unique_word_count  1500193 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 22.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "967905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the core path\n",
    "PATH = 'E:/Software/Data Science and AI/NLP/Edliyye/Legal Acts Question Answering/NLP project'\n",
    "\n",
    "text_path = os.path.join(PATH, 'full_qanun_text_chunks.parquet')\n",
    "\n",
    "data = pd.read_parquet(text_path)\n",
    "\n",
    "display(data.head())\n",
    "display(data.tail())\n",
    "data.info()\n",
    "display(len(data['text'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'].loc[data['text'].str.contains('azərmənzilkomplektlayihətəmir')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'].iloc[15671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 235331\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate number of unique words\n",
    "def count_unique_words(text_series):\n",
    "    all_words = set()\n",
    "    for text in text_series:\n",
    "        words = text.lower().split()\n",
    "        all_words.update(words)\n",
    "    return len(all_words)\n",
    "\n",
    "# Calculate number of unique words\n",
    "unique_word_count = count_unique_words(data['text'])\n",
    "print(f\"Number of unique words: {unique_word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training complete. Tokenizer saved to 'tokenizer.json'. Temporary corpus file removed.\n"
     ]
    }
   ],
   "source": [
    "# Define a tokenizer model\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Customize normalizer (you may adjust this based on your needs)\n",
    "normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.normalizer = normalizer\n",
    "\n",
    "# Pre-tokenizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Set up the trainer with WordPiece\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=100_000,  # Adjusting vocab size\n",
    "    min_frequency=10,  # Setting min frequency\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# Write the corpus to a temporary file\n",
    "corpus_file = \"corpus.txt\"\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    for text in data['text']:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train([corpus_file], trainer)\n",
    "\n",
    "# Post-processing to add CLS and SEP tokens\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\"))\n",
    ")\n",
    "\n",
    "# Set a reasonable max length for the tokenizer before saving\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# Cleanup the temporary corpus file\n",
    "os.remove(corpus_file)\n",
    "\n",
    "print(\"Tokenizer training complete. Tokenizer saved to 'tokenizer.json'. Temporary corpus file removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing to Remove Redundant Tokens\n",
    "\n",
    "We can implement a filtering function to remove redundant subwords if the full word is present in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Helper function to filter out redundant subwords\n",
    "def remove_redundant_subwords(vocab):\n",
    "    # Create a set of full words\n",
    "    full_words = {token for token in vocab.keys() if \"##\" not in token}\n",
    "    \n",
    "    # Create a new vocab dictionary without redundant subwords\n",
    "    new_vocab = {token: index for token, index in vocab.items() if token in full_words or \"##\" not in token}\n",
    "    \n",
    "    return new_vocab\n",
    "\n",
    "# Filter the vocabulary\n",
    "filtered_vocab = remove_redundant_subwords(vocab)\n",
    "\n",
    "# Save the filtered vocabulary back to the tokenizer\n",
    "tokenizer.model.vocab = filtered_vocab\n",
    "tokenizer.save(\"filtered_tokenizer.json\")\n",
    "\n",
    "print(\"Filtered tokenizer saved to 'filtered_tokenizer.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Trained Tokenizer for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 348, 61, 147, 28009, 58456, 0, 2]\n",
      "Tokens: ['[CLS]', 'sən', '##in', '##lə', 'danısmaq', 'istəyirəm', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Load the trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "\n",
    "# Example usage\n",
    "text = \"Səninlə danışmaq istəyirəm.\"\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", encoded.ids)\n",
    "print(\"Tokens:\", encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azərbaycan ssr mülki mülki prosessual məcəllələrinin qüvvəyə minməsi əlaqədar azərbaycan ssr qanunvericilik aktlarının dəyişdirilməsi qüvvədən düşmüş hesab edilməsi azərbaycan ssr ali soveti rəyasət heyətinin mart tarixli fərmanı azərbaycan ssr ali sovetinin məlumatı'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 97, 3126, 604, 604, 2221, 44158, 698, 1933, 758, 97, 3126, 464, 1101, 3051, 3751, 3220, 298, 228, 97, 3126, 989, 5052, 6866, 2207, 1401, 242, 1136, 97, 3126, 989, 2485, 2274, 2]\n",
      "\n",
      "Tokens: ['[CLS]', 'azərbaycan', 'ssr', 'mulki', 'mulki', 'prosessual', 'məcəllələrinin', 'quvvəyə', 'minməsi', 'əlaqədar', 'azərbaycan', 'ssr', 'qanunvericilik', 'aktlarının', 'dəyisdirilməsi', 'quvvədən', 'dusmus', 'hesab', 'edilməsi', 'azərbaycan', 'ssr', 'ali', 'soveti', 'rəyasət', 'heyətinin', 'mart', 'tarixli', 'fərmanı', 'azərbaycan', 'ssr', 'ali', 'sovetinin', 'məlumatı', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = data['text'].iloc[5]\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", encoded.ids)\n",
    "print()\n",
    "print(\"Tokens:\", encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from the \"tokenizers\" library to a format compatible with the \"transformers\" library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the custom tokenizer to a format compatible with Hugging Face\n",
    "bert_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "# Save the converted tokenizer\n",
    "bert_tokenizer.save_pretrained('tokenizer_directory')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
