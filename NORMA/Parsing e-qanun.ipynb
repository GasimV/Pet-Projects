{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_parquet(r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\all_links.parquet\")\n",
    "filtered_links = links.loc[links['typeName'] == 'AZƏRBAYCAN RESPUBLİKASI PREZİDENTİNİN SƏRƏNCAMLARI']\n",
    "filtered_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to Process All Filtered Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'filtered_links' is a pandas DataFrame and the 'id' column holds the last number for the URL\n",
    "base_url = 'https://e-qanun.az/framework/'\n",
    "\n",
    "for idx, row in filtered_links.iterrows():\n",
    "    # Construct the URL dynamically from the 'id' column\n",
    "    url = f\"{base_url}{row['id']}\"\n",
    "    print(f\"Processing URL: {url}\")\n",
    "\n",
    "    # Define file names based on 'id'\n",
    "    file_name = row['id']\n",
    "    \n",
    "    # Define paths for HTML and parquet files\n",
    "    base_path = \"E:/Software/Data Science and AI/NLP/Edliyye/Legal Acts Question Answering/NLP project/e-qanun HTMLs/\"\n",
    "    sub_directory = \"AZƏRBAYCAN RESPUBLİKASI PREZİDENTİNİN SƏRƏNCAMLARI/\"\n",
    "    output_dir = os.path.join(base_path, sub_directory)\n",
    "\n",
    "    # Now use output_dir in your file paths\n",
    "    html_file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
    "    parquet_file_path = os.path.join(output_dir, f\"{file_name}.parquet\")\n",
    "\n",
    "    # Skip processing if both the HTML and parquet files already exist\n",
    "    if os.path.exists(html_file_path) and os.path.exists(parquet_file_path):\n",
    "        print(f\"Skipping {file_name}: Both HTML and parquet files already exist.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Initialize the webdriver and scrape the page dynamically using Selenium\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run browser in headless mode if you don't want to see the browser UI\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait explicitly for the content within any \"SectionX\" or \"WordSectionX\" class to load\n",
    "    try:\n",
    "        # Use regex to wait for any class starting with \"Section\" or \"WordSection\" followed by a number\n",
    "        element = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[contains(@class, 'Section') or contains(@class, 'WordSection')]\"))\n",
    "        )\n",
    "        print(\"Page content with 'SectionX' or 'WordSectionX' loaded successfully\")\n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time! Exiting...\")\n",
    "        driver.quit()\n",
    "        continue  # Skip to the next iteration if loading failed\n",
    "\n",
    "    # Get the fully rendered page source\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Save the HTML to the specified directory\n",
    "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    print(f\"HTML saved to: {html_file_path}\")\n",
    "\n",
    "    # Step 3: Load the saved HTML file and parse it using BeautifulSoup\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Step 4: Extract the document name ('Aktın Adı') from the title\n",
    "    document_name = soup.title.get_text(strip=True)\n",
    "\n",
    "    # Step 5: Extract the text sections within any \"SectionX\" or \"WordSectionX\" div, excluding tables and certain divs\n",
    "    word_sections = soup.find_all(lambda tag: (\n",
    "        tag.name == 'div' and\n",
    "        re.match(r'(WordSection|Section)\\d+', ' '.join(tag.get('class', [])))  # Match any class like SectionX or WordSectionX\n",
    "    ))\n",
    "\n",
    "    # If no matching sections are found, skip this page\n",
    "    if not word_sections:\n",
    "        print(f\"No 'WordSectionX' or 'SectionX' found for {file_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    paragraphs = []\n",
    "    \n",
    "    # Extract paragraphs from all matched sections\n",
    "    for word_section in word_sections:\n",
    "        paragraphs.extend(word_section.find_all(lambda tag: (\n",
    "            tag.name == 'p' and \n",
    "            ('MsoNormal' in tag.get('class', []) or 'Mecelle' in tag.get('class', []) or '21' in tag.get('class', [])) and\n",
    "            not tag.find_parent(['table']) and  # Exclude content inside any table\n",
    "            not tag.find_parent('table', class_='MsoTableTheme') and  # Exclude content inside MsoTableTheme\n",
    "            # Exclude the specific 'div' style\n",
    "            not tag.find_parent('div', style=lambda s: s and 'border-bottom:double gray' in s) and\n",
    "            not tag.find_parent('table', class_='MsoTableGrid')  # Exclude another table class\n",
    "        )))\n",
    "\n",
    "    \n",
    "    def clean_text(text):\n",
    "        # Replace non-breaking spaces with a regular space\n",
    "        text = text.replace(u'\\xa0', ' ')\n",
    "        # Use regex to replace multiple spaces (including non-breaking) with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    # Step 6: Extract the text (including those that might be empty or contain only whitespace) and clean it\n",
    "    sections = [clean_text(p.get_text(strip=True).replace('\\n', ' ')) for p in paragraphs]\n",
    "\n",
    "    # Step 7: Structure the data into a DataFrame with sections\n",
    "    data = {\n",
    "        'Aktın Adı': [document_name] * len(sections),  # Document name repeated for each section\n",
    "        'Mətn': sections,  # Each section as a separate row\n",
    "        'e-qanun reference': [url] * len(sections),  # Use the dynamic URL from Selenium\n",
    "        'Embeddings': ['[Empty]'] * len(sections)  # Placeholder for embeddings\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Remove rows where 'Mətn' column is empty, contains only whitespace, or is shorter than 33 characters\n",
    "    df['Mətn'] = df['Mətn'].astype(str)\n",
    "    df['Mətn'] = df['Mətn'].apply(clean_text)  # Apply the cleaning function to all text entries\n",
    "    df = df[df['Mətn'].str.strip().astype(bool)]  # Remove empty/whitespace entries\n",
    "    df = df[df['Mətn'].str.len() >= 33]  # Remove rows where 'Mətn' length is less than 33 characters\n",
    "\n",
    "    # Step 9: Reset the index after filtering\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['Mətn'])  # Drop duplicate rows based on the 'Mətn' column\n",
    "    df = df.reset_index(drop=True)  # Reset the index again after dropping duplicates\n",
    "\n",
    "    # Step 10: Save the DataFrame to a .parquet file, using the dynamically named file based on the HTML file name\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "    print(f\"Data saved to {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(row):\n",
    "    base_url = 'https://e-qanun.az/framework/'\n",
    "\n",
    "    # Construct the URL dynamically from the 'id' column\n",
    "    url = f\"{base_url}{row['id']}\"\n",
    "    print(f\"Processing URL: {url}\")\n",
    "\n",
    "    # Define file names based on 'id'\n",
    "    file_name = row['id']\n",
    "\n",
    "    # Define paths for HTML and parquet files\n",
    "    base_path = \"E:/Software/Data Science and AI/NLP/Edliyye/Legal Acts Question Answering/NLP project/e-qanun HTMLs/\"\n",
    "    sub_directory = \"AZƏRBAYCAN RESPUBLİKASI PREZİDENTİNİN SƏRƏNCAMLARI/\"\n",
    "    output_dir = os.path.join(base_path, sub_directory)\n",
    "\n",
    "    # Now use output_dir in your file paths\n",
    "    html_file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
    "    parquet_file_path = os.path.join(output_dir, f\"{file_name}.parquet\")\n",
    "\n",
    "    # Skip processing if both the HTML and parquet files already exist\n",
    "    if os.path.exists(html_file_path) and os.path.exists(parquet_file_path):\n",
    "        print(f\"Skipping {file_name}: Both HTML and parquet files already exist.\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Initialize the webdriver and scrape the page dynamically using Selenium\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run browser in headless mode if you don't want to see the browser UI\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait explicitly for the content within any \"SectionX\" or \"WordSectionX\" class to load\n",
    "    try:\n",
    "        # Use regex to wait for any class starting with \"Section\" or \"WordSection\" followed by a number\n",
    "        element = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[contains(@class, 'Section') or contains(@class, 'WordSection')]\"))\n",
    "        )\n",
    "        print(\"Page content with 'SectionX' or 'WordSectionX' loaded successfully\")\n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time! Exiting...\")\n",
    "        driver.quit()\n",
    "        return  # Skip to the next iteration if loading failed\n",
    "\n",
    "    # Get the fully rendered page source\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Save the HTML to the specified directory\n",
    "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    print(f\"HTML saved to: {html_file_path}\")\n",
    "\n",
    "    # Step 3: Load the saved HTML file and parse it using BeautifulSoup\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Step 4: Extract the document name ('Aktın Adı') from the title\n",
    "    document_name = soup.title.get_text(strip=True)\n",
    "\n",
    "    # Step 5: Extract the text sections within any \"SectionX\" or \"WordSectionX\" div, excluding tables and certain divs\n",
    "    word_sections = soup.find_all(lambda tag: (\n",
    "        tag.name == 'div' and\n",
    "        re.match(r'(WordSection|Section)\\d+', ' '.join(tag.get('class', [])))  # Match any class like SectionX or WordSectionX\n",
    "    ))\n",
    "\n",
    "    # If no matching sections are found, skip this page\n",
    "    if not word_sections:\n",
    "        print(f\"No 'WordSectionX' or 'SectionX' found for {file_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    paragraphs = []\n",
    "    \n",
    "    # Extract paragraphs from all matched sections\n",
    "    for word_section in word_sections:\n",
    "            paragraphs.extend(word_section.find_all(lambda tag: (\n",
    "                tag.name == 'p' and \n",
    "                ('MsoNormal' in tag.get('class', []) or 'Mecelle' in tag.get('class', []) or '21' in tag.get('class', [])) and\n",
    "                not tag.find_parent(['table']) and  # Exclude content inside any table\n",
    "                not tag.find_parent('table', class_='MsoTableTheme') and  # Exclude content inside MsoTableTheme\n",
    "                # Exclude the specific 'div' style\n",
    "                not tag.find_parent('div', style=lambda s: s and 'border-bottom:double gray' in s) and\n",
    "                not tag.find_parent('table', class_='MsoTableGrid')  # Exclude another table class\n",
    "            )))\n",
    "\n",
    "    def clean_text(text):\n",
    "        # Replace non-breaking spaces with a regular space\n",
    "        text = text.replace(u'\\xa0', ' ')\n",
    "        # Use regex to replace multiple spaces (including non-breaking) with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    # Step 6: Extract the text (including those that might be empty or contain only whitespace) and clean it\n",
    "    sections = [clean_text(p.get_text(strip=True).replace('\\n', ' ')) for p in paragraphs]\n",
    "\n",
    "    # Step 7: Structure the data into a DataFrame with sections\n",
    "    data = {\n",
    "        'Aktın Adı': [document_name] * len(sections),  # Document name repeated for each section\n",
    "        'Mətn': sections,  # Each section as a separate row\n",
    "        'e-qanun reference': [url] * len(sections),  # Use the dynamic URL from Selenium\n",
    "        'Embeddings': ['[Empty]'] * len(sections)  # Placeholder for embeddings\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Remove rows where 'Mətn' column is empty, contains only whitespace, or is shorter than 33 characters\n",
    "    df['Mətn'] = df['Mətn'].astype(str)\n",
    "    df['Mətn'] = df['Mətn'].apply(clean_text)  # Apply the cleaning function to all text entries\n",
    "    df = df[df['Mətn'].str.strip().astype(bool)]  # Remove empty/whitespace entries\n",
    "    df = df[df['Mətn'].str.len() >= 33]  # Remove rows where 'Mətn' length is less than 33 characters\n",
    "\n",
    "    # Step 9: Reset the index after filtering\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['Mətn'])  # Drop duplicate rows based on the 'Mətn' column\n",
    "    df = df.reset_index(drop=True)  # Reset the index again after dropping duplicates\n",
    "\n",
    "    # Step 10: Save the DataFrame to a .parquet file, using the dynamically named file based on the HTML file name\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "    print(f\"Data saved to {parquet_file_path}\")\n",
    "\n",
    "# Use joblib to parallelize the process, using 50% of available CPUs\n",
    "num_cores = multiprocessing.cpu_count() // 2\n",
    "Parallel(n_jobs=num_cores)(delayed(process_url)(row) for idx, row in filtered_links.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Empty Ones and Remove Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"E:/Software/Data Science and AI/NLP/Edliyye/Legal Acts Question Answering/NLP project/e-qanun HTMLs/\"\n",
    "sub_directory = \"AZƏRBAYCAN RESPUBLİKASI PREZİDENTİNİN SƏRƏNCAMLARI/\"\n",
    "output_dir = os.path.join(base_path, sub_directory)\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(output_dir)\n",
    "\n",
    "# Filter only the .parquet files\n",
    "parquet_files = [f for f in all_files if f.endswith('.parquet')]\n",
    "\n",
    "# Counter to track the number of empty files\n",
    "empty_file_count = 0\n",
    "\n",
    "# List to store the names of the files that will be removed (for one-time printing)\n",
    "files_to_remove = []\n",
    "\n",
    "# Iterate through the parquet files\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_path = os.path.join(output_dir, parquet_file)\n",
    "    \n",
    "    try:\n",
    "        # Check if the parquet file is empty by reading it into a DataFrame\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            # If the parquet file is empty, count it and prepare to remove the corresponding files\n",
    "            empty_file_count += 1\n",
    "            files_to_remove.append(parquet_file)\n",
    "            \n",
    "            # Remove the parquet file\n",
    "            os.remove(parquet_path)\n",
    "            \n",
    "            # Construct the corresponding HTML file path\n",
    "            html_file = parquet_file.replace('.parquet', '.html')\n",
    "            html_path = os.path.join(output_dir, html_file)\n",
    "            \n",
    "            # Check if the corresponding HTML file exists and delete it\n",
    "            if os.path.exists(html_path):\n",
    "                os.remove(html_path)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {parquet_file}: {e}\")\n",
    "\n",
    "# Print the number of empty files found and removed\n",
    "print(f\"Total number of empty parquet files found and removed: {empty_file_count}\")\n",
    "\n",
    "# Optional: Print the list of removed files if you want to keep track\n",
    "#print(f\"Files removed: {files_to_remove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Files removed: {files_to_remove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"57645.parquet\" # 3185 also check for images included\n",
    "\n",
    "# Combine the variables to create the full file path\n",
    "file_path = f\"{base_path}{sub_directory}{file_name}\"\n",
    "\n",
    "# Read the parquet file\n",
    "d = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the data and unique values\n",
    "display(d)\n",
    "display(d['Aktın Adı'].unique())\n",
    "display(d['Mətn'][0])\n",
    "display(d['Mətn'][1])\n",
    "display(d['Mətn'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(d['Mətn'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(d['Mətn'][63])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Above Script on Single Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the target URL id (add the target one SECOND in order, because the FIRST one doesn't work!)\n",
    "# AFTER PROCESSING REMOVE THE FIRST DUMMY ONE !!!\n",
    "filtered_links = pd.DataFrame({\n",
    "    'id': [3488, 3805]\n",
    "})\n",
    "\n",
    "# Assuming 'filtered_links' is a pandas DataFrame and the 'id' column holds the last number for the URL\n",
    "base_url = 'https://e-qanun.az/framework/'\n",
    "\n",
    "for idx, row in filtered_links.iterrows():\n",
    "    # Construct the URL dynamically from the 'id' column\n",
    "    url = f\"{base_url}{row['id']}\"\n",
    "    print(f\"Processing URL: {url}\")\n",
    "\n",
    "    # Define file names based on 'id'\n",
    "    file_name = row['id']\n",
    "    \n",
    "    # Define paths for HTML and parquet files\n",
    "    base_path = \"E:/Software/Data Science and AI/NLP/Edliyye/Legal Acts Question Answering/NLP project/e-qanun HTMLs/\"\n",
    "    sub_directory = \"AZƏRBAYCAN RESPUBLİKASI NAZİRLƏR KABİNETİNİN SƏRƏNCAMLARI/\"\n",
    "    output_dir = os.path.join(base_path, sub_directory)\n",
    "\n",
    "    # Now use output_dir in your file paths\n",
    "    html_file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
    "    parquet_file_path = os.path.join(output_dir, f\"{file_name}.parquet\")\n",
    "\n",
    "    # Skip processing if both the HTML and parquet files already exist\n",
    "    if os.path.exists(html_file_path) and os.path.exists(parquet_file_path):\n",
    "        print(f\"Skipping {file_name}: Both HTML and parquet files already exist.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Initialize the webdriver and scrape the page dynamically using Selenium\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run browser in headless mode if you don't want to see the browser UI\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait explicitly for the content within any \"SectionX\" or \"WordSectionX\" class to load\n",
    "    try:\n",
    "        # Use regex to wait for any class starting with \"Section\" or \"WordSection\" followed by a number\n",
    "        element = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[contains(@class, 'Section') or contains(@class, 'WordSection')]\"))\n",
    "        )\n",
    "        print(\"Page content with 'SectionX' or 'WordSectionX' loaded successfully\")\n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time! Exiting...\")\n",
    "        driver.quit()\n",
    "        continue  # Skip to the next iteration if loading failed\n",
    "\n",
    "    # Get the fully rendered page source\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Save the HTML to the specified directory\n",
    "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    print(f\"HTML saved to: {html_file_path}\")\n",
    "\n",
    "    # Step 3: Load the saved HTML file and parse it using BeautifulSoup\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Step 4: Extract the document name ('Aktın Adı') from the title\n",
    "    document_name = soup.title.get_text(strip=True)\n",
    "\n",
    "    # Step 5: Extract the text sections within any \"SectionX\" or \"WordSectionX\" div, excluding tables and certain divs\n",
    "    word_sections = soup.find_all(lambda tag: (\n",
    "        tag.name == 'div' and\n",
    "        re.match(r'(WordSection|Section)\\d+', ' '.join(tag.get('class', [])))  # Match any class like SectionX or WordSectionX\n",
    "    ))\n",
    "\n",
    "    # If no matching sections are found, skip this page\n",
    "    if not word_sections:\n",
    "        print(f\"No 'WordSectionX' or 'SectionX' found for {file_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    paragraphs = []\n",
    "    \n",
    "    # Extract paragraphs from all matched sections\n",
    "    for word_section in word_sections:\n",
    "        paragraphs.extend(word_section.find_all(lambda tag: (\n",
    "            tag.name == 'p' and \n",
    "            ('MsoNormal' in tag.get('class', []) or 'Mecelle' in tag.get('class', [])) and\n",
    "            not tag.find_parent(['table']) and  # Exclude content inside any table\n",
    "            not tag.find_parent('table', class_='MsoTableTheme') and  # Exclude content inside MsoTableTheme\n",
    "            # Exclude the specific 'div' style\n",
    "            not tag.find_parent('div', style=lambda s: s and 'border-bottom:double gray' in s) and\n",
    "            not tag.find_parent('table', class_='MsoTableGrid')  # Exclude another table class\n",
    "        )))\n",
    "\n",
    "    \n",
    "    def clean_text(text):\n",
    "        # Replace non-breaking spaces with a regular space\n",
    "        text = text.replace(u'\\xa0', ' ')\n",
    "        # Use regex to replace multiple spaces (including non-breaking) with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    # Step 6: Extract the text (including those that might be empty or contain only whitespace) and clean it\n",
    "    sections = [clean_text(p.get_text(strip=True).replace('\\n', ' ')) for p in paragraphs]\n",
    "\n",
    "    # Step 7: Structure the data into a DataFrame with sections\n",
    "    data = {\n",
    "        'Aktın Adı': [document_name] * len(sections),  # Document name repeated for each section\n",
    "        'Mətn': sections,  # Each section as a separate row\n",
    "        'e-qanun reference': [url] * len(sections),  # Use the dynamic URL from Selenium\n",
    "        'Embeddings': ['[Empty]'] * len(sections)  # Placeholder for embeddings\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Remove rows where 'Mətn' column is empty, contains only whitespace, or is shorter than 33 characters\n",
    "    df['Mətn'] = df['Mətn'].astype(str)\n",
    "    df['Mətn'] = df['Mətn'].apply(clean_text)  # Apply the cleaning function to all text entries\n",
    "    df = df[df['Mətn'].str.strip().astype(bool)]  # Remove empty/whitespace entries\n",
    "    df = df[df['Mətn'].str.len() >= 33]  # Remove rows where 'Mətn' length is less than 33 characters\n",
    "\n",
    "    # Step 9: Reset the index after filtering\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['Mətn'])  # Drop duplicate rows based on the 'Mətn' column\n",
    "    df = df.reset_index(drop=True)  # Reset the index again after dropping duplicates\n",
    "\n",
    "    # Step 10: Save the DataFrame to a .parquet file, using the dynamically named file based on the HTML file name\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "    print(f\"Data saved to {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"3805.parquet\" # 3185 also check for images included\n",
    "\n",
    "# Combine the variables to create the full file path\n",
    "file_path = f\"{base_path}{sub_directory}{file_name}\"\n",
    "\n",
    "# Read the parquet file\n",
    "d = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the data and unique values\n",
    "display(d)\n",
    "display(d['Aktın Adı'].unique())\n",
    "display(d['Mətn'][0])\n",
    "display(d['Mətn'][1])\n",
    "display(d['Mətn'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these parameters to filter for your specific document type\n",
    "start = 100\n",
    "codeType = 1  # Change this to the desired document type\n",
    "secondType = 2  # Change this for subcategory filtering if necessary\n",
    "\n",
    "# Initial URL with filtering parameters\n",
    "url = f\"https://api.e-qanun.az/getDetailSearch?start=0&length={100}&orderColumn=8&orderDirection=desc&title=true&codeType={codeType}&dateType=1&statusId=1&secondType={secondType}&specialDate=false&array=\"\n",
    "\n",
    "# Fetch the first set of data\n",
    "text = json.loads(requests.get(url).text)\n",
    "\n",
    "# Replace 'start' with the total number of documents to fetch all at once\n",
    "url = url.replace(str(start), str(text[\"totalCount\"]))\n",
    "text = json.loads(requests.get(url).text)\n",
    "\n",
    "# Normalize the data to a DataFrame and drop unnecessary columns\n",
    "data = pd.json_normalize(text[\"data\"]).drop(\n",
    "    [\"rowNum\", \"citation\", \"effectDate\",\n",
    "     \"registerCode\", \"registerDate\", \"htmlPath\",\n",
    "     \"fields\", \"relation\", \"classCodes\"], axis=1)\n",
    "\n",
    "# Define the output directory\n",
    "out_dir_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(out_dir_path):\n",
    "    os.makedirs(out_dir_path)\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "output_file = os.path.join(out_dir_path, \"all_links.parquet\")\n",
    "data.to_parquet(output_file, index=False)\n",
    "\n",
    "print(f\"Data successfully saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the webdriver\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Optional: Run browser in headless mode if you don't want to see the browser UI\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load your filtered DataFrame containing the 'id' column\n",
    "# Assume `filtered_links` is the DataFrame containing the document ids from your filtered results\n",
    "document_ids = filtered_links['id'].tolist()  # Extract the ids as a list\n",
    "# Print the total number of document ids to be processed\n",
    "print(f\"Number of documents to process: {len(document_ids)}\")\n",
    "\n",
    "# Function to process a document based on its id\n",
    "def process_document(doc_id):\n",
    "    # Check if both the HTML and Parquet files already exist, if so, skip this document\n",
    "    html_file_path = os.path.join(output_dir, f\"{doc_id}.html\")\n",
    "    parquet_file_path = os.path.join(output_dir, f\"{doc_id}.parquet\")\n",
    "    \n",
    "    if os.path.exists(html_file_path) and os.path.exists(parquet_file_path):\n",
    "        print(f\"Skipping {doc_id}, already processed.\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Generate the URL dynamically\n",
    "    url = f'https://e-qanun.az/framework/{doc_id}'\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Step 2: Wait explicitly for the content within \"WordSection1\" to load\n",
    "        element = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"WordSection1\"))\n",
    "        )\n",
    "        print(f\"Page content loaded successfully for document ID: {doc_id}\")\n",
    "    except TimeoutException:\n",
    "        print(f\"Loading took too much time for document ID: {doc_id}, skipping...\")\n",
    "        return\n",
    "\n",
    "    # Get the fully rendered page source\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Save the HTML to the specified directory\n",
    "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML saved to: {html_file_path}\")\n",
    "\n",
    "    # Step 3: Load the saved HTML file and parse it using BeautifulSoup\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Step 4: Extract the document name ('Aktın Adı')\n",
    "    document_name = soup.title.get_text(strip=True)\n",
    "\n",
    "    # Step 5: Extract the text sections\n",
    "    word_section = soup.find('div', class_='WordSection1')\n",
    "    paragraphs = word_section.find_all('p', class_='MsoNormal')  # Adjust class as needed\n",
    "\n",
    "    # Extract the text (including those that might be empty or contain only whitespace)\n",
    "    sections = [p.get_text(strip=True).replace('\\n', ' ') for p in paragraphs]\n",
    "\n",
    "    # Step 6: Structure the data into a DataFrame with sections\n",
    "    data = {\n",
    "        'Aktın Adı': [document_name] * len(sections),  # Document name repeated for each section\n",
    "        'Mətn': sections,  # Each section as a separate row\n",
    "        'e-qanun reference': [url] * len(sections),  # Use the dynamic URL from Selenium\n",
    "        'Embeddings': ['[Empty]'] * len(sections)  # Placeholder for embeddings\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Step 7: Ensure all values in 'Mətn' column are strings before using .str\n",
    "    df['Mətn'] = df['Mətn'].astype(str)\n",
    "\n",
    "    # Step 8: Remove rows where 'Mətn' column is empty, contains only whitespace, or is shorter than 33 characters\n",
    "    df = df[df['Mətn'].str.strip().astype(bool)]  # Remove empty/whitespace entries\n",
    "    df = df[df['Mətn'].str.len() >= 33]  # Remove rows where 'Mətn' length is less than 33 characters\n",
    "\n",
    "    # Step 9: Reset the index after filtering\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Step 10: Save the DataFrame to a .parquet file\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "    print(f\"Data saved to {parquet_file_path}\")\n",
    "\n",
    "# Step 3: Loop through each document id and process it\n",
    "for doc_id in document_ids:\n",
    "    process_document(doc_id)\n",
    "\n",
    "# Close the driver after processing all documents\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading took too much time for document ID: 15918, skipping...** Check all these files to handle them also!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\\57810.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are empty parquet files, maybe beacuse the page has not been opened during crawling! They don't open even if I click on the saved HTML locally!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to Check and Remove Empty/Invalid Files After Parsing Session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the files are saved\n",
    "output_dir = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\"\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(output_dir)\n",
    "\n",
    "# Filter only the .parquet files\n",
    "parquet_files = [f for f in all_files if f.endswith('.parquet')]\n",
    "\n",
    "# Counter to track the number of empty files\n",
    "empty_file_count = 0\n",
    "\n",
    "# List to store the names of the files that will be removed (for one-time printing)\n",
    "files_to_remove = []\n",
    "\n",
    "# Iterate through the parquet files\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_path = os.path.join(output_dir, parquet_file)\n",
    "    \n",
    "    try:\n",
    "        # Check if the parquet file is empty by reading it into a DataFrame\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            # If the parquet file is empty, count it and prepare to remove the corresponding files\n",
    "            empty_file_count += 1\n",
    "            files_to_remove.append(parquet_file)\n",
    "            \n",
    "            # Remove the parquet file\n",
    "            os.remove(parquet_path)\n",
    "            \n",
    "            # Construct the corresponding HTML file path\n",
    "            html_file = parquet_file.replace('.parquet', '.html')\n",
    "            html_path = os.path.join(output_dir, html_file)\n",
    "            \n",
    "            # Check if the corresponding HTML file exists and delete it\n",
    "            if os.path.exists(html_path):\n",
    "                os.remove(html_path)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {parquet_file}: {e}\")\n",
    "\n",
    "# Print the number of empty files found and removed\n",
    "print(f\"Total number of empty parquet files found and removed: {empty_file_count}\")\n",
    "\n",
    "# Optional: Print the list of removed files if you want to keep track\n",
    "#print(f\"Files removed: {files_to_remove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Files removed: {files_to_remove}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is fully enough to process most cases at this stage of the Product Development! Leave rest cases for now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing YERLİ İCRA HAKİMİYYƏTİ ORQANININ QƏRARI (avoiding tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the webdriver and scrape the page dynamically using Selenium\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Optional: Run browser in headless mode if you don't want to see the browser UI\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Define the URL dynamically\n",
    "url = 'https://e-qanun.az/framework/29882'  # Change as needed\n",
    "driver.get(url)\n",
    "\n",
    "# Step 2: Wait explicitly for the content within \"WordSection1\" to load\n",
    "try:\n",
    "    element = WebDriverWait(driver, 30).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"Section1\"))\n",
    "    )\n",
    "    print(\"Page content loaded successfully\")\n",
    "except TimeoutException:\n",
    "    print(\"Loading took too much time! Exiting...\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Get the fully rendered page source\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Extract the last part of the URL to use as the file name (e.g., '57810')\n",
    "file_name = url.split('/')[-1]\n",
    "\n",
    "# Construct the full file path for saving the HTML\n",
    "output_dir = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\\YERLİ İCRA HAKİMİYYƏTİ ORQANLARININ QƏRARLARI\"\n",
    "file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
    "\n",
    "# Save the HTML to the specified directory\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "print(f\"HTML saved to: {file_path}\")\n",
    "\n",
    "# Step 3: Load the saved HTML file and parse it using BeautifulSoup\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 4: Extract the document name ('Aktın Adı') from the title\n",
    "document_name = soup.title.get_text(strip=True)\n",
    "\n",
    "# Step 5: Extract the text sections within the \"Section1\" div excluding tables and certain div sections\n",
    "word_section = soup.find('div', class_='Section1')\n",
    "\n",
    "# Find all paragraphs that are not inside the excluded tables or divs\n",
    "paragraphs = word_section.find_all(lambda tag: (\n",
    "    tag.name == 'p' and \n",
    "    'MsoNormal' in tag.get('class', []) and \n",
    "    not tag.find_parent(['table']) and  # Exclude content inside any table\n",
    "    not tag.find_parent('div', style=lambda s: s and 'border-bottom:double gray' in s) and  # Exclude the specific 'div' style\n",
    "    not tag.find_parent('table', class_='MsoTableGrid')  # Exclude another table class\n",
    "))\n",
    "\n",
    "# Extract the text (including those that might be empty or contain only whitespace)\n",
    "sections = [p.get_text(strip=True).replace('\\n', ' ') for p in paragraphs]\n",
    "\n",
    "# Step 7: Structure the data into a DataFrame with sections\n",
    "data = {\n",
    "    'Aktın Adı': [document_name] * len(sections),  # Document name repeated for each section\n",
    "    'Mətn': sections,  # Each section as a separate row\n",
    "    'e-qanun reference': [url] * len(sections),  # Use the dynamic URL from Selenium\n",
    "    'Embeddings': ['[Empty]'] * len(sections)  # Placeholder for embeddings\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 8: Remove rows where 'Mətn' column is empty, contains only whitespace, or is shorter than 33 characters\n",
    "df['Mətn'] = df['Mətn'].astype(str)\n",
    "df = df[df['Mətn'].str.strip().astype(bool)]  # Remove empty/whitespace entries\n",
    "df = df[df['Mətn'].str.len() >= 33]  # Remove rows where 'Mətn' length is less than 33 characters\n",
    "\n",
    "# Step 9: Reset the index after filtering\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Step 10: Save the DataFrame to a .parquet file, using the dynamically named file based on the HTML file name\n",
    "output_parquet_path = os.path.join(output_dir, f'{file_name}.parquet')\n",
    "df.to_parquet(output_parquet_path)\n",
    "\n",
    "print(f\"Data saved to {output_parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\\YERLİ İCRA HAKİMİYYƏTİ ORQANLARININ QƏRARLARI\\29882.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Aktın Adı'].unique())\n",
    "display(df['Mətn'][0])\n",
    "display(df['Mətn'][1])\n",
    "display(df['Mətn'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping Throigh filtered_links df to Construct the URL Dynamically from the 'id' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'filtered_links' is a pandas DataFrame and the 'id' column holds the last number for the URL\n",
    "base_url = 'https://e-qanun.az/framework/'\n",
    "\n",
    "for idx, row in filtered_links.iterrows():\n",
    "    # Construct the URL dynamically from the 'id' column\n",
    "    url = f\"{base_url}{row['id']}\"\n",
    "    print(f\"Processing URL: {url}\")\n",
    "\n",
    "    # Define file names based on 'id'\n",
    "    file_name = row['id']\n",
    "    \n",
    "    # Define paths for HTML and parquet files\n",
    "    output_dir = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\\YERLİ İCRA HAKİMİYYƏTİ ORQANLARININ QƏRARLARI\"\n",
    "    html_file_path = os.path.join(output_dir, f\"{file_name}.html\")\n",
    "    parquet_file_path = os.path.join(output_dir, f\"{file_name}.parquet\")\n",
    "    \n",
    "    # Skip processing if both the HTML and parquet files already exist\n",
    "    if os.path.exists(html_file_path) and os.path.exists(parquet_file_path):\n",
    "        print(f\"Skipping {file_name}: Both HTML and parquet files already exist.\")\n",
    "        continue\n",
    "\n",
    "    # Step 1: Initialize the webdriver and scrape the page dynamically using Selenium\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Optional: Run browser in headless mode if you don't want to see the browser UI\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Wait explicitly for the content within \"Section1\" to load\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"Section1\"))\n",
    "        )\n",
    "        print(\"Page content loaded successfully\")\n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time! Exiting...\")\n",
    "        driver.quit()\n",
    "        continue  # Skip to the next iteration if loading failed\n",
    "\n",
    "    # Get the fully rendered page source\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Save the HTML to the specified directory\n",
    "    with open(html_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    print(f\"HTML saved to: {html_file_path}\")\n",
    "\n",
    "    # Step 3: Load the saved HTML file and parse it using BeautifulSoup\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Step 4: Extract the document name ('Aktın Adı') from the title\n",
    "    document_name = soup.title.get_text(strip=True)\n",
    "\n",
    "    # Step 5: Extract the text sections within the \"Section1\" div excluding tables and certain div sections\n",
    "    word_section = soup.find('div', class_='Section1')\n",
    "\n",
    "    # Find all paragraphs that are not inside the excluded tables or divs\n",
    "    paragraphs = word_section.find_all(lambda tag: (\n",
    "        tag.name == 'p' and \n",
    "        'MsoNormal' in tag.get('class', []) and \n",
    "        not tag.find_parent(['table']) and  # Exclude content inside any table\n",
    "        not tag.find_parent('div', style=lambda s: s and 'border-bottom:double gray' in s) and  # Exclude the specific 'div' style\n",
    "        not tag.find_parent('table', class_='MsoTableGrid')  # Exclude another table class\n",
    "    ))\n",
    "\n",
    "    # Extract the text (including those that might be empty or contain only whitespace)\n",
    "    sections = [p.get_text(strip=True).replace('\\n', ' ') for p in paragraphs]\n",
    "\n",
    "    # Step 7: Structure the data into a DataFrame with sections\n",
    "    data = {\n",
    "        'Aktın Adı': [document_name] * len(sections),  # Document name repeated for each section\n",
    "        'Mətn': sections,  # Each section as a separate row\n",
    "        'e-qanun reference': [url] * len(sections),  # Use the dynamic URL from Selenium\n",
    "        'Embeddings': ['[Empty]'] * len(sections)  # Placeholder for embeddings\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Step 8: Remove rows where 'Mətn' column is empty, contains only whitespace, or is shorter than 33 characters\n",
    "    df['Mətn'] = df['Mətn'].astype(str)\n",
    "    df = df[df['Mətn'].str.strip().astype(bool)]  # Remove empty/whitespace entries\n",
    "    df = df[df['Mətn'].str.len() >= 33]  # Remove rows where 'Mətn' length is less than 33 characters\n",
    "\n",
    "    # Step 9: Reset the index after filtering\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Step 10: Save the DataFrame to a .parquet file, using the dynamically named file based on the HTML file name\n",
    "    df.to_parquet(parquet_file_path)\n",
    "\n",
    "    print(f\"Data saved to {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the files are saved\n",
    "output_dir = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\e-qanun HTMLs\\YERLİ İCRA HAKİMİYYƏTİ ORQANLARININ QƏRARLARI\"\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(output_dir)\n",
    "\n",
    "# Filter only the .parquet files\n",
    "parquet_files = [f for f in all_files if f.endswith('.parquet')]\n",
    "\n",
    "# Counter to track the number of empty files\n",
    "empty_file_count = 0\n",
    "\n",
    "# List to store the names of the files that will be removed (for one-time printing)\n",
    "files_to_remove = []\n",
    "\n",
    "# Iterate through the parquet files\n",
    "for parquet_file in parquet_files:\n",
    "    parquet_path = os.path.join(output_dir, parquet_file)\n",
    "    \n",
    "    try:\n",
    "        # Check if the parquet file is empty by reading it into a DataFrame\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            # If the parquet file is empty, count it and prepare to remove the corresponding files\n",
    "            empty_file_count += 1\n",
    "            files_to_remove.append(parquet_file)\n",
    "            \n",
    "            # Remove the parquet file\n",
    "            os.remove(parquet_path)\n",
    "            \n",
    "            # Construct the corresponding HTML file path\n",
    "            html_file = parquet_file.replace('.parquet', '.html')\n",
    "            html_path = os.path.join(output_dir, html_file)\n",
    "            \n",
    "            # Check if the corresponding HTML file exists and delete it\n",
    "            if os.path.exists(html_path):\n",
    "                os.remove(html_path)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {parquet_file}: {e}\")\n",
    "\n",
    "# Print the number of empty files found and removed\n",
    "print(f\"Total number of empty parquet files found and removed: {empty_file_count}\")\n",
    "\n",
    "# Optional: Print the list of removed files if you want to keep track\n",
    "#print(f\"Files removed: {files_to_remove}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Files removed: {files_to_remove}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your SQLite database\n",
    "db_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\sqlite3.db\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load the 'user_feedback' table into a DataFrame\n",
    "query = \"SELECT * FROM user_feedback\"\n",
    "df_user_feedback = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Save the DataFrame to a .parquet file\n",
    "output_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\user_feedback.parquet\"\n",
    "df_user_feedback.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading and Exploring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path1 = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\test_data.parquet\"\n",
    "df1 = pd.read_parquet(path1)\n",
    "\n",
    "display(df1)\n",
    "df1.info()\n",
    "df1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_parquet(path1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 19\n",
    "\n",
    "display(df1['query_text'].iloc[n])\n",
    "display(df1['gpt_response'].iloc[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\finetune_data.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['query'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure plots are displayed in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Class distribution plot\n",
    "df['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'borclunun ölkədən getmək hüququnun müvəqqəti məhdudlaşdırılması və ölkədən çıxarılması proseduru necədir?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly sampling rows from both classes (0s and 1s) to achieve a 10:10 proportion for the given query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of rows matching the query 's' and labels 0 and 1\n",
    "# Set \"n=\" to the number of rows to remove!\n",
    "indices_to_remove_0 = df[(df['label'] == 0) & (df['query'] == s)].sample(n=1, random_state=42).index\n",
    "#indices_to_remove_1 = df[(df['label'] == 1) & (df['query'] == s)].sample(n=1, random_state=42).index\n",
    "\n",
    "# Drop these rows from the original DataFrame inplace\n",
    "df.drop(indices_to_remove_0, inplace=True)\n",
    "#df.drop(indices_to_remove_1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(df['query'].loc[(df['label'] == 0) & (df['query'] == s)]))\n",
    "display(len(df['query'].loc[(df['label'] == 1) & (df['query'] == s)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.loc[df['query'] == s]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[109]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[(df['query'] == s) & (df.index != 226), 'label'] = 0\n",
    "\n",
    "#df.loc[(df['query'] == s) & (df.index == 48), 'label'] = 0\n",
    "\n",
    "#df.loc[(df['query'] == s) & (df.index == 294), 'text'] = ''\n",
    "\n",
    "#df[\"query\"] = df[\"query\"].replace([\"nikahın ləğvi \"], \"nikahın ləğvi\")\n",
    "\n",
    "#df['query'] = df['query'].str.lower()\n",
    "\n",
    "#indexes_to_drop = [109, 110]  # List of indices you want to drop\n",
    "#df = df.drop(indexes_to_drop)\n",
    "#df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexes_to_drop = [241]  # List of indices you want to drop\n",
    "#df = df.drop(indexes_to_drop)\n",
    "#df = df.reset_index(drop=True)\n",
    "\n",
    "# Step 1: Identify the rows where query == \"analıq məzuniyyəti\"\n",
    "new_rows = df[df['query'] == \"alimentin ödənilməsi\"].copy()\n",
    "\n",
    "# Step 2: Update the 'query' value of the new rows\n",
    "new_rows['query'] = \"aliment tələbi\"\n",
    "\n",
    "# Step 3: Append the new rows to the original DataFrame\n",
    "df = df.append(new_rows, ignore_index=True)\n",
    "\n",
    "display(df.loc[df['query'] == \"alimentin ödənilməsi\"])\n",
    "display(df.loc[df['query'] == \"aliment tələbi\"])\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Training Instances Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your list of texts\n",
    "new_texts = [\n",
    "    \"\"\n",
    "]\n",
    "label = 1\n",
    "\n",
    "# Create a new DataFrame for all the new rows\n",
    "new_rows_df = pd.DataFrame([{'query': s, 'text': text, 'label': label} for text in new_texts])\n",
    "\n",
    "# Add the new rows to the DataFrame using pd.concat\n",
    "df = pd.concat([df, new_rows_df], ignore_index=True)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "display(df)\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go through all unique Queries, open the act-finder.com, search them in it, check the existing ones and add labels manually to the dataset! If there is no one True answer, find them in the E-Qanun and Add Manually! <u>Use Google Search, ChatGPT and E-Qanun (Mündəricat) to Find Correct (Target) Answers!</u>**\n",
    "\n",
    "***This is the <u>Training Dataset</u>, so <u>I can add any relevant text from anywhere</u> (Article Titles and Texts by Clauses Separately!, Web Search, etc.!) <u>to make it richer for my 56 unique queries</u>!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "# Strings to compare\n",
    "texts = [\n",
    "    \"Paylı mülkiyyətim qanunsuz dağıdılıb\",\n",
    "    \"Maddə 354. Avtomobil yollarında yol hərəkəti təhlükəsizliyi qaydalarının pozulması 165 354.0. ...\",\n",
    "    \"Maddə 12 Tərəflərin səlahiyyətli orqanları maliyyə pozuntularının qarşısının alınması, aşkar olunması ...\",\n",
    "    \"Maddəyə əsasən qlobal texniki qaydaların müəyyənləşdirilməsi və dəyişdirilməsi; 14.3. 7-ci\",\n",
    "    \"Maddə 245: Birgə və paylı mülkiyyət hüququnun əsaslarını müəyyən edir. ...\",\n",
    "    \"Maddə 29: Hər kəsin mülkiyyət hüququ vardır və bu hüquq qanunla qorunur. ...\"\n",
    "]\n",
    "\n",
    "# Generate embeddings using the OpenAI API\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    embeddings.append(response['data'][0]['embedding'])\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "embedding_vectors = np.array(embeddings)\n",
    "\n",
    "# Calculate cosine similarity between the first text and the others\n",
    "similarities = cosine_similarity([embedding_vectors[0]], embedding_vectors[1:])\n",
    "\n",
    "# Print the similarities\n",
    "for i, similarity in enumerate(similarities[0]):\n",
    "    print(f\"Cosine similarity with text {i+1}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Cosine similarity with text 1: 0.3204981110849322 (correct label: 0)\n",
    "Cosine similarity with text 2: 0.34029572696519317 (correct label: 0)\n",
    "Cosine similarity with text 3: 0.3275207821570136 (correct label: 0)\n",
    "Cosine similarity with text 4: 0.5116664280377892 (correct label: 1)\n",
    "Cosine similarity with text 5: 0.4335759279244248 (correct label: 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Raw Text Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\codes.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.duplicated().sum())\n",
    "display(df.loc[df.duplicated() == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[6472]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "display(df.duplicated().sum())\n",
    "display(df.loc[df.duplicated() == True])\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df['text'][0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str[str.find(\"1. Ümumi müddəalar\")::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting CSV to Parquet and Concatenating All Parquets into the Single One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\NLP project\\!referendum_acts_processed.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Leading/Trailing Spaces\n",
    "def g(text):\n",
    "    return text.strip()\n",
    "\n",
    "df['text'] = df['text'].apply(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"\\xa0\" pattern\n",
    "df['text'] = df['text'].str.replace(r'\\xa0', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace newline characters\n",
    "df['text'] = df['text'].str.replace('\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Multiple Spaces with a Single One\n",
    "def g(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "df['text'] = df['text'].apply(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[6469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new save path\n",
    "save_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\referendum_acts.parquet\"\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "df.to_parquet(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the Parquet files\n",
    "directory = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\"\n",
    "\n",
    "# List to hold DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the Parquet file and append it to the list\n",
    "        df_list.append(pd.read_parquet(file_path))\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save the concatenated DataFrame to a single Parquet file\n",
    "output_path = os.path.join(directory, \"corpus.parquet\")\n",
    "concatenated_df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"All files have been concatenated and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the Parquet file\n",
    "output_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus.parquet\"\n",
    "\n",
    "# Save the DataFrame as a Parquet file\n",
    "df.to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['url'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['type'] == 'Konstitusiya']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'].loc[df['type'] == 'Məcəllələr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign '-' to rows where 'name' column has null values\n",
    "df.loc[df['name'].isnull(), 'name'] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['status'], inplace=True)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.duplicated().sum())\n",
    "display(df.loc[df.duplicated() == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "display(df.duplicated().sum())\n",
    "display(df.loc[df.duplicated() == True])\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].str.strip() != '']\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'title' column to 'name'\n",
    "df = df.rename(columns={'title': 'name', 'typeName': 'type'})\n",
    "\n",
    "# Reorder the columns\n",
    "df = df[['url', 'type', 'name', 'text']]\n",
    "\n",
    "# Verify the change\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
