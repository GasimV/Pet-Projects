{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Semantic Seach Model\n",
    "\n",
    "***sentence-transformers/msmarco-bert-base-dot-v5***\n",
    "\n",
    "https://huggingface.co/sentence-transformers/msmarco-bert-base-dot-v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the core path\n",
    "BASE_PATH = r'E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection'\n",
    "\n",
    "text_path = os.path.join(BASE_PATH, 'finetune_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(text_path)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "# Load and prepare the dataset\n",
    "data_path = \"/content/finetune_data.parquet\"  # Path to your file\n",
    "dataset = load_dataset(\"parquet\", data_files=data_path)[\"train\"]\n",
    "train_samples = [InputExample(texts=[row[\"query\"], row[\"text\"]], label=float(row[\"label\"])) for row in dataset]\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    sentence_pairs = [{\"sentence1\": example.texts[0], \"sentence2\": example.texts[1]} for example in batch]\n",
    "    labels = torch.tensor([example.label for example in batch], dtype=torch.float32)\n",
    "    return sentence_pairs, labels\n",
    "\n",
    "# Create DataLoader with the custom collate function\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=10, collate_fn=collate_fn)\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for sentence_pairs, labels in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move labels to the appropriate device\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Tokenize sentence pairs directly in the required format\n",
    "        sentence_features = [\n",
    "            {\n",
    "                \"input_ids\": model.tokenizer(pair[\"sentence1\"], pair[\"sentence2\"], \n",
    "                                             padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"],\n",
    "                \"attention_mask\": model.tokenizer(pair[\"sentence1\"], pair[\"sentence2\"], \n",
    "                                                  padding=True, truncation=True, return_tensors=\"pt\")[\"attention_mask\"]\n",
    "            }\n",
    "            for pair in sentence_pairs\n",
    "        ]\n",
    "\n",
    "        # Convert sentence_features to a format compatible with loss function\n",
    "        sentence_features = [{k: v.to(device) for k, v in feature.items()} for feature in sentence_features]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(sentence_features, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_dataloader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"/content/drive/MyDrive/msmarco-bert-base-dot-v5-cosine/fine-tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is the modified code with the following changes:**\n",
    "\n",
    "1. ***Lowered Learning Rate***: Set to 5e-6.\n",
    "2. ***Increased Epochs***: Set to 10.\n",
    "3. ***Gradient Accumulation***: Added accumulation for every 2 batches.\n",
    "4. ***Layer-Wise Learning Rate Decay (LLRD)***: Applied a different learning rate for each layer, with lower layers having smaller learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers datasets torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "# Load and prepare the dataset\n",
    "data_path = \"/content/finetune_data.parquet\"  # Path to your file (this one is for Colab)\n",
    "dataset = load_dataset(\"parquet\", data_files=data_path)[\"train\"]\n",
    "train_samples = [InputExample(texts=[row[\"query\"], row[\"text\"]], label=float(row[\"label\"])) for row in dataset]\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    sentence_pairs = [{\"sentence1\": example.texts[0], \"sentence2\": example.texts[1]} for example in batch]\n",
    "    labels = torch.tensor([example.label for example in batch], dtype=torch.float32)\n",
    "    return sentence_pairs, labels\n",
    "\n",
    "# Create DataLoader with the custom collate function and adjusted batch size\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=10, collate_fn=collate_fn)\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 5e-6\n",
    "accumulation_steps = 2\n",
    "\n",
    "# Apply Layer-Wise Learning Rate Decay (LLRD)\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [param for name, param in model.named_parameters() if \"encoder.layer.0\" in name],\n",
    "        \"lr\": learning_rate * 0.5\n",
    "    },\n",
    "    {\n",
    "        \"params\": [param for name, param in model.named_parameters() if \"encoder.layer.1\" in name],\n",
    "        \"lr\": learning_rate * 0.6\n",
    "    },\n",
    "    {\n",
    "        \"params\": [param for name, param in model.named_parameters() if \"encoder.layer.2\" in name],\n",
    "        \"lr\": learning_rate * 0.7\n",
    "    },\n",
    "    {\n",
    "        \"params\": [param for name, param in model.named_parameters() if \"encoder.layer.3\" in name],\n",
    "        \"lr\": learning_rate * 0.8\n",
    "    },\n",
    "    {\n",
    "        \"params\": [param for name, param in model.named_parameters() if \"encoder.layer.4\" in name],\n",
    "        \"lr\": learning_rate * 0.9\n",
    "    },\n",
    "    {\n",
    "        \"params\": [param for name, param in model.named_parameters() if \"encoder.layer.5\" in name],\n",
    "        \"lr\": learning_rate\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "# Training loop with gradient accumulation and LLRD\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for i, (sentence_pairs, labels) in enumerate(progress_bar):\n",
    "        # Move labels to the appropriate device\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Tokenize sentence pairs directly in the required format\n",
    "        sentence_features = [\n",
    "            {\n",
    "                \"input_ids\": model.tokenizer(pair[\"sentence1\"], pair[\"sentence2\"], \n",
    "                                             padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"],\n",
    "                \"attention_mask\": model.tokenizer(pair[\"sentence1\"], pair[\"sentence2\"], \n",
    "                                                  padding=True, truncation=True, return_tensors=\"pt\")[\"attention_mask\"]\n",
    "            }\n",
    "            for pair in sentence_pairs\n",
    "        ]\n",
    "\n",
    "        # Convert sentence_features to a format compatible with loss function\n",
    "        sentence_features = [{k: v.to(device) for k, v in feature.items()} for feature in sentence_features]\n",
    "        \n",
    "        # Calculate loss directly with sentence_features and labels\n",
    "        loss = loss_function(sentence_features, labels) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation step\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps  # Correct scaling for accumulated loss\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_dataloader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"/content/drive/MyDrive/msmarco-bert-base-dot-v5-cosine/fine-tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 1/10 - Loss: 0.36621989842790825\n",
    "Epoch 2/10 - Loss: 0.28407311846430483\n",
    "Epoch 3/10 - Loss: 0.2611631113749284\n",
    "Epoch 4/10 - Loss: 0.25623579483765824\n",
    "Epoch 5/10 - Loss: 0.2572602228476451\n",
    "Epoch 6/10 - Loss: 0.2544843180821492\n",
    "Epoch 7/10 - Loss: 0.2549075830441255\n",
    "Epoch 8/10 - Loss: 0.2535590521418131\n",
    "Epoch 9/10 - Loss: 0.2545134897415455\n",
    "Epoch 10/10 - Loss: 0.2544914065645291\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from documentation in Hugging Face!\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "query = \"cinayət təqibi nədir?\"\n",
    "docs = [\"83.1-1. Həm Azərbaycan Respublikasının, həm də ərazisində törədildiyi xarici dövlətin qanunvericiliyinə əsasən cinayət sayılan əmələ görə xarici dövlətin məhkəmə hökmünə əsasən yaranmış məhkumluq barədə məlumat cinayət təqibinin gedişində daxil olduqda və müvafiq hökm Azərbaycan Respublikasında qanunla müəyyən edilmiş qaydada tanındıqda, bu məhkumluq da cinayətlərin residivi zamanı və cəza təyin edilərkən nəzərə alınır.\", \n",
    "        \"1.1. Azərbaycan Respublikasının cinayət-prosessual qanunvericiliyi cinayətin əlamətlərini əks etdirən əməllərin cinayət olub-olmamağını, cinayəti törətməkdə təqsirləndirilən şəxsin təqsirli olub-olmamağını, habelə cinayət qanunu ilə nəzərdə tutulmuş əməlləri törətməkdə şübhəli və ya təqsirləndirilən şəxsin cinayət təqibinin və müdafiəsinin, hüquqi şəxs barəsində cinayət-hüquqi tədbirlərin tətbiq edilməsinin hüquqi prosedurlarını müəyyən edir.\", \n",
    "        \"1.1. Bu Məcəllə inzibati hüquq münasibətləri ilə bağlı mübahisələrin (bundan sonra - inzibati mübahisələr) məhkəmə aidiyyətini, həmin mübahisələrə məhkəmədə baxılmasının və həll edilməsinin prosessual prinsiplərini və qaydalarını müəyyən edir. 1.2. Bu Məcəllə ilə başqa qayda müəyyən edilmədiyi və bu Məcəllə ilə nəzərdə tutulmuş prosessual prinsiplərə zidd olmadığı hallarda, inzibati mübahisələrə dair işlər üzrə məhkəmə icraatında (bundan sonra - inzibati məhkəmə icraatı) Azərbaycan Respublikasının Mülki Prosessual Məcəlləsinin müddəaları tətbiq oluna bilər. 1.3. Ələt azad iqtisadi zonasında inzibati hüquq münasibətləri ilə bağlı mübahisələrə baxılması “Ələt azad iqtisadi zonası haqqında” Azərbaycan Respublikası Qanununun tələblərinə uyğun olaraq tənzimlənir.\", \n",
    "        \"Maddə 1. Şəhərsalma və tikinti qanunvericiliyinin təyinatı Azərbaycan Respublikasının şəhərsalma və tikinti qanunvericiliyi şəhərsalma və tikinti fəaliyyətinin hüquqi əsaslarını, prinsiplərini, eləcə də dövlətin, bələdiyyələrin, fiziki və ya hüquqi şəxslərin şəhərsalma və tikinti fəaliyyəti sahəsində hüquq və vəzifələrini müəyyən edir.\"]\n",
    "\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "#Encode query and documents\n",
    "query_emb = model.encode(query)\n",
    "doc_emb = model.encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "print(\"Query:\", query)\n",
    "print('\\n')\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Inference Code!\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a single query for testing\n",
    "query = \"cinayət təqibi nədir?\"\n",
    "\n",
    "# Define texts to compare with the query\n",
    "texts = [\n",
    "    \"1.1. Azərbaycan Respublikasının cinayət-prosessual qanunvericiliyi cinayətin əlamətlərini əks etdirən əməllərin cinayət olub-olmamağını, cinayəti törətməkdə təqsirləndirilən şəxsin təqsirli olub-olmamağını, habelə cinayət qanunu ilə nəzərdə tutulmuş əməlləri törətməkdə şübhəli və ya təqsirləndirilən şəxsin cinayət təqibinin və müdafiəsinin, hüquqi şəxs barəsində cinayət-hüquqi tədbirlərin tətbiq edilməsinin hüquqi prosedurlarını müəyyən edir.\",\n",
    "    \"83.1-1. Həm Azərbaycan Respublikasının, həm də ərazisində törədildiyi xarici dövlətin qanunvericiliyinə əsasən cinayət sayılan əmələ görə xarici dövlətin məhkəmə hökmünə əsasən yaranmış məhkumluq barədə məlumat cinayət təqibinin gedişində daxil olduqda və müvafiq hökm Azərbaycan Respublikasında qanunla müəyyən edilmiş qaydada tanındıqda, bu məhkumluq da cinayətlərin residivi zamanı və cəza təyin edilərkən nəzərə alınır.\",\n",
    "    \"Maddə 1. Şəhərsalma və tikinti qanunvericiliyinin təyinatı Azərbaycan Respublikasının şəhərsalma və tikinti qanunvericiliyi şəhərsalma və tikinti fəaliyyətinin hüquqi əsaslarını, prinsiplərini, eləcə də dövlətin, bələdiyyələrin, fiziki və ya hüquqi şəxslərin şəhərsalma və tikinti fəaliyyəti sahəsində hüquq və vəzifələrini müəyyən edir.\",\n",
    "    \"1.1. Bu Məcəllə inzibati hüquq münasibətləri ilə bağlı mübahisələrin (bundan sonra - inzibati mübahisələr) məhkəmə aidiyyətini, həmin mübahisələrə məhkəmədə baxılmasının və həll edilməsinin prosessual prinsiplərini və qaydalarını müəyyən edir. 1.2. Bu Məcəllə ilə başqa qayda müəyyən edilmədiyi və bu Məcəllə ilə nəzərdə tutulmuş prosessual prinsiplərə zidd olmadığı hallarda, inzibati mübahisələrə dair işlər üzrə məhkəmə icraatında (bundan sonra - inzibati məhkəmə icraatı) Azərbaycan Respublikasının Mülki Prosessual Məcəlləsinin müddəaları tətbiq oluna bilər. 1.3. Ələt azad iqtisadi zonasında inzibati hüquq münasibətləri ilə bağlı mübahisələrə baxılması “Ələt azad iqtisadi zonası haqqında” Azərbaycan Respublikası Qanununun tələblərinə uyğun olaraq tənzimlənir.\",\n",
    "    \"7.0.4. cinayət təqibi — cinayət hadisəsinin müəyyən edilməsi, cinayət qanunu ilə nəzərdə tutulmuş əməli törətmiş şəxsin ifşa olunması, ittiham irəli sürülməsi, bu ittihamın məhkəmədə müdafiə edilməsi, ona cəza təyin edilməsi, zəruri olduqda prosessual məcburiyyət tədbirlərinin təmin edilməsi məqsədi ilə həyata keçirilən cinayət-prosessual fəaliyyətdir;\"\n",
    "]\n",
    "\n",
    "# Function to compute similarity scores\n",
    "def compute_similarity(query, texts):\n",
    "    # Tokenize and embed query once\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "        # Tokenize and embed each text\n",
    "        text_embedding = model.encode(text, convert_to_tensor=True, device=device)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_score = util.cos_sim(query_embedding, text_embedding).item()\n",
    "        \n",
    "        # Collect the results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"text\": text,\n",
    "            \"similarity_score\": cosine_score\n",
    "        })\n",
    "\n",
    "    # Sort results by similarity score in descending order\n",
    "    results = sorted(results, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "# Run similarity computation\n",
    "results = compute_similarity(query, texts)\n",
    "\n",
    "# Display sorted results\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Similarity Score: {result['similarity_score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***The test comparison has demonstrated that the fine-tuned model's performance is much more better than default (raw, pre-trained) one!***\n",
    "- ***The model before fine-tuning is not able to distinguish similar sentences from those that have absolutely no common meaning with the user query and assigned all texts similarily high scores, whereas the fine-tuned model assigned >0.5 scores for relevant ones and <0.5 for irrelevant ones!***\n",
    "- ***That is why, I can set the filter to 0.5 and display results above 0.5 in prompt!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on the Sample n% of the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the text data from the corpus file\n",
    "corpus_df = pd.read_parquet(corpus_path)\n",
    "\n",
    "# Sample n% of the corpus\n",
    "sampled_corpus_df = corpus_df.sample(frac=0.01, random_state=42)\n",
    "texts = sampled_corpus_df['text'].tolist()  # Extract the 'text' column as a list\n",
    "\n",
    "# Define a single query for testing\n",
    "query = \"icra hərəkətləri neçə günə başa çatdırılmalıdır?\"\n",
    "\n",
    "# Define a threshold for similarity score (e.g., 0.5)\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Function to compute similarity scores with a threshold\n",
    "def compute_similarity(query, texts, threshold=SIMILARITY_THRESHOLD):\n",
    "    # Tokenize and embed the query once\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "        # Tokenize and embed each text\n",
    "        text_embedding = model.encode(text, convert_to_tensor=True, device=device)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_score = util.cos_sim(query_embedding, text_embedding).item()\n",
    "        \n",
    "        # Collect results if they meet the threshold\n",
    "        if cosine_score >= threshold:\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"text\": text,\n",
    "                \"similarity_score\": cosine_score\n",
    "            })\n",
    "\n",
    "    # Sort results by similarity score in descending order\n",
    "    results = sorted(results, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "# Run similarity computation\n",
    "results = compute_similarity(query, texts)\n",
    "\n",
    "# Display sorted results that meet the threshold\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Similarity Score: {result['similarity_score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on the Whole Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the text data from the corpus file\n",
    "corpus_df = pd.read_parquet(corpus_path)\n",
    "texts = corpus_df['text'].tolist()  # Extract the 'text' column as a list\n",
    "\n",
    "# Define a single query for testing\n",
    "query = \"icra hərəkətləri neçə günə başa çatdırılmalıdır?\"\n",
    "\n",
    "# Define a threshold for similarity score (e.g., 0.5)\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Function to compute similarity scores with a threshold\n",
    "def compute_similarity(query, texts, threshold=SIMILARITY_THRESHOLD):\n",
    "    # Tokenize and embed the query once\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    results = []\n",
    "    for text in texts:\n",
    "        # Tokenize and embed each text\n",
    "        text_embedding = model.encode(text, convert_to_tensor=True, device=device)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_score = util.cos_sim(query_embedding, text_embedding).item()\n",
    "        \n",
    "        # Collect results if they meet the threshold\n",
    "        if cosine_score >= threshold:\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"text\": text,\n",
    "                \"similarity_score\": cosine_score\n",
    "            })\n",
    "\n",
    "    # Sort results by similarity score in descending order\n",
    "    results = sorted(results, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "# Run similarity computation\n",
    "results = compute_similarity(query, texts)\n",
    "\n",
    "# Display sorted results that meet the threshold\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Similarity Score: {result['similarity_score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Inference Instead of Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the text data from the corpus file\n",
    "corpus_df = pd.read_parquet(corpus_path)\n",
    "texts = corpus_df['text'].tolist()  # Extract the 'text' column as a list\n",
    "\n",
    "# Define a single query for testing\n",
    "query = \"icra hərəkətləri neçə günə başa çatdırılmalıdır?\"\n",
    "\n",
    "# Define a threshold for similarity score (e.g., 0.5)\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Function to compute similarity scores with batching and optimized cosine similarity\n",
    "def compute_similarity(query, texts, threshold=SIMILARITY_THRESHOLD):\n",
    "    # Tokenize and embed the query once\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    # Encode all texts in a single batch\n",
    "    text_embeddings = model.encode(texts, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    # Compute cosine similarities between the query and all texts at once\n",
    "    cosine_scores = util.cos_sim(query_embedding, text_embeddings).cpu().numpy().flatten()\n",
    "    \n",
    "    # Collect results that meet the threshold\n",
    "    results = [\n",
    "        {\"query\": query, \"text\": texts[i], \"similarity_score\": score}\n",
    "        for i, score in enumerate(cosine_scores) if score >= threshold\n",
    "    ]\n",
    "\n",
    "    # Sort results by similarity score in descending order\n",
    "    results = sorted(results, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "# Run similarity computation\n",
    "results = compute_similarity(query, texts)\n",
    "\n",
    "# Display sorted results that meet the threshold\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Similarity Score: {result['similarity_score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Computing and Saving the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the text data from the corpus file\n",
    "corpus_df = pd.read_parquet(corpus_path)\n",
    "texts = corpus_df['text'].tolist()  # Extract the 'text' column as a list\n",
    "\n",
    "# Compute embeddings for all texts in batches\n",
    "batch_size = 64  # Adjust batch size based on GPU memory availability\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    # Encode batch and move to the CPU for storage (detach from GPU)\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, device=device).cpu()\n",
    "    embeddings.extend(batch_embeddings.numpy())  # Convert to numpy and add to list\n",
    "\n",
    "# Add embeddings as a new column to the DataFrame\n",
    "corpus_df['embeddings'] = embeddings\n",
    "\n",
    "# Save the modified DataFrame back to the same parquet file\n",
    "corpus_df.to_parquet(corpus_path, index=False)\n",
    "print(\"Embeddings have been precomputed and saved to the corpus file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = \"/content/drive/MyDrive/msmarco-bert-base-dot-v5-cosine/final\"\n",
    "corpus_path = \"/content/drive/MyDrive/corpus.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the text data from the corpus file\n",
    "corpus_df = pd.read_parquet(corpus_path)\n",
    "texts = corpus_df['text'].tolist()  # Extract the 'text' column as a list\n",
    "\n",
    "# Compute embeddings for all texts in batches\n",
    "batch_size = 32  # Adjust batch size based on GPU memory availability\n",
    "embeddings = []\n",
    "\n",
    "# Measure time for the process\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    # Encode batch and move to the CPU for storage (detach from GPU)\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, device=device).cpu()\n",
    "    embeddings.extend(batch_embeddings.numpy())  # Convert to numpy and add to list\n",
    "    \n",
    "    # Optionally log progress every few batches\n",
    "    if (i // batch_size) % 10 == 0:\n",
    "        print(f\"Processed batch {i // batch_size} of {len(texts) // batch_size}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total time for embedding computation: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Add embeddings as a new column to the DataFrame\n",
    "corpus_df['embeddings'] = embeddings\n",
    "\n",
    "# Save the modified DataFrame back to the same parquet file\n",
    "corpus_df.to_parquet(corpus_path, index=False)\n",
    "print(\"Embeddings have been precomputed and saved to the corpus file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus_embed.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path).to(\"cuda\")\n",
    "\n",
    "data = pd.read_parquet(corpus_path)[[\"text\", \"embeddings\"]]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = lambda x, y: np.dot(x, y)/(norm(x)*norm(y))\n",
    "\n",
    "model_vector = model.encode(\"İcra hərəkətlərinin həyata keçirilməsi müddətləri\")\n",
    "\n",
    "data[\"cosine\"] = data[\"embeddings\"].progress_apply(lambda x: cosine(x, model_vector))\n",
    "\n",
    "s = data.loc[data[\"cosine\"] >= 0.75][[\"cosine\", \"text\"]].sort_values(\"cosine\", ascending=False).head(20).reset_index(drop=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['text'].iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s['text']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus_embed.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the embeddings, texts, urls, and names from the corpus file\n",
    "corpus_df = pd.read_parquet(corpus_path)\n",
    "embeddings = np.array(corpus_df['embeddings'].tolist())  # Assuming 'embeddings' column contains precomputed embeddings\n",
    "texts = corpus_df['text'].tolist()  # Texts for reference\n",
    "urls = corpus_df['url'].tolist()    # URLs for reference\n",
    "names = corpus_df['name'].tolist()  # Names for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single query for testing\n",
    "query = \"İcra hərəkətlərinin həyata keçirilməsi müddətləri\"\n",
    "\n",
    "# Define a threshold for similarity score\n",
    "SIMILARITY_THRESHOLD = 0.75\n",
    "\n",
    "# Function to compute similarity scores with filtering and limit results to top K\n",
    "def compute_similarity(query, embeddings, threshold=SIMILARITY_THRESHOLD, top_k=20):\n",
    "    # Embed the query once and convert to NumPy array\n",
    "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "\n",
    "    # Compute cosine similarity for all embeddings in a single operation\n",
    "    similarities = util.cos_sim(query_embedding, embeddings).cpu().numpy().flatten()\n",
    "\n",
    "    # Collect results with filtering for unique texts\n",
    "    results = []\n",
    "    seen_texts = set()\n",
    "    for i, score in enumerate(similarities):\n",
    "        if score >= threshold and texts[i] not in seen_texts:\n",
    "            seen_texts.add(texts[i])  # Track text to avoid duplicates\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"text\": texts[i],\n",
    "                \"similarity_score\": score,\n",
    "                \"url\": urls[i],\n",
    "                \"name\": names[i]\n",
    "            })\n",
    "\n",
    "    # Sort results by similarity score in descending order and limit to top_k results\n",
    "    results = sorted(results, key=lambda x: x[\"similarity_score\"], reverse=True)[:top_k]\n",
    "    return results\n",
    "\n",
    "# Run similarity computation\n",
    "results = compute_similarity(query, embeddings)\n",
    "\n",
    "print(f\"User Query: {result['query']}\")\n",
    "print()\n",
    "print(\"Retrieved Text Passages:\")\n",
    "print()\n",
    "# Display sorted results that meet the threshold\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"URL: {result['url']}\")\n",
    "    print(f\"Document Name: {result['name']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.loc[corpus_df['text'].str.contains(\"Maddə 12. İcra hərəkətlərinin həyata keçirilməsi müddətləri\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maddə 12. İcra hərəkətlərinin həyata keçirilməsi müddətləri 12.1. Bu Qanunun 8.2-ci maddəsinin üçüncü bəndində nəzərdə tutulmuş hal istisna olmaqla, icra məmuru icra sənədini aldığı gündən iki ay müddətində bütün zəruri icra hərəkətlərini həyata keçirməlidir. [22] 12.2. Aşağıdakı icra sənədləri dərhal icra olunmalıdır: 12.2.1. alimentlərin, əmək haqqının, habelə bu ödəmələr üzrə bütün borc məbləğlərinin tutulması haqqında; 12.2.2. qanunsuz işdən çıxarılmış və ya başqa işə keçirilmiş işçinin işə və ya əvvəlki vəzifəyə bərpası haqqında. [23] 12.3. Azərbaycan Respublikasının qanunvericiliyi ilə və ya icra sənədi ilə icrası dərhal nəzərdə tutulmuş tələblər təxirəsalınmadan icra olunur. [24]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df['text'].iloc[251274]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error message indicates that the request you sent to the OpenAI API has exceeded the maximum allowed tokens per minute (TPM) limit. Specifically:\n",
    "\n",
    "- **Token Limit Exceeded**: The model has a cap on the number of tokens it can process in a single minute. In your case, the request tried to use 34,449 tokens, exceeding the 30,000 token-per-minute limit for GPT-4.\n",
    "  \n",
    "- **Token Calculation**: Tokens are the basic units of text that OpenAI models process, roughly equivalent to words or characters. When building your input, the API counts both input (prompt) and expected output tokens.\n",
    "\n",
    "To resolve this:\n",
    "1. **Reduce Input Size**: Truncate or summarize the input prompt to ensure it doesn’t exceed the token limit. For example, you could limit the number of search results or reduce the length of text passages.\n",
    "2. **Handle Errors**: Use a `try-except` block to handle `RateLimitError` and retry with a smaller input if the error occurs.\n",
    "\n",
    "You may also need to make multiple smaller API calls if there is too much text to fit within the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Apps\\python_envs\\cuda_gpu_env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "You try to use a model that was created with version 3.2.1, however, your version is 3.0.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorğunuzu daxil edin: Torpaqlar işğaldan azad edildikdə bunun implications nə ola bilər\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Apps\\python_envs\\cuda_gpu_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final token count: 651\n",
      "\n",
      "Torpaqlar işğaldan azad edildikdə bir neçə hüquqi və iqtisadi nəticələr ortaya çıxa bilər. Bu nəticələr arasında iqtisadi inkişafın təşviq edilməsi, mülkiyyət hüquqlarının bərpası və ərazilərin yenidən qurulması aspektləri mühüm yer tutur. İşğaldan azad edilmiş ərazilərdə fəaliyyət göstərən sahibkarlar üçün dövlət tərəfindən müəyyən maliyyə resurslarına çıxış imkanları təmin edilir. Belə ki, dövlət zəmanəti ilə kreditlərin verilməsi və kredit faizlərinin subsidiyalaşdırılması kimi mexanizmlər tətbiq edilə bilər. Bu barədə daha ətraflı \"İşğaldan azad edilmiş ərazilərdə fəaliyyət göstərən sahibkarlıq subyektlərinə verilən kreditlərin dövlət zəmanəti ilə təmin edilməsi və kredit faizlərinin subsidiyalaşdırılması Qaydası\"nda məlumat verilir. Siz [bu keçiddən](https://e-qanun.az/framework/53147) bu qaydalar haqqında daha ətraflı oxuya bilərsiniz. \n",
      "\n",
      "Həmçinin, işğaldan azad edilmiş ərazilərdə mülkiyyət hüquqlarının bərpası məsələləri də önə çəkilir. Bu çərçivədə, ərazilərin yenidən qurulması, infrastruktur layihələrinin həyata keçirilməsi və yerli mülkiyyət sahiblərinin hüquqlarının tanınması məsələləri diqqət mərkəzindədir. Belə proseslər hüquqi çərçivədə təşkil edilərək mülkiyyətçəlik hüquqlarının bərpasına və bu ərazilərin iqtisadi həyatının canlanmasına yönəldir.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from decouple import Config, RepositoryEnv\n",
    "\n",
    "# Paths to model and corpus file\n",
    "model_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Fine-Tuning\\msmarco-bert-base-dot-v5-cosine\\final\"\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus_embed.parquet\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load the embeddings, texts, urls, and names from the corpus file\n",
    "corpus_df = pd.read_parquet(corpus_path)\n",
    "embeddings = np.array(corpus_df['embeddings'].tolist())\n",
    "texts = corpus_df['text'].tolist()\n",
    "urls = corpus_df['url'].tolist()\n",
    "names = corpus_df['name'].tolist()\n",
    "\n",
    "# Specify the path to the .env file\n",
    "env_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\GitHub (Private)\\.env\"\n",
    "config = Config(RepositoryEnv(env_path))\n",
    "\n",
    "# Fetch the OpenAI API key\n",
    "openai_api_key = config('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the OpenAI client using the API key from .env\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Define a function to clean the query by removing symbols and numbers at the end\n",
    "def clean_query(query):\n",
    "    return re.sub(r'[\\d!?.,;]*$', '', query).strip()\n",
    "\n",
    "# Define a threshold for similarity score\n",
    "SIMILARITY_THRESHOLD = 0.75  # Adjust the threshold value as needed\n",
    "\n",
    "# Define a function to compute semantic similarity\n",
    "def compute_similarity(query, embeddings, top_k=20):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "    similarities = util.cos_sim(query_embedding, embeddings).cpu().numpy().flatten()\n",
    "    results = []\n",
    "    seen_texts = set()\n",
    "    for i, score in enumerate(similarities):\n",
    "        if score >= SIMILARITY_THRESHOLD and texts[i] not in seen_texts:\n",
    "            seen_texts.add(texts[i])\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"text\": texts[i],\n",
    "                \"similarity_score\": score,\n",
    "                \"url\": urls[i],\n",
    "                \"name\": names[i]\n",
    "            })\n",
    "    results = sorted(results, key=lambda x: x[\"similarity_score\"], reverse=True)[:top_k]\n",
    "    return results\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text, model=\"gpt-4o-2024-08-06\"):\n",
    "    tokenizer = tiktoken.encoding_for_model(model)\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Define the maximum allowed tokens for the input (adjust based on your limits)\n",
    "MAX_TOKENS = 30_000\n",
    "\n",
    "# Define the main function for handling searches and generating the final response\n",
    "def handle_user_query(user_input):\n",
    "    # Clean the query\n",
    "    cleaned_query = clean_query(user_input)\n",
    "\n",
    "    # Set initial values for top_k and unlimited text_search_limit\n",
    "    top_k = 20\n",
    "    text_search_limit = None  # Start with unlimited search limit\n",
    "\n",
    "    # Try to generate a prompt and catch RateLimitError\n",
    "    while True:\n",
    "        try:\n",
    "            # Conduct text search with initial unlimited limit\n",
    "            text_search_results = corpus_df[corpus_df['text'].str.contains(cleaned_query, case=False)]\n",
    "            if text_search_limit is not None:\n",
    "                text_search_results = text_search_results[:text_search_limit]\n",
    "\n",
    "            # Run similarity search with current top_k and store results in a DataFrame\n",
    "            similarity_results = compute_similarity(cleaned_query, embeddings, top_k=top_k)\n",
    "            similarity_df = pd.DataFrame(similarity_results)\n",
    "\n",
    "            # Combine results and remove duplicates based on 'text', 'url', and 'name' columns only\n",
    "            combined_results = pd.concat([text_search_results, similarity_df]).drop_duplicates(subset=['text', 'url', 'name'])\n",
    "\n",
    "            # Construct the prompt with combined results\n",
    "            prompt = f\"User Query: {cleaned_query}\\n\\nRetrieved Text Passages:\\n\\n\"\n",
    "            for _, row in combined_results.iterrows():\n",
    "                prompt += f\"Document Name: {row['name']}\\nText: {row['text']}\\nURL: {row['url']}\\n\" + \"-\" * 50 + \"\\n\"\n",
    "\n",
    "            # Count tokens in the prompt\n",
    "            token_count = count_tokens(prompt)\n",
    "\n",
    "            # Check if token count is within the limit\n",
    "            if token_count <= MAX_TOKENS:\n",
    "                print(f\"Final token count: {token_count}\")\n",
    "                print()\n",
    "                break  # Exit the loop if within limit\n",
    "\n",
    "            # If not within limit, set text_search_limit if not set, then reduce both parameters\n",
    "            print(f\"Token count {token_count} exceeds limit. Reducing top_k and applying text_search_limit...\")\n",
    "            top_k = max(5, top_k - 5)\n",
    "            if text_search_limit is None:\n",
    "                text_search_limit = 10  # Apply an initial limit if previously unlimited\n",
    "            else:\n",
    "                text_search_limit = max(5, text_search_limit - 5)\n",
    "\n",
    "        except openai.error.RateLimitError as e:\n",
    "            print(f\"Rate limit exceeded: {e}. Adjusting input and retrying...\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are an Azerbaijani lawyer who provides legal advice strictly based on Azerbaijani legislation. Always respond in Azerbaijani. When answering questions, refer to relevant legal acts by the URL and their specific clauses, and provide clear references to the exact articles or sections that apply to the user's query. If there are no relevant text passages among retrieved ones, discard them and base your answer on your own to address the user query!\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract and return the final response\n",
    "    gpt_response = response.choices[0].message.content\n",
    "    return gpt_response\n",
    "\n",
    "# Test with user input\n",
    "user_query = input(\"Sorğunuzu daxil edin: \")\n",
    "print()\n",
    "final_response = handle_user_query(user_query)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorğunuzu daxil edin: Azərbaycan Respublikasının torpaqlarının erməni işğalından azad edilməsinin hüquqi nəticələri hansılardır?\n",
      "\n",
      "Final token count: 2462\n",
      "\n",
      "Azərbaycan torpaqlarının erməni işğalından azad edilməsinin hüquqi nəticələri Azərbaycan Respublikasının qanunvericilik aktlarına və beynəlxalq hüquq normalarına əsaslanır. Bu nəticələr arasında aşağıdakıları qeyd etmək olar:\n",
      "\n",
      "1. **Ərazi bütövlüyünün bərpası və dövlət suverenliyinin tam təmin edilməsi**. Öz ərazilərinin idarə olunmasının və bu ərazilərin inkişafının tam şəkildə Azərbaycanın qanuni hakimiyyəti tərəfindən həyata keçirilməsi təmin olunacaq.\n",
      "\n",
      "2. **Qaçqınların və məcburi köçkünlərin öz yurdlarına qayıtması**. İşğaldan azad edilmiş ərazilərə yenidən qayıdan məcburi köçkünlərin yaşayış şəraitinin yaxşılaşdırılması və bu ərazilərdə təsərrüfat fəaliyyətinin bərpası üçün müxtəlif dövlət səviyyəsində proqramlar hazırlanır və həyata keçirilir. Qaçqınların və məcburi köçkünlərin yaşayış şəraitinin yaxşılaşdırılması və məşğulluğunun artırılması üzrə Dövlət Proqramı müvafiq dəyişikliklərlə qüvvəyə minmişdir ([URL](https://e-qanun.az/framework/13956)).\n",
      "\n",
      "3. **Daşınmaz əmlakın və torpaq sahələrinin qanuni qeydiyyatı və istifadəsi**. Torpaqların qeydiyyatı və istifadəsi Azərbaycan Respublikasının Torpaq Məcəlləsi və digər məsələlərə dair müvafiq qanunvericilik aktları ilə tənzimlənir. Torpaq bazarı ilə bağlı məsələlərə aid normativ aktlar mövcuddur ([URL](https://e-qanun.az/framework/3082)).\n",
      "\n",
      "4. **Əmlak hüquqlarının bərpası**. İşğaldan öncə mövcud olan və ya işğal dövründə yaranmış əmlak hüquqi problemlərin həlli zəruridir və bu məsələlər müvafiq qanunvericilik yolu ilə həll edilir.\n",
      "\n",
      "5. **Qanun və nizamın bərpası**. Azad edilmiş ərazilərdə tam qanun və nizamın bərpa olunması üçün hüquqi çərçivə və dövlət qurumlarının fəaliyyəti intensiv şəkildə həyata keçirilir.\n",
      "\n",
      "Qeyd edilən hüquqi nəticələr Azərbaycan Respublikasının ərazi bütövlüyünü və suverenliyini təmin etmək üçün mühüm əhəmiyyət kəsb edir və müxtəlif qanunvericilik aktları ilə hüquqqa əsaslanır.\n"
     ]
    }
   ],
   "source": [
    "# Test with user input\n",
    "user_query = input(\"Sorğunuzu daxil edin: \")\n",
    "print()\n",
    "final_response = handle_user_query(user_query)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorğunuzu daxil edin: İcra hərəkətlərinin həyata keçirilməsi müddətləri?\n",
      "\n",
      "Final token count: 2567\n",
      "İcra hərəkətlərinin həyata keçirilməsi müddətləri barədə Azərbaycan Respublikasının qanunvericiliyinə əsasən, \"İcra haqqında\" Azərbaycan Respublikası Qanununun 12-ci maddəsinə əsasən icra məmuru icra sənədini aldığı gündən iki ay müddətində bütün zəruri icra hərəkətlərini həyata keçirməlidir. Bu müddət yalnız istisna hallardan başqa bütün icra sənədlərinə aiddir. \n",
      "\n",
      "Maddə 12.2. bəzi xüsusi icra sənədlərinin dərhal icra edilməsini nəzərdə tutur, bunlara alimentlərin, əmək haqqının və qanunsuz işdən çıxarılmış işçinin işə bərpası haqqında əmrlər daxildir. Həmin sənədlər dərhal icra olunmalıdır.\n",
      "\n",
      "Daha ətraflı məlumat əldə etmək üçün \"İcra haqqında\" Qanunun tam mətni ilə tanış ola bilərsiniz: [İcra haqqında Qanun](https://e-qanun.az/framework/1406).\n"
     ]
    }
   ],
   "source": [
    "# Test with user input\n",
    "user_query = input(\"Sorğunuzu daxil edin: \")\n",
    "print()\n",
    "final_response = handle_user_query(user_query)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load the Parquet file\n",
    "parquet_file_path = r'E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\finetune_data.parquet'\n",
    "parquet_df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "# Connect to the SQLite database\n",
    "sqlite_db_path = r'E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\db.sqlite3'\n",
    "conn = sqlite3.connect(sqlite_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_text</th>\n",
       "      <th>gpt_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>miras</td>\n",
       "      <td>Miras hüququ ilə bağlı suallarınıza cavab verə...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mirasda məcburi pay</td>\n",
       "      <td>Azərbaycan Respublikası Mülki Məcəlləsinə əsas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>informasiya təhlükəsizliyi</td>\n",
       "      <td>Azərbaycan Respublikasının qanunvericiliyinə g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>valideyn hüququ</td>\n",
       "      <td>Ailə Məcəlləsinə (AM) əsasən, valideyn hüquqla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mirasda məcburi pay</td>\n",
       "      <td>Azərbaycan Respublikasının Mülki Məcəlləsinə ə...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query_text  \\\n",
       "0                       miras   \n",
       "1         mirasda məcburi pay   \n",
       "2  informasiya təhlükəsizliyi   \n",
       "3             valideyn hüququ   \n",
       "4         mirasda məcburi pay   \n",
       "\n",
       "                                        gpt_response  \n",
       "0  Miras hüququ ilə bağlı suallarınıza cavab verə...  \n",
       "1  Azərbaycan Respublikası Mülki Məcəlləsinə əsas...  \n",
       "2  Azərbaycan Respublikasının qanunvericiliyinə g...  \n",
       "3  Ailə Məcəlləsinə (AM) əsasən, valideyn hüquqla...  \n",
       "4  Azərbaycan Respublikasının Mülki Məcəlləsinə ə...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etibarnamə</td>\n",
       "      <td>Etibarnamə, bir şəxsin (etibar edən) digər şəx...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>etibarnamə</td>\n",
       "      <td>Maddə 363. Etibarnamənin müddəti 363.1. Bu Məc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i̇stehlakcı haqqında</td>\n",
       "      <td>Maddə 452. Fiziki şəxslərə dərman vasitələrini...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i̇stehlakcı haqqında</td>\n",
       "      <td>Maddə 746-2. İstehlak krediti müqaviləsinin ba...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i̇stehlakcı haqqında</td>\n",
       "      <td>Maddə 1071. İstiqraz şəklində borc öhdəliyində...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  query                                               text  \\\n",
       "0            etibarnamə  Etibarnamə, bir şəxsin (etibar edən) digər şəx...   \n",
       "1            etibarnamə  Maddə 363. Etibarnamənin müddəti 363.1. Bu Məc...   \n",
       "2  i̇stehlakcı haqqında  Maddə 452. Fiziki şəxslərə dərman vasitələrini...   \n",
       "3  i̇stehlakcı haqqında  Maddə 746-2. İstehlak krediti müqaviləsinin ba...   \n",
       "4  i̇stehlakcı haqqında  Maddə 1071. İstiqraz şəklində borc öhdəliyində...   \n",
       "\n",
       "   label  \n",
       "0    1.0  \n",
       "1    1.0  \n",
       "2    1.0  \n",
       "3    1.0  \n",
       "4    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load existing data from SQLite\n",
    "sqlite_df = pd.read_sql_query(\"SELECT query_text, gpt_response FROM ai_queries_userquery\", conn)\n",
    "display(sqlite_df.head())\n",
    "display(parquet_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['etibarnamə', 'i̇stehlakcı haqqında', 'vətəndaşlıq',\n",
       "       'vətəndaş hüquqları', 'məhkəmə', 'mülki iş', 'iddia müddəti',\n",
       "       'kassasiya şikayəti', 'müqavilə', 'aliment', 'uşaq hüququ',\n",
       "       'nikah', 'qəyyumluq', 'valideyn hüququ', 'cinayət məcəlləsi',\n",
       "       'cinayət təqibi nədir', 'analıq məzuniyyəti',\n",
       "       'ər-arvadın birgə mülkiyyəti',\n",
       "       'vəsiyyatnamə üzrə vərəsəliyin açılması',\n",
       "       'qeyri hökumət təşkilatının ləğvi necə olur?', 'boşanma',\n",
       "       'yol nişanını xətti pozmağa görə cərimə', 'nikahın ləğvi',\n",
       "       'lizinq obyektinin geri qaytarilmasi', 'əqdin etibarsizligi',\n",
       "       'mirasda mecburi pay', 'ekspert rəyi sübut hesab olunurmu?',\n",
       "       'məhkəməyə müraciət qaydası', 'qəsdən adam öldürmə',\n",
       "       'alimentin ödənilməsi', 'girov',\n",
       "       'notarial qaydada təsdiqlənməsi məcburi olan müqavilələrin siyahısı',\n",
       "       'istintaqın aparılması', 'vərəsəlik hüququ', 'yaşamaq hüququ',\n",
       "       'birgə mülkiyyət', 'hüquqi şəxsin yenidən təşkili',\n",
       "       'prokurorluq orqanı hansı funksiyanı yerinə yetirir',\n",
       "       'paylı mülkiyyətim qanunsuz dağıdılıb',\n",
       "       'xaricdən maşın gətirəndə nə lazımdır?',\n",
       "       'i̇nformasiya təhlükəsizliyi', 'daşınmaz əmlakın qeydiyyatı',\n",
       "       '\"dar təfsir\"',\n",
       "       'utilizasiya nəticəsində əldə edilmiş yararlı material və əşyalar',\n",
       "       'prezidentin səlahiyyətləri', 'daimi yaşamaq hüququ',\n",
       "       'media və kütləvi informasiya', 'kino çəkməyin dövlət reestri',\n",
       "       'fərdi məlumatların informasiya ehtiyatlarının formalaşdırılması və informasiya sistemlərinin yaradılması, onlara xidmətlərin göstərilməsinə lisenziya verilməsi üçün tələb olunan sənədlər',\n",
       "       'lisenziyalar və icazələr haqqında',\n",
       "       'hamiləliyə və doğuşa görə məzuniyyətlər', 'aliment tələbi',\n",
       "       'notariat qaydasında təsdiq edilmiş müqavilə',\n",
       "       'informasiya və media haqqında bütün hüquqi aktların siyahısını ver',\n",
       "       'vətəndaş sorğularına dövlət qurumunun cavabı',\n",
       "       'i̇ctimai iştirakçılığın təmin edilməsi',\n",
       "       'informasiya resurslarının tərtibatı necə olmalıdır',\n",
       "       'cinayət nə vaxt ağır hesab olunur?',\n",
       "       'hansı hallarda əqdlər etibarsız sayılır?',\n",
       "       'boşanma nəticəsində övlad kimin himayəsinə verilir?',\n",
       "       'boşanma haqqında ərizə forması yaz!',\n",
       "       'vərəsələyin açılması şərtləri', 'qanuni vərəsələr',\n",
       "       'icra hərəkətləri hansı müddətə başa çatdırılmalıdır?',\n",
       "       'borclunun ölkədən getmək hüququnun müvəqqəti məhdudlaşdırılması və ölkədən çıxarılması proseduru necədir?'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(parquet_df['query'].unique())\n",
    "len(parquet_df['query'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find new rows to add (exclude duplicates)\n",
    "new_data = parquet_df.merge(sqlite_df, on=['query_text', 'gpt_response'], how='left', indicator=True)\n",
    "new_data = new_data[new_data['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# Append new rows to SQLite\n",
    "new_data.to_sql('ProductionQuery', conn, if_exists='append', index=False)\n",
    "\n",
    "print(f\"Added {len(new_data)} new rows to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus_path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus_embed.parquet\"\n",
    "\n",
    "df = pd.read_parquet(corpus_path)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter text passages where there is no words or only numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['text'] == '– – – – digərləri: 4407 29610 0 – – – –']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern to match entries with exactly one word, mixed with numbers and symbols\n",
    "pattern = r'^(?:[^\\w]*\\b\\w+\\b[^\\w\\d\\s]*)?(?:[\\d.,–:\\s-]*\\b\\w+\\b[\\d.,–:\\s-]*){1}$'\n",
    "\n",
    "# Filter out rows that match the pattern\n",
    "df = df[~df['text'].str.contains(pattern, regex=True)]\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern to match rows with only numbers and symbols\n",
    "pattern = r'^[\\d\\s.,–-]*$'\n",
    "\n",
    "# Filter out rows that match the pattern\n",
    "df = df[~df['text'].str.contains(pattern)]\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern to match rows with only numbers, dots, commas, or dashes (and possibly spaces between them)\n",
    "pattern = r'^(?:[\\d.,\\-\\s]+)$'\n",
    "\n",
    "# Filter out rows that match the pattern\n",
    "df = df[~df['text'].str.contains(pattern)]\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword-Based Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all text\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.loc[df['text'].str.contains(\"icra hərəkətləri\", case=False)]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[s['name'] == 'İcra haqqında']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[251274]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"icra hərəkətləri\", \"basa çatdırılma müddəti\", \"icra müddəti\", \"icra prosesi\", \"müddət tələbləri\", \"icra vaxtı\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(corpus_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'icra hərəkətləri neçə günə başa çatdırılmalıdır?'\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(query, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = corpus_df['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for similarity score (e.g., 0.5)\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Function to compute similarity scores with batching and optimized cosine similarity\n",
    "def compute_similarity(query, texts, threshold=SIMILARITY_THRESHOLD):    \n",
    "    # Compute cosine similarities between the query and all texts at once\n",
    "    cosine_scores = util.cos_sim(query_embedding, text_embeddings).cpu().numpy().flatten()\n",
    "    \n",
    "    # Collect results that meet the threshold\n",
    "    results = [\n",
    "        {\"query\": query, \"text\": texts[i], \"similarity_score\": score}\n",
    "        for i, score in enumerate(cosine_scores) if score >= threshold\n",
    "    ]\n",
    "\n",
    "    # Sort results by similarity score in descending order\n",
    "    results = sorted(results, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "# Run similarity computation\n",
    "results = compute_similarity(query, texts)\n",
    "\n",
    "# Display sorted results that meet the threshold\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Similarity Score: {result['similarity_score']:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "cosine_score = util.cos_sim(query_embedding, text_embedding).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df['text'].iloc[277320]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's Tokenizer Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer(\"sentence-transformers/msmarco-bert-base-dot-v5\")\n",
    "\n",
    "# Access the tokenizer\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Define sample texts\n",
    "texts = [\"cinayət təqibi nədir?\", \"cinayət məsuliyyəti\", \"işgəncə vermək\", \"QARABAĞA QAYIDIŞ!\"]\n",
    "\n",
    "# Tokenize each text and display tokens\n",
    "for text in texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"E:\\Software\\Data Science and AI\\NLP\\Edliyye\\Legal Acts Question Answering\\Data Collection\\corpus.parquet\"\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count for each row in the 'text' column\n",
    "#df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display basic statistics for the word count column\n",
    "word_count_stats = df['word_count'].describe()\n",
    "display(word_count_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where word count is less than 7\n",
    "#df = df.loc[df['word_count'] > 7].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['word_count'] <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[523333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['text'] == '']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# New text without the unwanted part\n",
    "new_text = ''\n",
    "\n",
    "# Assign new text to the specified row\n",
    "df.at[131062, 'text'] = new_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display(df['type'].value_counts().head(41))\n",
    "display(df['type'].value_counts().tail(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referendum aktları\n",
    "Konstitusiya\n",
    "AZƏRBAYCAN RESPUBLİKASININ KONSTİTUSİYA QANUNLARI\n",
    "Məcəllələr\n",
    "Qanunlar\n",
    "\n",
    "AZƏRBAYCAN RESPUBLİKASI NAZİRLƏR KABİNETİNİN QƏRARLARI\n",
    "AZƏRBAYCAN RESPUBLİKASI PREZİDENTİNİN FƏRMANLARI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate entries with the pattern \"– çıxarılmışdır\" in the text column\n",
    "df.loc[df['text'].str.contains(r\"ləğv edilmişdir\", regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].iloc[133057]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Specify indices of rows to remove\n",
    "indices_to_remove = [131656, 131657, 131744, 131751, 131758, 131954, 131955, 131960]\n",
    "\n",
    "# Drop the specified rows and reset the index\n",
    "df = df.drop(indices_to_remove).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Display basic statistics for the word count column\n",
    "word_count_stats = df['word_count'].describe()\n",
    "display(word_count_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['word_count'] > 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[30265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure you have the sentence tokenizer data\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "# Sample the first 5 rows of the DataFrame\n",
    "#sample_df = df.head(5)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "expanded_rows = []\n",
    "\n",
    "# Iterate over each row\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    sentences = sent_tokenize(text)  # Split text into sentences\n",
    "    temp_text = []  # Temporary list to accumulate sentences\n",
    "    word_count = 0  # Initialize word count for the current segment\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Calculate word count of the sentence\n",
    "        sentence_word_count = len(sentence.split())\n",
    "        \n",
    "        # Check if adding this sentence will exceed the 100-word limit\n",
    "        if word_count + sentence_word_count > 100:\n",
    "            # Append the current segment as a new row\n",
    "            new_row = row.copy()  # Copy original row data\n",
    "            new_row['text'] = ' '.join(temp_text)  # Join accumulated sentences\n",
    "            expanded_rows.append(new_row)\n",
    "            \n",
    "            # Reset for the next segment\n",
    "            temp_text = []\n",
    "            word_count = 0\n",
    "        \n",
    "        # Add the sentence to the current segment\n",
    "        temp_text.append(sentence)\n",
    "        word_count += sentence_word_count\n",
    "    \n",
    "    # Add any remaining sentences as a new row\n",
    "    if temp_text:\n",
    "        new_row = row.copy()\n",
    "        new_row['text'] = ' '.join(temp_text)\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame from the expanded rows\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Remove rows with empty strings in the 'text' column\n",
    "expanded_df = expanded_df[expanded_df['text'].str.strip() != '']\n",
    "\n",
    "# Display the new DataFrame\n",
    "expanded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "expanded_df['word_count'] = expanded_df['text'].apply(lambda x: len(x.split()))\n",
    "display(expanded_df['word_count'].describe())\n",
    "\n",
    "display(expanded_df)\n",
    "expanded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to split text into 100-word segments\n",
    "def split_into_segments(text, segment_size=100):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+segment_size]) for i in range(0, len(words), segment_size)]\n",
    "\n",
    "# Apply the function and explode in-place if word count is 400 or more\n",
    "df['text'] = df.apply(lambda row: split_into_segments(row['text']) if row['word_count'] >= 400 else row['text'], axis=1)\n",
    "df = df.explode('text', ignore_index=True)  # Explode to get each segment as a new row\n",
    "\n",
    "# Recalculate word count if needed\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Display the result\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(df)\n",
    "df.info()\n",
    "df.index\n",
    "\n",
    "word_count_stats = df['word_count'].describe()\n",
    "display(word_count_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['text'].iloc[0])\n",
    "display(expanded_df['text'].iloc[0])\n",
    "display(expanded_df['text'].iloc[1])\n",
    "display(expanded_df['text'].iloc[2])\n",
    "display(expanded_df['text'].iloc[3])\n",
    "display(expanded_df['text'].iloc[4])\n",
    "display(expanded_df['text'].iloc[5])\n",
    "display(expanded_df['text'].iloc[6])\n",
    "\n",
    "display(expanded_df['text'].iloc[7])\n",
    "display(df['text'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(expanded_df['text'].iloc[11])\n",
    "display(expanded_df['text'].iloc[12])\n",
    "display(df['text'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.loc[expanded_df['text'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for the word count column\n",
    "word_count_stats = expanded_df['word_count'].describe()\n",
    "display(word_count_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertConfig, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as HFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the core path\n",
    "PATH_DATA = 'E:/Software/Data Science and AI/NLP/Edliyye/Legal Acts Question Answering/NLP project'\n",
    "PATH_VOCAB = 'E:/Software/Data Science and AI/NLP/Edliyye/Legal Acts Question Answering/GitHub (Private)/tokenizer_directory'\n",
    "\n",
    "text_path = os.path.join(PATH_DATA, 'full_qanun_text_chunks.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "base_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased') # Originally vocab_size=105_879\n",
    "\n",
    "# Load your custom tokenizer\n",
    "custom_tokenizer = BertTokenizer.from_pretrained(PATH_VOCAB) # vocab_size=80_342\n",
    "\n",
    "# Correct model_max_length if necessary\n",
    "custom_tokenizer.model_max_length = 512\n",
    "\n",
    "# Add new tokens from custom tokenizer to base tokenizer\n",
    "new_tokens = set(custom_tokenizer.get_vocab().keys()) - set(base_tokenizer.get_vocab().keys())\n",
    "base_tokenizer.add_tokens(list(new_tokens))\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Resize the model's embeddings to accommodate new tokens\n",
    "model.resize_token_embeddings(len(base_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Azerbaijani corpus\n",
    "df = pd.read_parquet(text_path)\n",
    "\n",
    "# Extract the text column\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts = train_test_split(texts, test_size=0.2)\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "base_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')  # Originally vocab_size=105_879\n",
    "\n",
    "# Load your custom tokenizer\n",
    "custom_tokenizer = BertTokenizer.from_pretrained(PATH_VOCAB)  # vocab_size=80_342\n",
    "\n",
    "# Ensure the custom tokenizer has a reasonable max length\n",
    "custom_tokenizer.model_max_length = 512\n",
    "\n",
    "# Add new tokens from custom tokenizer to base tokenizer\n",
    "new_tokens = set(custom_tokenizer.get_vocab().keys()) - set(base_tokenizer.get_vocab().keys())\n",
    "base_tokenizer.add_tokens(list(new_tokens))\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# Resize the model's embeddings to accommodate new tokens\n",
    "model.resize_token_embeddings(len(base_tokenizer))\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Ensure the model is on GPU\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_and_encode(texts):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for text in texts:\n",
    "        encoded = base_tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "train_input_ids, train_attention_masks = tokenize_and_encode(train_texts)\n",
    "val_input_ids, val_attention_masks = tokenize_and_encode(val_texts)\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(train_input_ids, train_attention_masks)\n",
    "val_dataset = CustomDataset(val_input_ids, val_attention_masks)\n",
    "\n",
    "# Data collator for MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=base_tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"none\"  # To disable logging reports\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine-tuned-model')\n",
    "base_tokenizer.save_pretrained('./fine-tuned-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(f'{PATH}/fine-tuned-model')\n",
    "base_tokenizer.save_pretrained(f'{PATH}/fine-tuned-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoding.input_ids[0] # Explicitly move the data to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TextDataset(texts, base_tokenizer)\n",
    "\n",
    "# Define a custom data collator to ensure tensors are on the correct device\n",
    "class DataCollatorWithDevice:\n",
    "    def __init__(self, data_collator, device):\n",
    "        self.data_collator = data_collator\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch = self.data_collator(examples)\n",
    "        # Move batch to the correct device\n",
    "        batch['input_ids'] = batch['input_ids'].to(self.device)\n",
    "        batch['attention_mask'] = batch['attention_mask'].to(self.device)\n",
    "        if 'labels' in batch:\n",
    "            batch['labels'] = batch['labels'].to(self.device)\n",
    "        return batch\n",
    "\n",
    "# Create the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=base_tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Wrap the data collator with the custom collator to handle device placement\n",
    "data_collator_with_device = DataCollatorWithDevice(data_collator, device)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=data_collator_with_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    dataloader_num_workers=4  # To speed up data loading\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator_with_device,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Verify CUDA Availability\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(f'{PATH}/fine-tuned-model')\n",
    "base_tokenizer.save_pretrained(f'{PATH}/fine-tuned-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Use the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(PATH, 'fine-tuned-model')\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 (CUDA GPU)",
   "language": "python",
   "name": "cuda_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
