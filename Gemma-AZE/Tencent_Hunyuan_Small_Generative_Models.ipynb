{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yqhlrqu0_kN-",
    "outputId": "fc22b24a-c8c4-4e63-d02a-94344b585c79"
   },
   "source": [
    "pip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594,
     "referenced_widgets": [
      "c5e362c588da43b88371b52a68600258",
      "38bff1e0215e427fa6af64380f48f7cc",
      "46f0ee118241495c90e6f4a4e7a382d4",
      "7ab6709c2d674904992f88aaa20e34e5",
      "8260debfd96447df83609e1c815c95ea",
      "3ac9cf388842491487d876d0ea0d34b6",
      "ed6818b8750f46988f1049420aa0cf46",
      "e878131ba6ab4f60a67122fe41bcb720",
      "4b15e2fc2f0843409676e950655f9ec8",
      "a270d2d9e48544af953edd8bad2f08f0",
      "54a2a3ef941a479494c228a925188eba",
      "7622254ba664422b9b7985efa0c2794e",
      "0a898d8ea60e4e889f1afaf5d592cd23",
      "6c78636c9b694d54a9aaed939514c60e",
      "d4f1b753fc1649948cb687c2f2ee7423",
      "ba7207aa52394ad4b8781de4a7e3ee67",
      "731eeea39698407abadc0974b3c3a310",
      "d738c759dc7846ad8e557617d4dca718",
      "5f8e64b34cdf4be2bfc07be8d4e8a416",
      "ea0b59bc139a4386a9515c4d32658584",
      "bca193495ea84340aafa3d18954f8527",
      "82534b20ee7c45e7a62893785dd22b2a",
      "731e293005414b2ba8cb221b3787e8cb",
      "15b218ba3ef94c8c9b7bc6bc825380ed",
      "c004e3f2386d49968ba120b94e605336",
      "e2ea1eec68fd4330871018eaac603c5c",
      "ddfe36c02cce415689accaa2ea619bb6",
      "ff093751d9fd453fbe7157643fcf832e",
      "0166509ed7d14e72b0163fcb2b96c721",
      "9aa0ef68153e4c9699bebf32c94d2fae",
      "b16b09974b964f9f9804ce78f17ceb7e",
      "8795ef1c8cff47ef99ac560b6ff81c97",
      "cb93063ece3b4867b82592e0f403a3b0",
      "c9571206569b40e08f3612e2f7b548ec",
      "f371c6ba9a75459a9f51fe3594be2833",
      "0647fbb7109d4b0cb34622540070979d",
      "7ed6a1d991c34610ac82669442bfffd1",
      "fbfd602eae1140fd8ecee29d7ae0f9f8",
      "99ecc575bdba4d818c97af224351a0d6",
      "ecc90cba69a348dbb5fc1ef8f5be2d94",
      "bc748ddb8c09482d9a5428e31fff1429",
      "03d7405a06774dbb9d11675e87fddcf2",
      "9dd24fcf4ed54ea9b354df39fd876fc3",
      "f7fddc60c5b2403ab30cf8b2ad9f378e",
      "6742ab7d55ef496189254e35501c7f6f",
      "e01a745b7f2340d6866a611a7d8eaff8",
      "4a44376ffc854bc99a3a7f5f8ca61566",
      "263cfff0dd444afcad0e5705515145fe",
      "58019e8909a7444a92339d06e28b5b14",
      "7229f479031a4841871d0b619824ab8e",
      "71d4f9eed4344ea09130a1545f88ddd7",
      "69238e073dd04ef98d675044c73cccb6",
      "0420ad9dae704fef891bd5c421280ace",
      "d57407dcc2ce4b9b89fea57a55140e7f",
      "585c4f3f8cef495ca873503d1a4707fb",
      "7fc2f01d4de2470b9554c771ac95be5a",
      "1f7439a85a8e481b9f5ab2d49704a1bd",
      "4b6d26de90a9400395883b567c3ef7ec",
      "47a6cdcb82f34569ba3293f77935571a",
      "07f46b35f3f84dad9a3e4b947dec78f0",
      "b84331321d6e4d72a1dfbeaf2de8c418",
      "f811c7c558d64746827aba3dcb080310",
      "417b784e845b4490a6e6649abe38ade5",
      "477b21384570492eae8a968a695c11a7",
      "32d3167f2bda4883970ba432d27a9e40",
      "b49084621ac64375a284d0ace63bae50"
     ]
    },
    "id": "YLSy4R4K9jrm",
    "outputId": "20da819f-1c11-42d3-b5cc-51ad70bdf2b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e362c588da43b88371b52a68600258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7622254ba664422b9b7985efa0c2794e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731e293005414b2ba8cb221b3787e8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9571206569b40e08f3612e2f7b548ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6742ab7d55ef496189254e35501c7f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1582: UserWarning: Current model requires 1342178560 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc2f01d4de2470b9554c771ac95be5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/221 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_text= <ÔΩúhy_begin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúhy_UserÔΩú>/no_think Salam! Az…ôrbaycanca cavab ver v…ô √∂z√ºn√º t…ôqdim et.<ÔΩúhy_AssistantÔΩú><think>\n",
      "\n",
      "</think>\n",
      "<answer>\n",
      "Az…ôrbaycanca cavab ver, √∂z√ºn√ºn t…ôqdim etmek istiyorum. üòä\n",
      "</answer><ÔΩúhy_place‚ñÅholder‚ñÅno‚ñÅ2ÔΩú>\n",
      "thinking_content:\n",
      "\n",
      "\n",
      "answer_content:Az…ôrbaycanca cavab ver, √∂z√ºn√ºn t…ôqdim etmek istiyorum. üòä\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import re\n",
    "\n",
    "model_name_or_path = \"tencent/Hunyuan-1.8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.bfloat16 # You may want to use bfloat16 and/or move to GPU here\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    #{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"/no_think Salam! Az…ôrbaycanca cavab ver v…ô √∂z√ºn√º t…ôqdim et.\"} # /no_think as prefix to disable thinking mode.\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    enable_thinking=False # Toggle thinking mode (default: True)\n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    tokenized_chat.to(model.device),\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    top_p=0.8,\n",
    "    repetition_penalty=1.05,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(\"output_text=\",output_text)\n",
    "think_pattern = r'<think>(.*?)</think>'\n",
    "think_matches = re.findall(think_pattern, output_text, re.DOTALL)\n",
    "\n",
    "answer_pattern = r'<answer>(.*?)</answer>'\n",
    "answer_matches = re.findall(answer_pattern, output_text, re.DOTALL)\n",
    "\n",
    "think_content = [match.strip() for match in think_matches][0]\n",
    "answer_content = [match.strip() for match in answer_matches][0]\n",
    "print(f\"thinking_content:{think_content}\\n\\n\")\n",
    "print(f\"answer_content:{answer_content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wBrP5trAPeX",
    "outputId": "17939eb5-7abc-420e-c25e-2aed160001ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_text= <ÔΩúhy_begin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant.<ÔΩúhy_place‚ñÅholder‚ñÅno‚ñÅ3ÔΩú><ÔΩúhy_UserÔΩú>/no_think Salam! Nec…ôs…ôn?! Az…ôrbaycanca bilirs…ôn?<ÔΩúhy_AssistantÔΩú><think>\n",
      "\n",
      "</think>\n",
      "<answer>\n",
      "Salam! Nec…ôs…ôn? Az…ôrbaycanca bilirs…ôn? üòä\n",
      "</answer><ÔΩúhy_place‚ñÅholder‚ñÅno‚ñÅ2ÔΩú>\n",
      "thinking_content: \n",
      "\n",
      "answer_content: Salam! Nec…ôs…ôn? Az…ôrbaycanca bilirs…ôn? üòä\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# === CHANGE THIS ===\n",
    "new_message_content = \"/no_think Salam! Nec…ôs…ôn?! Az…ôrbaycanca bilirs…ôn?\"\n",
    "\n",
    "# Update messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": new_message_content}\n",
    "]\n",
    "\n",
    "# Tokenize with thinking disabled\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    tokenized_chat.to(model.device),\n",
    "    max_new_tokens=2048,\n",
    "    #do_sample=True,\n",
    "    #top_k=20,\n",
    "    #top_p=0.8,\n",
    "    #repetition_penalty=1.05,\n",
    "    #temperature=0.7\n",
    ")\n",
    "\n",
    "# Decode & Extract\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "print(\"output_text=\", output_text)\n",
    "\n",
    "# Extract thinking and answer\n",
    "think_matches = re.findall(r'<think>(.*?)</think>', output_text, re.DOTALL)\n",
    "answer_matches = re.findall(r'<answer>(.*?)</answer>', output_text, re.DOTALL)\n",
    "\n",
    "think_content = think_matches[0].strip() if think_matches else \"\"\n",
    "answer_content = answer_matches[0].strip() if answer_matches else \"\"\n",
    "\n",
    "print(f\"thinking_content: {think_content}\\n\")\n",
    "print(f\"answer_content: {answer_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw6VsZa4K3H2"
   },
   "source": [
    "The evaluation of **Tencent's Hunyuan Small Generative Model (1.8B parameters)** demonstrated valuable observations regarding its Azerbaijani language handling, even in its pre-trained form:\n",
    "\n",
    "1. **Tokenizer Coverage in Azerbaijani**: Despite being a small-scale model with just 1.8 billion parameters, Hunyuan's tokenizer includes Azerbaijani tokens, allowing the model to process and recognize Azerbaijani text inputs at the token level.\n",
    "\n",
    "2. **Fallback Behavior ‚Äì Prompt Repetition**: When the model lacks sufficient pre-training exposure to Azerbaijani, it tends to repeat or slightly modify the user‚Äôs prompt as a default response. This \"prompt echoing\" is a common fallback mechanism in under-tuned small generative models.\n",
    "\n",
    "3. **Instruction Tuning as a Path to Enhancement**: By applying instruction fine-tuning on a high-quality, curated Azerbaijani dataset, we can potentially teach the model to follow instructions more reliably and generate contextually appropriate responses. Such targeted tuning would significantly enhance its utility for Azerbaijani language applications.\n",
    "\n",
    "4. **Scalability with Larger Hunyuan Variants**: The larger Hunyuan models (4B, 7B parameters), while still lightweight compared to large-scale FMs, are expected to provide richer linguistic capabilities and context handling. They offer a promising balance between computational efficiency and model performance for low-resource languages like Azerbaijani.\n",
    "\n",
    "5. **Consumer Hardware Deployment**: Hunyuan‚Äôs small generative models are optimized for efficient inference on resource-constrained hardware. They can operate on single-GPU servers and even CPUs, making them viable for deployment on personal devices like laptops, desktops, and potentially smartphones with further optimization.\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "The **Hunyuan small model series** represents a compelling foundation for building **Azerbaijani-centric AI applications**, provided they are instruction-tuned on domain-specific datasets. Their lightweight design aligns well with edge deployment scenarios, allowing the development of localized, private AI services without heavy infrastructure dependencies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
