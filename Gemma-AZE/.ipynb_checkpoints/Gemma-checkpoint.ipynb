{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24a7fa4-9c3d-405f-9ca2-3ade4dd7128b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83f35a49-82b7-4caa-8cd9-1a1e5391cf3d",
   "metadata": {},
   "source": [
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec37b6-d9c2-4bb2-ab79-d0f0c2e67ef2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadff5a5-a0cb-44c4-889c-f673b56de0d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPU version"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bff441b-250e-41c4-9309-1c0499d27042",
   "metadata": {},
   "source": [
    "# GPU\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d6baa-db7d-42df-a13d-ba0f60fe6f16",
   "metadata": {},
   "source": [
    "## Text-to-Text (Chat Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4d8601-bd8e-46c8-bcfc-7616f46b62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank Zəng Mərkəzinə Xoş Gəlmisiniz! (Çıxmaq üçün 'exit' yazın)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Müştəri:  Salam! Bura bankdı?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator: Salam! Həyat necə gedir? Mən yaxşıyam, sən necəsən?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Müştəri:  Bankdakı kartımın balansını necə öyrənə bil\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator: Bankdakı kartının balansını öyrənmək üçün bir neçə yol var:\n",
      "\n",
      "**1. Mobil bankçılıq:**\n",
      "\n",
      "*   Çoğunluq bankları mobil bankçılıq tətbiqi (örneğin, Bankassistent, MobilBank, PayFix) təqdim edir. Bu tətbiqlərə giriş edərək, kartınızın balansını və digər məlumatları necə görmək üçün, bankın tətbiqi üzrə göstərilən göstərişləri izləmək lazımdır.\n",
      "\n",
      "**2. İnternet bankçılığı:**\n",
      "\n",
      "*   Bankın internet bankçılığı portalına (örneğin, [https://www.bankofamerica.com/](https://www.bankofamerica.com/)) giriş edin. Onlayn hesabınıza daxil olun və kartınızın balansını orada görməyiniz mümkünd\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Müştəri:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Çıxış edilir. Sağ olun!\n"
     ]
    }
   ],
   "source": [
    "# CPU (no need for Triton)\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load token from file and set as environment variable\n",
    "with open('Gemma-Token.txt', 'r') as f:\n",
    "    hf_token = f.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# Login using the environment variable\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=-1,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Initialize chat history with system prompt (optional)\n",
    "chat_history = \"\"\n",
    "\n",
    "print(\"Bank Zəng Mərkəzinə Xoş Gəlmisiniz! (Çıxmaq üçün 'exit' yazın)\")\n",
    "\n",
    "while True:\n",
    "    # Get user input in Azerbaijani\n",
    "    user_input = input(\"Müştəri: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Çıxış edilir. Sağ olun!\")\n",
    "        break\n",
    "    \n",
    "    # Add user message to chat history\n",
    "    chat_history += f\"<start_of_turn>user\\n{user_input}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "    # Generate model response\n",
    "    output = pipe(chat_history, max_new_tokens=200)\n",
    "\n",
    "    # Extract model's response after <start_of_turn>model\n",
    "    response_text = output[0]['generated_text'].split('<start_of_turn>model')[-1].strip()\n",
    "\n",
    "    # Print response\n",
    "    print(f\"Operator: {response_text}\")\n",
    "\n",
    "    # Update chat history with model's reply for context\n",
    "    chat_history += f\"{response_text}\\n<end_of_turn>\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fcc683-d609-4516-96d9-1c9009c7c17b",
   "metadata": {},
   "source": [
    "## Text-to-Text (Single-Prompt Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7f3a82-4e5a-4276-8a7f-6771651e8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d11db92a3614be191a9106f197c35e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator: Kredit limitinizi artırmaq üçün, kredit kartınızın müştərilik müqaviləsində göstərilən prosedur üzrə müraciət edə bilərsiniz. Əlavə kredit limitləri üzərində danışıqlar aparmaq üçün bank şöbəsinə başvura bilərsiniz. Əlavə olaraq, kredit limitinizin artırılması üçün əlavə təminat göstərməyiniz də ehtiyac ola bilər. Ətraflı məlumat üçün bank şöbəsinə müraciət etməyə dəvət edirəm.\n"
     ]
    }
   ],
   "source": [
    "# CPU (no need for Triton)\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load token from file and set as environment variable\n",
    "with open('Gemma-Token.txt', 'r') as f:\n",
    "    hf_token = f.read().strip()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "# Login using the environment variable\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    device=-1,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Define the structured message prompt (text-only)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Sən bir bank zəng mərkəzi operatorusan. Müştəri suallar verir, sən nəzakətli cavab verirsən.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Müştəri: Salam, mənim kartımın balansını necə yoxlaya bilərəm?\"},\n",
    "            {\"type\": \"text\", \"text\": \"Operator: Kart balansınızı yoxlamaq üçün mobil bank tətbiqinizə daxil olun və ya *123# yığın.\"},\n",
    "            {\"type\": \"text\", \"text\": \"Müştəri: Kredit limitimi necə artıra bilərəm?\"},\n",
    "            {\"type\": \"text\", \"text\": \"Operator:\"}  # This is where the model will continue.\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate the response\n",
    "output = pipe(text=messages, max_new_tokens=200)\n",
    "\n",
    "# Extract and print the model's reply\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b0dcb-12cb-4a3b-a63e-05bcf01d6280",
   "metadata": {},
   "source": [
    "## Image-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ed5304-ef7d-4355-8f9e-a6a1cf20b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfet üzərində quş var.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Sən köməkçi bir asistentsən.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"Konfet üzərində hansı heyvan var?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=200)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f776dd-9df8-4f0d-9b37-e6fac452ef00",
   "metadata": {},
   "source": [
    "## **Conclusion & Insights on Gemma 3-4B-IT for Azerbaijani Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3403c-9b55-4fbe-9f56-982563308435",
   "metadata": {},
   "source": [
    "Based on the code executions and outputs above, here are the **Conclusion & Insights**:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Lightweight Yet Capable Model**\n",
    "\n",
    "* The **Gemma 3-4B-IT** model, despite being just 4 billion parameters, was able to process and understand Azerbaijani prompts effectively.\n",
    "* It handled **both simple QA (single prompt completions)** and **contextual multi-turn dialogues (interactive chat loop)** without the need for a large memory footprint.\n",
    "* This performance was achieved running entirely on a **CPU-only environment**, without any GPU acceleration.\n",
    "* The inference times were still within a range acceptable for practical applications, proving that large-scale distributed compute is **not a necessity** for Azerbaijani language AI tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Strong Instruction-Following Ability in Azerbaijani**\n",
    "\n",
    "* The model correctly followed role-play instructions, acting as a **bank call center operator** when prompted, and responded in fluent Azerbaijani.\n",
    "* Even without explicit fine-tuning on Azerbaijani customer service data, its base multilingual capabilities were sufficient for structured conversation patterns.\n",
    "* Given this, **instruction-tuning with domain-specific datasets (Bank Call Center, Government Services, Customer Support, etc.)** would greatly enhance its domain expertise with minimal compute resources.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Single-GPU / CPU Deployment Feasibility**\n",
    "\n",
    "* The model’s **lightweight architecture** enables deployment on **single GPU systems (e.g., RTX 3090, 4090)** and even **CPU-only setups** for inference.\n",
    "* This eliminates the need for expensive **distributed cluster setups** or renting **high-end GPU servers** (e.g., A100/H100s) for Azerbaijani-language applications.\n",
    "* **On-premise deployment becomes viable**, especially for organizations (banks, government agencies, local businesses) that prioritize **data privacy** and **cost-efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Scalable for Localized AI Solutions**\n",
    "\n",
    "* The combination of **small model size, low infrastructure cost, and acceptable language capability** makes Gemma 3-4B-IT a strong foundation for localized AI products:\n",
    "\n",
    "  * **Bank Call Center Automation**\n",
    "  * **E-Government Citizen Services**\n",
    "  * **Azerbaijani Virtual Assistants**\n",
    "  * **Chatbots for Local Enterprises**\n",
    "* With focused **instruction tuning on Azerbaijani-specific datasets**, this model can be scaled across multiple domains without the need for large compute clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thought:\n",
    "\n",
    "> **Gemma 3-4B-IT shows that building high-quality Azerbaijani AI systems is now feasible with minimal compute resources. With instruction tuning and dataset curation, this model could power a wave of cost-effective, privacy-friendly, and locally deployed AI solutions in Azerbaijan.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb0748-e6d8-4180-a94f-e7aa57a4ed54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
