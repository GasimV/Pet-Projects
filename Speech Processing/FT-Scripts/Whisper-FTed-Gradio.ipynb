{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81a54c0-b0ea-4247-bc4d-99dca5d72271",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 30 seconds processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# Audio I/O\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "from transformers import (\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "\n",
    "# Run root that contains tokenizer/preprocessor files (tokenizer.json, preprocessor_config.json, etc.)\n",
    "RUN_ROOT = r\"...\"\n",
    "\n",
    "# Checkpoint folder that contains model weights/config (pytorch_model.bin / model.safetensors, config.json, etc.)\n",
    "CKPT_DIR = os.path.join(RUN_ROOT, \"...\")  # change to the checkpoint you want\n",
    "\n",
    "# -----------------------------\n",
    "# Load model & processor\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (device == \"cuda\") and torch.cuda.get_device_capability(0)[0] >= 8  # Hopper+\n",
    "dtype = torch.bfloat16 if use_bf16 else torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# Processor (tokenizer + feature extractor) lives in RUN_ROOT\n",
    "processor = WhisperProcessor.from_pretrained(RUN_ROOT)\n",
    "\n",
    "# Model weights live in the checkpoint directory\n",
    "model = WhisperForConditionalGeneration.from_pretrained(CKPT_DIR, torch_dtype=dtype)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Make sure generation settings are right for AZ transcription\n",
    "model.generation_config.language = \"az\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.generation_config.suppress_tokens = []\n",
    "\n",
    "SR = 16000  # Whisper expects 16kHz features\n",
    "\n",
    "def _load_audio_from_path(path: str, target_sr: int = SR) -> np.ndarray:\n",
    "    \"\"\"Load any audio file path -> float32 mono @ target_sr.\"\"\"\n",
    "    # soundfile preserves native sr; librosa resamples\n",
    "    wav, sr = sf.read(path, always_2d=False)\n",
    "    if wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)  # mono\n",
    "    if sr != target_sr:\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)\n",
    "    wav = wav.astype(np.float32)\n",
    "    # Clamp extreme amplitudes just in case\n",
    "    maxabs = np.max(np.abs(wav)) if wav.size else 0.0\n",
    "    if maxabs > 1.0:\n",
    "        wav = wav / maxabs\n",
    "    return wav\n",
    "\n",
    "def _to_float32(wav: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ensure float32 in [-1,1].\"\"\"\n",
    "    if np.issubdtype(wav.dtype, np.integer):\n",
    "        # common case: int16 mic input\n",
    "        max_int = np.iinfo(wav.dtype).max\n",
    "        wav = wav.astype(np.float32) / max_int\n",
    "    elif wav.dtype != np.float32:\n",
    "        wav = wav.astype(np.float32)\n",
    "    # clamp (just in case)\n",
    "    if wav.size:\n",
    "        m = np.max(np.abs(wav))\n",
    "        if m > 1.0:\n",
    "            wav /= m\n",
    "    return wav\n",
    "\n",
    "def _normalize_input(audio):\n",
    "    \"\"\"\n",
    "    Accepts: (sr, np.ndarray) from mic, np.ndarray, dict{'path':...}, or str path.\n",
    "    Returns: float32 mono @ 16 kHz.\n",
    "    \"\"\"\n",
    "    # mic: (sr, wav)\n",
    "    if isinstance(audio, tuple) and len(audio) == 2:\n",
    "        sr, wav = audio\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        wav = _to_float32(wav)             # <-- cast BEFORE resample\n",
    "        if sr != SR:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=SR)\n",
    "        return wav\n",
    "\n",
    "    # gradio v4 sometimes returns just a numpy array\n",
    "    if isinstance(audio, np.ndarray):\n",
    "        wav = audio\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        # we don't know the true sr; assume SR (UI forces 16k for mic)\n",
    "        return _to_float32(wav)\n",
    "\n",
    "    # file upload: dict with 'path'\n",
    "    if isinstance(audio, dict) and \"path\" in audio:\n",
    "        wav, sr = sf.read(audio[\"path\"], always_2d=False)\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        wav = _to_float32(wav)             # <-- cast BEFORE resample\n",
    "        if sr != SR:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=SR)\n",
    "        return wav\n",
    "\n",
    "    # raw string path\n",
    "    if isinstance(audio, str) and os.path.exists(audio):\n",
    "        wav, sr = sf.read(audio, always_2d=False)\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        wav = _to_float32(wav)\n",
    "        if sr != SR:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=SR)\n",
    "        return wav\n",
    "\n",
    "    raise ValueError(\"Unsupported audio input format\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def transcribe(audio_in):\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        wav = _normalize_input(audio_in)\n",
    "        if wav.size == 0:\n",
    "            return \"Empty audio.\"\n",
    "\n",
    "        # Feature extraction (log-Mel) via processor\n",
    "        feats = processor.feature_extractor(\n",
    "            wav, sampling_rate=SR, return_tensors=\"pt\"\n",
    "        ).input_features  # [1, 80, T]\n",
    "        feats = feats.to(device)\n",
    "\n",
    "        # Generate (greedy by default; tune max_length if needed)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=dtype) if device == \"cuda\" else torch.no_grad():\n",
    "            out_ids = model.generate(\n",
    "                inputs=feats,\n",
    "                max_length=448,           # consistent with your training\n",
    "                do_sample=False,\n",
    "                num_beams=3,\n",
    "            )\n",
    "\n",
    "        text = processor.tokenizer.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "        elapsed = time.time() - t0\n",
    "        return f\"{text}\\n\\n‚è±Ô∏è {elapsed:.2f}s\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Gradio UI\n",
    "# -----------------------------\n",
    "with gr.Blocks(title=\"Whisper AZ Transcriber\") as demo:\n",
    "    gr.Markdown(\"## Whisper AZ Transcriber\\nRecord or upload audio; model returns the transcript.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(\n",
    "            sources=[\"microphone\", \"upload\"],\n",
    "            type=\"numpy\",            # gives (sr, numpy) for mic; file uploads return dict\n",
    "            label=\"Microphone or upload (.wav/.mp3/etc.)\",\n",
    "        )\n",
    "    btn = gr.Button(\"Transcribe\")\n",
    "    out = gr.Textbox(label=\"Transcript\", lines=6)\n",
    "\n",
    "    btn.click(transcribe, inputs=audio, outputs=out)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch on all interfaces so you can open it from your laptop browser via the server IP\n",
    "    demo.launch(server_port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02789d4-e073-4d74-a3f1-e9271d3a4af7",
   "metadata": {},
   "source": [
    "## Chunking for long audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134e1ba-0c94-47e1-bd32-71beaa0c10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# Audio I/O\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "from transformers import (\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "# If you exported to a single folder, set MODEL_DIR to that folder.\n",
    "# Otherwise, set it to your run root that also contains tokenizer/preprocessor files.\n",
    "MODEL_DIR = r\"...\"  # or \".../export-278145\"\n",
    "\n",
    "# The folder that contains tokenizer.json, vocab.json, merges.txt, etc.\n",
    "ROOT_DIR = r\"...\"\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"...\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load model & processor\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (device == \"cuda\") and torch.cuda.get_device_capability(0)[0] >= 8  # Hopper+\n",
    "dtype = torch.bfloat16 if use_bf16 else torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(ROOT_DIR)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_DIR, torch_dtype=dtype)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Make sure generation settings are right for AZ transcription\n",
    "model.generation_config.language = \"az\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.generation_config.suppress_tokens = []\n",
    "\n",
    "SR = 16000  # Whisper expects 16kHz features\n",
    "\n",
    "def _load_audio_from_path(path: str, target_sr: int = SR) -> np.ndarray:\n",
    "    \"\"\"Load any audio file path -> float32 mono @ target_sr.\"\"\"\n",
    "    # soundfile preserves native sr; librosa resamples\n",
    "    wav, sr = sf.read(path, always_2d=False)\n",
    "    if wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)  # mono\n",
    "    if sr != target_sr:\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)\n",
    "    wav = wav.astype(np.float32)\n",
    "    # Clamp extreme amplitudes just in case\n",
    "    maxabs = np.max(np.abs(wav)) if wav.size else 0.0\n",
    "    if maxabs > 1.0:\n",
    "        wav = wav / maxabs\n",
    "    return wav\n",
    "\n",
    "def _to_float32(wav: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ensure float32 in [-1,1].\"\"\"\n",
    "    if np.issubdtype(wav.dtype, np.integer):\n",
    "        # common case: int16 mic input\n",
    "        max_int = np.iinfo(wav.dtype).max\n",
    "        wav = wav.astype(np.float32) / max_int\n",
    "    elif wav.dtype != np.float32:\n",
    "        wav = wav.astype(np.float32)\n",
    "    # clamp (just in case)\n",
    "    if wav.size:\n",
    "        m = np.max(np.abs(wav))\n",
    "        if m > 1.0:\n",
    "            wav /= m\n",
    "    return wav\n",
    "\n",
    "def _normalize_input(audio):\n",
    "    \"\"\"\n",
    "    Accepts: (sr, np.ndarray) from mic, np.ndarray, dict{'path':...}, or str path.\n",
    "    Returns: float32 mono @ 16 kHz.\n",
    "    \"\"\"\n",
    "    # mic: (sr, wav)\n",
    "    if isinstance(audio, tuple) and len(audio) == 2:\n",
    "        sr, wav = audio\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        wav = _to_float32(wav)             # <-- cast BEFORE resample\n",
    "        if sr != SR:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=SR)\n",
    "        return wav\n",
    "\n",
    "    # gradio v4 sometimes returns just a numpy array\n",
    "    if isinstance(audio, np.ndarray):\n",
    "        wav = audio\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        # we don't know the true sr; assume SR (UI forces 16k for mic)\n",
    "        return _to_float32(wav)\n",
    "\n",
    "    # file upload: dict with 'path'\n",
    "    if isinstance(audio, dict) and \"path\" in audio:\n",
    "        wav, sr = sf.read(audio[\"path\"], always_2d=False)\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        wav = _to_float32(wav)             # <-- cast BEFORE resample\n",
    "        if sr != SR:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=SR)\n",
    "        return wav\n",
    "\n",
    "    # raw string path\n",
    "    if isinstance(audio, str) and os.path.exists(audio):\n",
    "        wav, sr = sf.read(audio, always_2d=False)\n",
    "        if wav.ndim == 2:\n",
    "            wav = wav.mean(axis=1)\n",
    "        wav = _to_float32(wav)\n",
    "        if sr != SR:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=SR)\n",
    "        return wav\n",
    "\n",
    "    raise ValueError(\"Unsupported audio input format\")\n",
    "\n",
    "def extract_original_audio_tuple(audio):\n",
    "    \"\"\"\n",
    "    Returns a (sr, wav) tuple of the ORIGINAL, unmodified audio for playback.\n",
    "    - Mic: already (sr, wav)\n",
    "    - Numpy only: assume SR (UI mic is 16k); best-effort\n",
    "    - File upload/path: read from disk with dtype='float32'\n",
    "    \"\"\"\n",
    "    if isinstance(audio, tuple) and len(audio) == 2:\n",
    "        # mic path: (sr, wav) as-is\n",
    "        return audio\n",
    "\n",
    "    if isinstance(audio, np.ndarray):\n",
    "        # no explicit sr from gradio in this case; assume SR\n",
    "        return (SR, audio)\n",
    "\n",
    "    if isinstance(audio, dict) and \"path\" in audio:\n",
    "        wav, sr = sf.read(audio[\"path\"], always_2d=False, dtype=\"float32\")\n",
    "        return (sr, wav)\n",
    "\n",
    "    if isinstance(audio, str) and os.path.exists(audio):\n",
    "        wav, sr = sf.read(audio, always_2d=False, dtype=\"float32\")\n",
    "        return (sr, wav)\n",
    "\n",
    "    raise ValueError(\"Unsupported audio input format (original)\")\n",
    "\n",
    "CHUNK_SEC  = 15   # was 25\n",
    "STRIDE_SEC = 3    # was 5 (more context still)\n",
    "BATCH_SIZE = 4    # increase if you have GPU room\n",
    "\n",
    "def calc_max_len(chunk_sec):\n",
    "    # generous budget: ~16 tokens/sec, capped by Whisper limit (448)\n",
    "    return min(448, int(16 * chunk_sec))\n",
    "\n",
    "def chunk_audio(wav: np.ndarray, sr: int = SR,\n",
    "                chunk_sec: int = CHUNK_SEC, stride_sec: int = STRIDE_SEC):\n",
    "    \"\"\"Return list of (start_sample, end_sample) windows with overlap.\"\"\"\n",
    "    chunk = int(chunk_sec * sr)\n",
    "    stride = int(stride_sec * sr)\n",
    "    if wav.shape[0] <= chunk:\n",
    "        return [(0, wav.shape[0])]\n",
    "    spans = []\n",
    "    i = 0\n",
    "    N = wav.shape[0]\n",
    "    while i < N:\n",
    "        s = i\n",
    "        e = min(i + chunk, N)\n",
    "        spans.append((s, e))\n",
    "        if e == N:\n",
    "            break\n",
    "        i += (chunk - stride)\n",
    "    return spans\n",
    "\n",
    "def dedup_join(texts, max_overlap_words=12):\n",
    "    \"\"\"Remove only the exact overlapping prefix of the next chunk\n",
    "       that matches the suffix of the previous chunk.\"\"\"\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if not out:\n",
    "            out.append(t)\n",
    "            continue\n",
    "        prev_words = out[-1].split()\n",
    "        curr_words = t.split()\n",
    "\n",
    "        # find longest common prefix/suffix overlap up to max_overlap_words\n",
    "        cut = 0\n",
    "        lim = min(len(prev_words), len(curr_words), max_overlap_words)\n",
    "        for k in range(lim, 0, -1):\n",
    "            if prev_words[-k:] == curr_words[:k]:\n",
    "                cut = k\n",
    "                break\n",
    "        out.append(\" \".join(curr_words[cut:]))\n",
    "    return \" \".join(out).strip()\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def fuzzy_join(texts, max_tail_words=30, min_ratio=0.6):\n",
    "    \"\"\"\n",
    "    Join chunk texts while removing near-duplicate overlap even if punctuation/\n",
    "    casing differ. Looks for the longest suffix of prev that roughly matches\n",
    "    the prefix of curr and trims it.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if not out:\n",
    "            out.append(t)\n",
    "            continue\n",
    "\n",
    "        prev = out[-1]\n",
    "        prev_words = prev.split()\n",
    "        curr_words = t.split()\n",
    "\n",
    "        # Compare up to last N words of prev with first N words of curr\n",
    "        N = min(max_tail_words, len(prev_words), len(curr_words))\n",
    "        cut = 0\n",
    "        best_ratio = 0.0\n",
    "        for k in range(N, 0, -1):\n",
    "            tail = \" \".join(prev_words[-k:])\n",
    "            head = \" \".join(curr_words[:k])\n",
    "            r = SequenceMatcher(None, tail.lower(), head.lower()).ratio()\n",
    "            if r >= min_ratio and r >= best_ratio:\n",
    "                best_ratio = r\n",
    "                cut = k\n",
    "                # break on first good-enough match from the longest side\n",
    "                break\n",
    "\n",
    "        out.append(\" \".join(curr_words[cut:]))\n",
    "    return \" \".join(out).strip()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def transcribe(audio_in):\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # ORIGINAL (for playback)\n",
    "        orig_sr, orig_wav = extract_original_audio_tuple(audio_in)\n",
    "\n",
    "        # PROCESSED (what Whisper actually gets)\n",
    "        wav = _normalize_input(audio_in)\n",
    "        if wav.size == 0:\n",
    "            return \"Empty audio.\", (orig_sr, orig_wav), (SR, wav)\n",
    "\n",
    "        spans = chunk_audio(wav, SR, CHUNK_SEC, STRIDE_SEC)\n",
    "        texts = []\n",
    "\n",
    "        for i in range(0, len(spans), BATCH_SIZE):\n",
    "            batch_spans = spans[i:i + BATCH_SIZE]\n",
    "            waves = [wav[s:e] for (s, e) in batch_spans]\n",
    "\n",
    "            feats = processor.feature_extractor(\n",
    "                waves, sampling_rate=SR, return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "\n",
    "            with (torch.autocast(device_type=\"cuda\", dtype=dtype)\n",
    "                  if device == \"cuda\" else torch.no_grad()):\n",
    "                out_ids = model.generate(\n",
    "                    inputs=feats,\n",
    "                    max_length=calc_max_len(CHUNK_SEC),\n",
    "                    do_sample=False,\n",
    "                )\n",
    "\n",
    "            texts.extend(\n",
    "                processor.tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        full_text = dedup_join(texts, max_overlap_words=12)\n",
    "        elapsed = time.time() - t0\n",
    "        transcript = f\"{full_text}\\n\\n‚è±Ô∏è {elapsed:.2f}s  (chunks: {len(spans)})\"\n",
    "\n",
    "        # Return: text, original audio, processed audio\n",
    "        return transcript, (orig_sr, orig_wav), (SR, wav)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", None, None\n",
    "\n",
    "# -----------------------------\n",
    "# Gradio UI\n",
    "# -----------------------------\n",
    "with gr.Blocks(title=\"Whisper AZ Transcriber\") as demo:\n",
    "    gr.Markdown(\"## Whisper AZ Transcriber\\nRecord or upload audio; model returns the transcript.\\n\\n\"\n",
    "                \"**Tip:** Compare the original vs processed audio below.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(\n",
    "            sources=[\"microphone\", \"upload\"],\n",
    "            type=\"numpy\",            # mic = (sr, numpy); uploads may be dict in some cases\n",
    "            label=\"Microphone or upload (.wav/.mp3/etc.)\",\n",
    "        )\n",
    "\n",
    "    btn = gr.Button(\"Transcribe\")\n",
    "\n",
    "    with gr.Row():\n",
    "        out_text = gr.Textbox(label=\"Transcript\", lines=6)\n",
    "\n",
    "    with gr.Row():\n",
    "        out_orig = gr.Audio(label=\"Original (as provided)\", type=\"numpy\")\n",
    "        out_proc = gr.Audio(label=\"Processed (mono @ 16 kHz for Whisper)\", type=\"numpy\")\n",
    "\n",
    "    # three outputs now: text, original, processed\n",
    "    btn.click(transcribe, inputs=audio, outputs=[out_text, out_orig, out_proc])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f87176-205d-4d62-87fd-98b4da04b642",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üéß Why the ‚ÄúOriginal‚Äù sounds noisier than the mic preview\n",
    "\n",
    "The **first waveform at the top** (from the Gradio input widget) is actually how the browser encodes and streams the audio to Python ‚Äî typically as **compressed WebM/Opus or MP3**, and Gradio plays it back *before* any decoding.\n",
    "When you play that same audio again inside the Gradio app (after it‚Äôs passed to Python, decoded, processed, possibly resampled, and re-encoded as WAV for the player), you‚Äôre hearing a slightly different signal path.\n",
    "\n",
    "This introduces three differences:\n",
    "\n",
    "1. **Codec differences** ‚Äì The browser preview is using a lossy but psychoacoustically optimized codec (Opus). When Gradio sends it to Python, it‚Äôs decoded to PCM, then re-encoded to WAV for playback. The WAV has no compression masking, so low-level background noise is more audible.\n",
    "2. **Resampling noise** ‚Äì Your code resamples 8 kHz ‚Üí 16 kHz using a high-quality band-limited algorithm. That upsampling *doesn‚Äôt* add new information above 4 kHz but may emphasize existing broadband noise in that band.\n",
    "3. **Mono down-mixing** ‚Äì Stereo averaging (`mean(axis=1)`) can bring out noise that was previously panned differently in each channel. If the two channels weren‚Äôt perfectly correlated, summing them may slightly decorrelate ambient hiss or room noise, making it sound ‚Äúfuller.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **The one that Whisper actually ‚Äúhears‚Äù and transcribes** is the **processed version** ‚Äî the one labeled\n",
    "\n",
    "> üéß *Processed (mono @ 16 kHz for Whisper)*\n",
    "\n",
    "That‚Äôs the float-32, mono, 16 kHz waveform that you create inside `_normalize_input()` before feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Does this affect transcription quality?\n",
    "\n",
    "No ‚Äî this is **completely safe** and **expected**.\n",
    "Here‚Äôs why:\n",
    "\n",
    "| Step                           | Purpose                                                                                            | Effect on ASR quality |\n",
    "| ------------------------------ | -------------------------------------------------------------------------------------------------- | --------------------- |\n",
    "| Stereo ‚Üí Mono                  | Whisper was trained on mono audio; down-mixing is required                                         | **Safe / required**   |\n",
    "| 8 kHz ‚Üí 16 kHz                 | Whisper expects 16 kHz input; upsampling simply doubles the sample rate so the FFTs match training | **Safe / required**   |\n",
    "| 8-bit ‚Üí float32                | Expands to full dynamic range, no extra quantization noise                                         | **Safe / required**   |\n",
    "| Float normalization / clamping | Prevents clipping, ensures consistent loudness                                                     | **Safe**              |\n",
    "\n",
    "So the ‚Äúnoise‚Äù you hear in playback is a *perceptual artifact* of resampling and mixing ‚Äî not a data-quality issue.\n",
    "Whisper itself sees exactly what it was trained for (mono 16 kHz float PCM), and that small background hiss doesn‚Äôt change the model‚Äôs ability to recognize speech.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Bottom line\n",
    "\n",
    "* The **processed 16 kHz mono signal** is what Whisper uses.\n",
    "* The extra hiss is *audible* only in playback; it **does not hurt transcription accuracy**.\n",
    "* Your preprocessing pipeline is **fully correct** for Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75253be3-4081-4f72-9930-58c4ef91598f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LLM)",
   "language": "python",
   "name": "llm-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
