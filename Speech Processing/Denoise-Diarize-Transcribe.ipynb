{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "\n",
    "# Replace with your Hugging Face access token\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"read_token_here\" # Needed only once to download the gated models. Safe to comment out after models are cached locally.\n",
    ")\n",
    "\n",
    "# Send pipeline to GPU if available\n",
    "pipeline.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Apply pretrained pipeline to an audio file (\"path/to/your/audio.wav\")\n",
    "diarization = pipeline(r\".wav\")\n",
    "\n",
    "# Print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## \"Denoise → Diarize → Transcribe\" pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import gradio as gr\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, pipeline as whisper_pipeline\n",
    "from pyannote.audio import Pipeline as DiarizationPipeline\n",
    "import uuid\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "# ========== Device Setup ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== Load Denoising Model ==========\n",
    "denoise_model = pretrained.dns64().to(device)\n",
    "\n",
    "DEBUG_DIR = \"debug/\"\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "def denoise_audio(audio_path):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = convert_audio(wav, sr, denoise_model.sample_rate, denoise_model.chin)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enhanced = denoise_model(wav.to(device))\n",
    "\n",
    "    enhanced = enhanced.squeeze(0).cpu()\n",
    "    out_path = os.path.join(DEBUG_DIR, f\"denoised_{uuid.uuid4().hex}.wav\")\n",
    "    torchaudio.save(out_path, enhanced, denoise_model.sample_rate)\n",
    "    return out_path\n",
    "\n",
    "# ========== Load Whisper Large-V3 ==========\n",
    "large_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n",
    "large_processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "\n",
    "large_model.generation_config.language = \"az\"\n",
    "large_model.config.forced_decoder_ids = large_processor.get_decoder_prompt_ids(language=\"azerbaijani\", task=\"transcribe\")\n",
    "\n",
    "large_pipe = whisper_pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=large_model,\n",
    "    tokenizer=large_processor.tokenizer,\n",
    "    feature_extractor=large_processor.feature_extractor,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ========== Load PyAnnote Diarization Pipeline ==========\n",
    "# NOTE: Replace with your real Hugging Face token\n",
    "HF_TOKEN = \"your_hf_token_here\"\n",
    "\n",
    "diarization_pipeline = DiarizationPipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    #use_auth_token=HF_TOKEN\n",
    ")\n",
    "diarization_pipeline.to(device)\n",
    "\n",
    "def diarize_audio(audio_path):\n",
    "    diarization = diarization_pipeline(audio_path)\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append(f\"{turn.start:.1f}s - {turn.end:.1f}s: Speaker {speaker}\")\n",
    "    return \"\\n\".join(segments)\n",
    "\n",
    "# ========== Main Processing Function ==========\n",
    "def process_audio(audio_path):\n",
    "    # 1) save a copy of the raw input\n",
    "    raw_copy = os.path.join(DEBUG_DIR, f\"original_{uuid.uuid4().hex}.wav\")\n",
    "    shutil.copy(audio_path, raw_copy)\n",
    "\n",
    "    # 2) denoise & save in debug/\n",
    "    denoised_path = denoise_audio(audio_path)\n",
    "\n",
    "    # 3) diarize & load the denoised audio\n",
    "    diarization = diarization_pipeline(denoised_path)\n",
    "    wav, sr   = torchaudio.load(denoised_path)\n",
    "    duration  = wav.shape[-1] / sr\n",
    "\n",
    "    speaker_clips = {}\n",
    "    PAD = 0.2  # 200 ms padding\n",
    "\n",
    "    for i, (segment, _, speaker) in enumerate(diarization.itertracks(yield_label=True)):\n",
    "        # pad and round\n",
    "        start = max(0.0, segment.start - PAD)\n",
    "        end   = min(duration, segment.end   + PAD)\n",
    "        start_s = int(start * sr)\n",
    "        end_s   = int(math.ceil(end * sr))\n",
    "\n",
    "        clip = wav[:, start_s:end_s]\n",
    "        clip_name = f\"clip_{speaker}_{i}_{uuid.uuid4().hex}.wav\"\n",
    "        clip_path = os.path.join(DEBUG_DIR, clip_name)\n",
    "        torchaudio.save(clip_path, clip, sr)\n",
    "\n",
    "        # transcribe\n",
    "        transcript = large_pipe(clip_path)[\"text\"].strip()\n",
    "        speaker_clips.setdefault(speaker, []).append((clip_path, transcript))\n",
    "\n",
    "    # (optionally) remove denoised_path if you don’t need even that\n",
    "    # os.remove(denoised_path)\n",
    "\n",
    "    # 4) build HTML output…\n",
    "    html = \"\"\n",
    "    for spk, clips in speaker_clips.items():\n",
    "        html += f\"<h3>{spk}</h3><ul>\"\n",
    "        for clip_path, txt in clips:\n",
    "            html += (\n",
    "                f\"<li>\"\n",
    "                f\"<audio controls src=\\\"{clip_path}\\\"></audio> \"\n",
    "                f\"<b>Transcription:</b> {txt}\"\n",
    "                f\"</li>\"\n",
    "            )\n",
    "        html += \"</ul>\"\n",
    "\n",
    "    return html\n",
    "\n",
    "# ——— Gradio UI ———\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Azerbaijani Audio → Per-Speaker Transcript & Clips\")\n",
    "    audio_input = gr.Audio(type=\"filepath\", label=\"Your audio\")\n",
    "    out = gr.HTML(label=\"Per-Speaker Transcripts and Audio\")\n",
    "    btn = gr.Button(\"Process\")\n",
    "    \n",
    "    btn.click(fn=process_audio, inputs=audio_input, outputs=out)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## “Whole+Align”\n",
    "\n",
    "denoise → diarize → one-shot full-audio transcription with Stable-Whisper → align each transcribed segment to speakers by midpoint → display per-speaker utterances alongside a single full-audio player for context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Comparison of two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "from pyannote.audio import Pipeline as DiarizationPipeline\n",
    "from stable_whisper import load_model as load_sw_model\n",
    "import gradio as gr\n",
    "\n",
    "# ========== Device Setup ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== Load Denoising Model ==========\n",
    "denoise_model = pretrained.dns64().to(device)\n",
    "DEBUG_DIR = \"debug/\"\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "def denoise_audio(audio_path):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = convert_audio(wav, sr, denoise_model.sample_rate, denoise_model.chin)\n",
    "    with torch.no_grad():\n",
    "        enhanced = denoise_model(wav.to(device))\n",
    "    enhanced = enhanced.squeeze(0).cpu()\n",
    "    out_path = os.path.join(DEBUG_DIR, f\"denoised_{uuid.uuid4().hex}.wav\")\n",
    "    torchaudio.save(out_path, enhanced, denoise_model.sample_rate)\n",
    "    return out_path\n",
    "\n",
    "# ========== Load PyAnnote Diarization Pipeline ==========\n",
    "# NOTE: replace with your Hugging Face token if needed\n",
    "HF_TOKEN = None\n",
    "diarization_pipeline = DiarizationPipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "diarization_pipeline.to(device)\n",
    "\n",
    "# ========== Load Stable-Whisper ==========\n",
    "sw_model = load_sw_model(\"large-v3\", device=device)\n",
    "\n",
    "# ================= Approach 1 =================\n",
    "# Transcribe whole audio with Stable-Whisper, then align segments to speakers\n",
    "def process_audio_whole_align(audio_path):\n",
    "    # 1) save a copy of raw input\n",
    "    raw_copy = os.path.join(DEBUG_DIR, f\"original_{uuid.uuid4().hex}.wav\")\n",
    "    shutil.copy(audio_path, raw_copy)\n",
    "\n",
    "    # 2) denoise\n",
    "    denoised_path = denoise_audio(audio_path)\n",
    "\n",
    "    # 3) full transcription with timestamps for context & comparison\n",
    "    full_result = sw_model.transcribe(\n",
    "        denoised_path,\n",
    "        language=\"azerbaijani\",\n",
    "        word_timestamps=True\n",
    "    )\n",
    "    full_text = full_result.text.strip()\n",
    "    sw_segments = full_result.segments  # list of Segment with .start, .end, .text\n",
    "\n",
    "    # 4) diarize\n",
    "    diarization = diarization_pipeline(denoised_path)\n",
    "    diar_list = [(turn.start, turn.end, speaker) for turn, _, speaker in diarization.itertracks(yield_label=True)]\n",
    "\n",
    "    # 5) assign segments to speakers\n",
    "    speaker_texts = defaultdict(list)\n",
    "    for seg in sw_segments:\n",
    "        text = seg.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        mid = (seg.start + seg.end) / 2\n",
    "        for start, end, spk in diar_list:\n",
    "            if start <= mid <= end:\n",
    "                speaker_texts[spk].append(text)\n",
    "                break\n",
    "\n",
    "    # 6) build HTML output\n",
    "    html = \"\"\n",
    "    html += \"<h2>Full Transcription (Whole Audio)</h2>\"\n",
    "    html += f\"<div style='white-space: pre-wrap; padding:8px; border:1px solid #ccc; margin-bottom:16px;'>{full_text}</div>\"\n",
    "    html += f\"<audio controls src='{denoised_path}' style='width:100%; margin-bottom:16px;'></audio>\"\n",
    "    for spk, texts in speaker_texts.items():\n",
    "        html += f\"<h3>Speaker {spk}</h3>\"\n",
    "        for utt in texts:\n",
    "            html += f\"<p>{utt}</p>\"\n",
    "    return html\n",
    "\n",
    "# =============== Approach 2 ===============\n",
    "# Use Stable-Whisper per speaker clip with built-in chunking/segmentation logic\n",
    "PAD = 0.2  # seconds of padding\n",
    "\n",
    "def process_audio_perclip_stable(audio_path):\n",
    "    # 1) save copy\n",
    "    raw_copy = os.path.join(DEBUG_DIR, f\"original_{uuid.uuid4().hex}.wav\")\n",
    "    shutil.copy(audio_path, raw_copy)\n",
    "\n",
    "    # 2) denoise\n",
    "    denoised_path = denoise_audio(audio_path)\n",
    "\n",
    "    # 3) full transcription for comparison\n",
    "    full_result = sw_model.transcribe(\n",
    "        denoised_path,\n",
    "        language=\"azerbaijani\",\n",
    "        beam_size=5,\n",
    "        word_timestamps=False\n",
    "    )\n",
    "    full_text = full_result.text.strip()\n",
    "\n",
    "    # 4) diarize & load wav\n",
    "    diarization = diarization_pipeline(denoised_path)\n",
    "    wav, sr = torchaudio.load(denoised_path)\n",
    "    duration = wav.shape[-1] / sr\n",
    "    speaker_clips = defaultdict(list)\n",
    "\n",
    "    # 5) extract each speaker segment, pad, and transcribe\n",
    "    for i, (segment, _, speaker) in enumerate(diarization.itertracks(yield_label=True)):\n",
    "        start = max(0.0, segment.start - PAD)\n",
    "        end = min(duration, segment.end + PAD)\n",
    "        start_s = int(start * sr)\n",
    "        end_s = int(math.ceil(end * sr))\n",
    "        clip = wav[:, start_s:end_s]\n",
    "        clip_name = f\"clip_{speaker}_{i}_{uuid.uuid4().hex}.wav\"\n",
    "        clip_path = os.path.join(DEBUG_DIR, clip_name)\n",
    "        torchaudio.save(clip_path, clip, sr)\n",
    "\n",
    "        # transcribe with Stable-Whisper\n",
    "        result = sw_model.transcribe(\n",
    "            clip_path,\n",
    "            language=\"azerbaijani\",\n",
    "            beam_size=5,\n",
    "            word_timestamps=False\n",
    "        )\n",
    "        transcript = result.text.strip()\n",
    "        speaker_clips[speaker].append((clip_path, transcript))\n",
    "\n",
    "    # 6) build HTML output\n",
    "    html = \"\"\n",
    "    html += \"<h2>Full Transcription (Whole Audio)</h2>\"\n",
    "    html += f\"<div style='white-space: pre-wrap; padding:8px; border:1px solid #ccc; margin-bottom:16px;'>{full_text}</div>\"\n",
    "    for spk, clips in speaker_clips.items():\n",
    "        html += f\"<h3>Speaker {spk}</h3><ul>\"\n",
    "        for clip_path, txt in clips:\n",
    "            html += (\n",
    "                f\"<li><audio controls src='{clip_path}'></audio> \"\n",
    "                f\"<b>Transcription:</b> {txt}</li>\"\n",
    "            )\n",
    "        html += \"</ul>\"\n",
    "    return html\n",
    "\n",
    "# ================= Gradio UI =================\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Azerbaijani Audio → Per-Speaker Transcript & Clips\")\n",
    "    audio_input = gr.Audio(type=\"filepath\", label=\"Your audio\")\n",
    "    method = gr.Radio(\n",
    "        choices=[\"Whole+Align\", \"Per-Clip Stable\"],\n",
    "        value=\"Whole+Align\",\n",
    "        label=\"Choose Processing Method\"\n",
    "    )\n",
    "    out = gr.HTML()\n",
    "    btn = gr.Button(\"Process\")\n",
    "    btn.click(\n",
    "        fn=lambda p, m: process_audio_whole_align(p) if m == \"Whole+Align\" else process_audio_perclip_stable(p),\n",
    "        inputs=[audio_input, method],\n",
    "        outputs=out\n",
    "    )\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Processing Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully integrated denoising, speaker diarization, and transcription components into our pipeline. The denoising and diarization modules are performing well. However, transcription remains the weakest link. To improve transcription quality, we plan to first generate initial transcriptions using the Whisper-Large-V3 model, then pass them through an LLM for refinement into high-quality text. We'll perform random manual checks on a representative sample, pair these corrected transcriptions with their corresponding audio clips, and use this dataset to fine-tune the transcription model.\n",
    "\n",
    "It's important to note that **transcription quality is highly dependent on the quality of the audio input**. *When speaking clearly into the microphone, the transcription results are very accurate*. However, *for audio clips with lower quality*—such as those involving multiple people speaking over each other or instances where words are mumbled or inaudible—*the transcription quality deteriorates significantly*.\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose and Limitations of ASR/STT Systems like Whisper**\n",
    "\n",
    "Automatic Speech Recognition (ASR) or Speech-to-Text (STT) systems, such as OpenAI’s Whisper, are designed primarily to **transcribe audio speech into text**. Their core function is to convert the acoustic signal into its textual equivalent as faithfully as possible, without engaging in deeper reasoning, contextual understanding, or correction of factual or grammatical inaccuracies in the speech itself.\n",
    "\n",
    "The **primary goal** of such systems is **not to \"understand\" or \"reason\" about the meaning or correctness** of what is being said, but to **accurately reflect the audible content**—even if the original speech contains errors, hesitations, or truncations. If a speaker says something incorrect or unclear, the ASR model aims to transcribe exactly that, not to correct or reinterpret it.\n",
    "\n",
    "This limitation is similar to human perception: **even humans often struggle to decipher poor-quality or unclear speech** without relying on broader context, background knowledge, or guesswork. In AI systems, that interpretive reasoning is the domain of **Language Models (LLMs)** like GPT, not ASR models.\n",
    "\n",
    "Thus, **LLMs can be layered on top of ASR output to perform higher-level tasks**, such as:\n",
    "\n",
    "* Interpreting meaning\n",
    "* Correcting grammar\n",
    "* Inferring intent\n",
    "* Handling ambiguous or low-quality transcriptions\n",
    "\n",
    "This layered approach aligns with how human cognition works: first perceive (ASR), then comprehend and reason (LLM).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "from pyannote.audio import Pipeline as DiarizationPipeline\n",
    "from stable_whisper import load_model as load_sw_model\n",
    "import gradio as gr\n",
    "\n",
    "# ========== Device Setup ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== Load Denoising Model ==========\n",
    "denoise_model = pretrained.dns64().to(device)\n",
    "DEBUG_DIR = \"debug/\"\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "def denoise_audio(audio_path):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = convert_audio(wav, sr, denoise_model.sample_rate, denoise_model.chin)\n",
    "    with torch.no_grad():\n",
    "        enhanced = denoise_model(wav.to(device))\n",
    "    enhanced = enhanced.squeeze(0).cpu()\n",
    "    out_path = os.path.join(DEBUG_DIR, f\"denoised_{uuid.uuid4().hex}.wav\")\n",
    "    torchaudio.save(out_path, enhanced, denoise_model.sample_rate)\n",
    "    return out_path\n",
    "\n",
    "# ========== Load PyAnnote Diarization Pipeline ==========\n",
    "HF_TOKEN = None\n",
    "\n",
    "diarization_pipeline = DiarizationPipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "diarization_pipeline.to(device)\n",
    "\n",
    "# ========== Load Stable-Whisper ==========\n",
    "sw_model = load_sw_model(\"large-v3\", device=device)\n",
    "\n",
    "# ========== Single Processing Approach ==========\n",
    "def process_audio(audio_path):\n",
    "    # 1) backup original\n",
    "    raw_copy = os.path.join(DEBUG_DIR, f\"original_{uuid.uuid4().hex}.wav\")\n",
    "    shutil.copy(audio_path, raw_copy)\n",
    "\n",
    "    # 2) denoise\n",
    "    denoised_path = denoise_audio(audio_path)\n",
    "\n",
    "    # 3) full transcription with timestamps\n",
    "    full_result = sw_model.transcribe(\n",
    "        denoised_path,\n",
    "        language=\"azerbaijani\",\n",
    "        word_timestamps=True\n",
    "    )\n",
    "    full_text = full_result.text.strip()\n",
    "    sw_segments = full_result.segments\n",
    "\n",
    "    # 4) diarize\n",
    "    diarization = diarization_pipeline(denoised_path)\n",
    "    diar_list = [(turn.start, turn.end, speaker) for turn, _, speaker in diarization.itertracks(yield_label=True)]\n",
    "\n",
    "    # 5) align words to speakers\n",
    "    speaker_texts = defaultdict(list)\n",
    "    for seg in sw_segments:\n",
    "        text = seg.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        mid = (seg.start + seg.end) / 2\n",
    "        for start, end, spk in diar_list:\n",
    "            if start <= mid <= end:\n",
    "                speaker_texts[spk].append(text)\n",
    "                break\n",
    "\n",
    "    # 6) build HTML\n",
    "    html = []\n",
    "    html.append(\"<h2>Full Transcription (Whole Audio)</h2>\")\n",
    "    html.append(f\"<div style='white-space: pre-wrap; padding:8px; border:1px solid #ccc; margin-bottom:16px;'>{full_text}</div>\")\n",
    "    html.append(f\"<audio controls src='{denoised_path}' style='width:100%; margin-bottom:16px;'></audio>\")\n",
    "    for spk, texts in speaker_texts.items():\n",
    "        html.append(f\"<h3>Speaker {spk}</h3>\")\n",
    "        for utt in texts:\n",
    "            html.append(f\"<p>{utt}</p>\")\n",
    "    return \"\".join(html)\n",
    "\n",
    "# ========== Gradio UI ==========\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Azerbaijani Audio → Per-Speaker Transcript\")\n",
    "    audio_input = gr.Audio(type=\"filepath\", label=\"Upload your audio\")\n",
    "    out = gr.HTML()\n",
    "    btn = gr.Button(\"Process\")\n",
    "    btn.click(\n",
    "        fn=process_audio,\n",
    "        inputs=audio_input,\n",
    "        outputs=out\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LLM)",
   "language": "python",
   "name": "llm-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
