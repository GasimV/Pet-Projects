{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "packages = [\n",
    "    \"datasets\", \"transformers\", \"accelerate\", \"soundfile\",\n",
    "    \"librosa\", \"evaluate\", \"jiwer\", \"tensorboard\", \"gradio\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        version = importlib.import_module(pkg).__version__\n",
    "        print(f\"{pkg}: ‚úÖ Installed (version {version})\")\n",
    "    except ImportError:\n",
    "        print(f\"{pkg}: ‚ùå Not installed\")\n",
    "    except AttributeError:\n",
    "        print(f\"{pkg}: ‚úÖ Installed (version unknown)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "# ground truth\n",
    "reference = \"this is a test\"\n",
    "# model output\n",
    "hypothesis = \"this is test\"\n",
    "\n",
    "print(\"WER:\", wer(reference, hypothesis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transcribing with whisper-large-v3 model the single long audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset, Audio as HF_Audio\n",
    "from pydub import AudioSegment\n",
    "from stable_whisper import load_model  # from stable-ts\n",
    "import torch\n",
    "\n",
    "# 0. Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# 1. Convert your MP3 to WAV (if needed)\n",
    "input_path = r\"C:\\OpenAI Whisper Fine-Tune\\raw_audio.mp3\"\n",
    "output_path = r\"C:\\OpenAI Whisper Fine-Tune\\raw_audio.wav\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    print(f\"üéµ Converted MP3 to WAV ‚Üí {output_path}\")\n",
    "\n",
    "# 2. Load the Stable Whisper model with forced GPU\n",
    "model = load_model(\"large-v3\", device=device)  # ‚úÖ FORCES GPU if available\n",
    "\n",
    "# 3. Transcribe with automatic chunking and timestamps\n",
    "result = model.transcribe(output_path, language=\"az\", regroup=True)\n",
    "\n",
    "# Optional: Show detailed segments\n",
    "for i, seg in enumerate(result.segments):\n",
    "    print(f\"[{seg.start:.2f} ‚Üí {seg.end:.2f}] {seg.text}\")\n",
    "\n",
    "# 4. Join the full transcription\n",
    "full_text = result.text\n",
    "\n",
    "# 5. Save to ü§ó Hugging Face Dataset\n",
    "records = {\"audio\": [output_path], \"sentence\": [full_text]}\n",
    "ds = Dataset.from_dict(records)\n",
    "ds.save_to_disk(\"./az_transcription_dataset_full\")\n",
    "print(\"\\n‚úÖ Saved transcription dataset to ./az_transcription_dataset_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from datasets import load_from_disk, Audio as HF_Audio\n",
    "\n",
    "# Load and cast the dataset\n",
    "ds = load_from_disk(\"./az_transcription_dataset_full\")\n",
    "ds = ds.cast_column(\"audio\", HF_Audio(sampling_rate=16000))\n",
    "\n",
    "# Preview the full audio and its transcription\n",
    "example = ds[0]\n",
    "print(\"‚ñ∂Ô∏è Full Audio\")\n",
    "print(\"Transcription:\", example[\"sentence\"])\n",
    "display(Audio(example[\"audio\"][\"array\"], rate=example[\"audio\"][\"sampling_rate\"]))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Preview first 5 examples (or however many you want)\n",
    "for i in range(5):\n",
    "    example = ds[i]\n",
    "    print(f\"‚ñ∂Ô∏è Chunk {i}\")\n",
    "    print(\"Transcription:\", example[\"sentence\"])\n",
    "    display(Audio(example[\"audio\"][\"array\"], rate=example[\"audio\"][\"sampling_rate\"]))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Comparison with the Whisper-Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset, Audio as HF_Audio\n",
    "from pydub import AudioSegment\n",
    "from stable_whisper import load_model  # from stable-ts\n",
    "import torch\n",
    "\n",
    "# 0. Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# 1. Convert your MP3 to WAV (if needed)\n",
    "input_path = r\"C:\\OpenAI Whisper Fine-Tune\\raw_audio.mp3\"\n",
    "output_path = r\"C:\\OpenAI Whisper Fine-Tune\\raw_audio.wav\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "    print(f\"üéµ Converted MP3 to WAV ‚Üí {output_path}\")\n",
    "\n",
    "# 2. Load the Stable Whisper model with forced GPU\n",
    "model = load_model(\"small\", device=device)  # ‚úÖ FORCES GPU if available\n",
    "\n",
    "# 3. Transcribe with automatic chunking and timestamps\n",
    "result = model.transcribe(output_path, language=\"az\", regroup=True)\n",
    "\n",
    "# Optional: Show detailed segments\n",
    "for i, seg in enumerate(result.segments):\n",
    "    print(f\"[{seg.start:.2f} ‚Üí {seg.end:.2f}] {seg.text}\")\n",
    "\n",
    "# 4. Join the full transcription\n",
    "full_text = result.text\n",
    "\n",
    "# 5. Save to ü§ó Hugging Face Dataset\n",
    "records = {\"audio\": [output_path], \"sentence\": [full_text]}\n",
    "ds = Dataset.from_dict(records)\n",
    "ds.save_to_disk(\"./az_transcription_dataset_small\")\n",
    "print(\"\\n‚úÖ Saved transcription dataset to ./az_transcription_dataset_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from datasets import load_from_disk, Audio as HF_Audio\n",
    "\n",
    "# Load and cast the dataset\n",
    "ds = load_from_disk(\"./az_transcription_dataset_small\")\n",
    "ds = ds.cast_column(\"audio\", HF_Audio(sampling_rate=16000))\n",
    "\n",
    "# Preview the full audio and its transcription\n",
    "example = ds[0]\n",
    "print(\"‚ñ∂Ô∏è Full Audio\")\n",
    "print(\"Transcription:\", example[\"sentence\"])\n",
    "display(Audio(example[\"audio\"][\"array\"], rate=example[\"audio\"][\"sampling_rate\"]))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Conclusion**  \n",
    "\n",
    "In this step, we transcribed the same Azerbaijani audio using both the **Whisper Large-v3** and **Whisper Small** models via `stable-ts`. The comparison revealed that the **Whisper Small model's output was significantly less accurate** than that of the **Large-v3** model. However, this gap presents a great opportunity: by using the high-quality transcriptions from the Large-v3 model as reference data, we can **fine-tune the Small model** to significantly improve its performance ‚Äî especially for transcription tasks in under-resourced languages like Azerbaijani."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Whisper-Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Chop long audio into segments using the time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from datasets import Dataset, Audio\n",
    "from stable_whisper import load_model  # stable-ts\n",
    "\n",
    "# 0. Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"üöÄ Using device:\", device)\n",
    "\n",
    "# 1. MP3 ‚Üí WAV (if needed)\n",
    "input_path  = r\"C:\\OpenAI Whisper Fine-Tune\\raw_audio.mp3\"\n",
    "output_path = r\"C:\\OpenAI Whisper Fine-Tune\\raw_audio.wav\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    audio_mp3 = AudioSegment.from_file(input_path)\n",
    "    audio_mp3.export(output_path, format=\"wav\")\n",
    "    print(\"Converted MP3 ‚Üí WAV\")\n",
    "\n",
    "# 2. Load the raw wave into a NumPy array\n",
    "audio_arr, sr = sf.read(output_path, dtype=\"float32\")\n",
    "print(f\"Loaded WAV, {audio_arr.shape[0]/sr:.1f}s at {sr} Hz\")\n",
    "\n",
    "# 3. Transcribe with stable-ts\n",
    "model = load_model(\"large-v3\", device=device)\n",
    "result = model.transcribe(output_path, language=\"az\", regroup=True)\n",
    "\n",
    "# 4. Chop into segments using the time stamps\n",
    "records = []\n",
    "for seg in result.segments:\n",
    "    start, end, txt = seg.start, seg.end, seg.text.strip()\n",
    "    s_idx, e_idx = int(start * sr), int(end * sr)\n",
    "    clip = audio_arr[s_idx:e_idx]\n",
    "    records.append({\n",
    "        \"audio\": {\"array\": clip, \"sampling_rate\": sr},\n",
    "        \"sentence\": txt\n",
    "    })\n",
    "\n",
    "# 5. Build a Hugging Face Dataset of many short examples\n",
    "ds = Dataset.from_list(records)\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# 6. Inspect & save\n",
    "print(ds)           # you'll see e.g.  hundred rows of ~2‚Äì30 s clips\n",
    "print(\"Example:\", ds[0])\n",
    "ds.save_to_disk(\"./az_transcription_dataset_segments\")\n",
    "print(\"‚úÖ Saved segmented dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for GPU\n",
    "import torch\n",
    "print(\"üöÄ Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Install dependencies (uncomment if running in a fresh environment)\n",
    "# !pip install -q datasets>=2.6.1 transformers>=4.30.0 accelerate librosa evaluate jiwer soundfile\n",
    "\n",
    "# 3. Imports\n",
    "import os\n",
    "from datasets import load_from_disk, Audio\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import evaluate\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "# 4. Load your dataset\n",
    "ds = load_from_disk(\"./az_transcription_dataset_segments\")\n",
    "print(ds)\n",
    "\n",
    "# 5. Cast audio column to the 16 kHz Whisper format\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# 6. Load Whisper processor for \"small\" and set to Azerbaijani\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\n",
    "    \"openai/whisper-small\",\n",
    "    language=\"Azerbaijani\",   # Whisper‚Äôs language token\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\",\n",
    "    language=\"Azerbaijani\",\n",
    "    task=\"transcribe\"\n",
    ")\n",
    "\n",
    "# 7. Prepare the dataset: compute log-Mel inputs and tokenize transcripts\n",
    "def prepare_batch(batch):\n",
    "    # load + resample audio\n",
    "    audio_arr = batch[\"audio\"][\"array\"]\n",
    "    sr = batch[\"audio\"][\"sampling_rate\"]\n",
    "    # feature extraction\n",
    "    features = feature_extractor(audio_arr, sampling_rate=sr).input_features[0]\n",
    "    # tokenize\n",
    "    labels = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return {\"input_features\": features, \"labels\": labels}\n",
    "\n",
    "ds = ds.map(\n",
    "    prepare_batch,\n",
    "    remove_columns=ds.column_names,\n",
    "    num_proc=1\n",
    ")\n",
    "\n",
    "# Split into train & test sets\n",
    "ds = ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, eval_ds = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "# 8. Data collator to batch and pad correctly\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # the WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # separate audio inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        inputs = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(inputs, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        label_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = label_batch[\"input_ids\"].masked_fill(label_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        # remove leading bos (beginning of a sentence) if present:\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# 9. Load WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# We need to craft a function that processes our model predictions and calculates the WER metric. \n",
    "# This function, named `compute_metrics`, initially substitutes `-100` with the `pad_token_id` in the `label_ids`, \n",
    "# reversing the adjustment made in the data collator to accurately exclude padded tokens from the loss calculation. \n",
    "# Subsequently, it translates the predicted and label ids into strings. \n",
    "# Ultimately, it determines the WER by comparing the predictions with the reference labels:\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "# 10. Load the pretrained Whisper-Small model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.generation_config.language = \"az\" # Set decoding language\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Adjust generation parameters - no tokens are predetermined as decoder outputs, and \n",
    "# no tokens are excluded during the generation process\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.generation_config.suppress_tokens    = []\n",
    "\n",
    "# 11. Setup training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-az-small-finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=10,\n",
    "    max_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=20,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# 12. Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 13. Save processor (necessary for inference)\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "\n",
    "# 14. Launch training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Hugging Face fine-tuning guide for Whisper, the `Seq2SeqTrainer` is instantiated with `tokenizer=processor.feature_extractor` so that the Trainer treats the **audio** pre-processor as its main input handler rather than a text tokenizer. This is because the Trainer‚Äôs `tokenizer` parameter is not strictly for text tokenization‚Äîit designates the object used to name, save, and map model inputs during training and checkpointing. By passing the feature extractor, you ensure that audio padding, resampling, and log-Mel spectrogram conversion are correctly registered in the training pipeline, while text tokenization for labels remains under the separate `processor.tokenizer` in your data collator.\n",
    "\n",
    "https://huggingface.co/blog/fine-tune-whisper\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 (GPU)",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
