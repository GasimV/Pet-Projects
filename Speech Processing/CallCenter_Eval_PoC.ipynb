{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transcribing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from stable_whisper import load_model\n",
    "\n",
    "# === SETTINGS ===\n",
    "root_dir = r\"...\"\n",
    "output_csv = r\"...\"  # ‚úÖ Save here\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model = load_model(\"large-v3\", device=device)\n",
    "\n",
    "# === HELPER FUNCTION ===\n",
    "def transcribe_audio_files(directory):\n",
    "    results = []\n",
    "\n",
    "    # Walk through all subdirectories\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith(\".wav\"):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                print(f\"üéß Transcribing: {filepath}\")\n",
    "\n",
    "                try:\n",
    "                    result = model.transcribe(filepath, language=\"az\")\n",
    "                    text = result.text.strip()  # ‚úÖ Access object attribute, not dict\n",
    "\n",
    "                    if text:  # Skip empty transcriptions\n",
    "                        results.append([text, 1])  # Dummy label '1'\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Failed to transcribe {filename}: {e}\")\n",
    "    return results\n",
    "\n",
    "# === PROCESS ===\n",
    "transcriptions = transcribe_audio_files(root_dir)\n",
    "\n",
    "# === SAVE TO CSV ===\n",
    "if transcriptions:\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)  # ‚úÖ Ensure output dir exists\n",
    "\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Transcription\", \"Label\"])  # Header\n",
    "        writer.writerows(transcriptions)\n",
    "\n",
    "    print(f\"‚úÖ Transcriptions saved to {output_csv}\")\n",
    "else:\n",
    "    print(\"‚ùå No transcriptions were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Single-Audio Transcrabing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from stable_whisper import load_model\n",
    "\n",
    "# === SETTINGS ===\n",
    "audio_path = r\".wav\"\n",
    "output_csv = r\".csv\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model = load_model(\"large-v3\", device=device)\n",
    "\n",
    "# === TRANSCRIBE SINGLE AUDIO ===\n",
    "try:\n",
    "    print(f\"üéß Transcribing: {audio_path}\")\n",
    "    result = model.transcribe(audio_path, language=\"az\")\n",
    "    text = result.text.strip()\n",
    "\n",
    "    if text:\n",
    "        # === SAVE TO CSV ===\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Transcription\", \"Label\"])  # Header\n",
    "            writer.writerow([text, 1])  # Dummy label\n",
    "\n",
    "        print(f\"‚úÖ Transcription saved to {output_csv}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Transcription is empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during transcription: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\".csv\")\n",
    "display(df)\n",
    "print()\n",
    "display(df['Transcription'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\".csv\")\n",
    "display(df)\n",
    "print()\n",
    "display(df['Transcription'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Transcription'].iloc[97])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Lowercase the transcription text\n",
    "df[\"Transcription\"] = df[\"Transcription\"].str.lower()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save the new version to a separate CSV\n",
    "lowercased_file_path = r\"C:\\Pasha-PoC\\transcriptions.csv\"\n",
    "df.to_csv(lowercased_file_path, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.drop(index=13).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Existing DataFrame\n",
    "df_transcripts = pd.read_csv(r\".csv\")\n",
    "\n",
    "# Pasting synthetic transcriptions here as a list of strings\n",
    "negative_samples = [\n",
    "    \"alo salam m…ôn xaric…ô getm…ôk √º√ß√ºn m…ôhk√ºm olmamaƒüƒ±m bar…ôd…ô s…ôn…ôd lazƒ±m olduƒüunu dedil…ôr onu asan xidm…ôtd…ôn ala bil…ôr…ômmi salam xanƒ±m bu m…ôs…ôl…ôy…ô biz baxmƒ±rƒ±q dey…ôs…ôn ba≈üqa yerd…ô verirl…ôr bel…ô s…ôn…ôdl…ôri m…ônc…ô siz notariusa yaxƒ±nla≈üƒ±n amma deyibl…ôr ki asan verir el…ô s…ôn…ôdi m…ônc…ô …ôvv…ôll…ôr verirdi indi d…ôyi≈üib d…ôqiq bilmir…ôm bir d…ôqiq…ô z…ôhm…ôt olmasa g√∂zl…ôyin alo s…ôsiniz g…ôlmir z…ôifdi m…ôn d…ô yax≈üƒ± ba≈üa d√º≈üm√ºr…ôm n…ô ist…ôyirsiz s…ôn…ôdi bir d…ô z…ông edin b…ôlk…ô ba≈üqa …ôm…ôkda≈ü cavab ver…ô bil…ôr\",\n",
    "    \"salam m…ônim ≈ü…ôxsiyy…ôt v…ôsiq…ômin m√ºdd…ôti ke√ßib onu d…ôyi≈üm…ôk √º√ß√ºn n…ô etm…ôliy…ôm salam buyurun siz…ô deyibl…ôr biz d…ôyi≈üirik dey…ôs…ôn amma indi olmur el…ô ≈üey bizd…ô y…ôni m…ôlumat yoxdur m…ônd…ô s…ôn…ôd lazƒ±mdƒ± amma n…ô s…ôn…ôdi bilmir…ôm d…ôqiq bel…ô deyibl…ôr sad…ôc…ô ≈ü…ôxsiyy…ôt v…ôsiq…ôsi √º√ß√ºn adƒ± d…ôyi≈üib deyibl…ôr onda notarius baxar dey…ôs…ôn bizd…ô olmur s…ôn…ôd verm…ô proseduru m…ônlik deyil z…ôhm…ôt olmasa sonra z…ông edin ba≈üqa …ôm…ôkda≈ü bil…ôr\",\n",
    "    \"salam m…ôn ev √º√ß√ºn qeydiyyat arayƒ±≈üƒ± almaq ist…ôyirdim deyibl…ôr asan xidm…ôtd…ôn alƒ±nƒ±r xanƒ±m o qeydiyyatla baƒülƒ± s…ôn…ôdl…ôr biz vermirik m…ônc…ô icra hakimiyy…ôti baxƒ±r ona yox m…ôn konkret soru≈üuram siz vermirsinizs…ô deyin m…ôn gedim ba≈üqa yer…ô baxƒ±n google yazƒ±n g√∂r√ºn hardan verilir bizd…ô olmur bu g√ºn o s…ôn…ôd dey…ôs…ôn √ßƒ±xarƒ±lmƒ±r sistemd…ô problem var ba≈üqa sualƒ±nƒ±z varsa buyurun\",\n",
    "    \"alo salam m…ôn mehkum olmamaƒüƒ±m bar…ôd…ô arayƒ±≈ü almaq ist…ôyir…ôm √ß√ºnki xaric…ô s…ôn…ôd t…ôqdim etm…ôliy…ôm onu haradan ala bil…ôr…ôm salam xanƒ±m d…ôqiq dey…ô bilm…ôr…ôm vallah bizd…ô olmur m…ônc…ô …ôdliyy…ô baxƒ±r o s…ôn…ôdl…ôr…ô b…ôlk…ô ora ged…ôsiniz amma bu asan xidm…ôt deyilmi siz verm…ôlisiz axƒ± m…ônc…ô …ôvv…ôll…ôr verilirdi indi nec…ôdi bilmir…ôm z…ông edin soru≈üun ya ba≈üqa …ôm…ôkda≈üa y√∂nl…ôndirilsin bir d…ôqiq…ô alo s…ôs k…ôsilir sonra z…ông edin yax≈üƒ±\",\n",
    "    \"salam m…ôn u≈üaƒüƒ±n doƒüum haqqƒ±nda ≈ü…ôhad…ôtnam…ôsini almaq ist…ôyir…ôm onlayn m√ºraci…ôt etmi≈üdim amma alƒ±nmadƒ± onu nec…ô ala bil…ôr…ôm salam bizd…ô o sistem i≈ül…ômir dey…ôs…ôn indi siz gedin notariusa b…ôlk…ô onlar k√∂m…ôk ed…ô bil…ôr onlayn alƒ±nmadƒ±sa b…ôlk…ô sistemd…ô nasazlƒ±q var m…ôn burada he√ß n…ô g√∂r…ô bilmir…ôm y…ôni siz deyirsiniz ki asan xidm…ôt vermir m…ônc…ô …ôvv…ôll…ôr verirdi amma indi d…ôqiq bilmir…ôm siz…ô d√ºzg√ºn dey…ô bilm…ôr…ôm\",\n",
    "    \"alo salam m…ôn xaric…ô getm…ôk √º√ß√ºn s…ôn…ôdl…ôr toplamalƒ±yam v…ô m…ôhk√ºm olmamaƒüƒ±m bar…ôd…ô arayƒ±≈ü lazƒ±mdƒ±r onu nec…ô ala bil…ôr…ôm salam xanƒ±m bel…ô s…ôn…ôdl…ôri biz vermirik dey…ôs…ôn y…ôni siz deyirsiniz he√ß vermirsiniz …ôvv…ôll…ôr demi≈üdil…ôr sizd…ô olurdu indi yox dey…ôs…ôn d…ôyi≈üib sistemd…ô g√∂r√ºnm√ºr m…ônlik deyil siz gedin notariusa ora ver…ô bil…ôr amma d…ôqiq dey…ô bilm…ôr…ôm yax≈üƒ± ba≈üa d√º≈üm…ôdim indi siz verirsiniz ya yox yox yox dey…ôs…ôn biz vermirik z…ôhm…ôt olmasa sonra z…ông edin b…ôlk…ô ba≈üqa …ôm…ôkda≈ü bil…ôr\",\n",
    "    \"salam m…ôn…ô ≈ü…ôxsiyy…ôt v…ôsiq…ôsi √º√ß√ºn qeydiyyat arayƒ±≈üƒ± lazƒ±mdƒ±r internetd…ô yazƒ±lƒ±b asan xidm…ôtd…ôn g√∂t√ºr√ºl√ºr buyurun xanƒ±m m…ôn bir ≈üey bilmir…ôm o arayƒ±≈ü b…ôlk…ô b…ôl…ôdiyy…ôd…ôn alƒ±nƒ±r y…ôni bu bar…ôd…ô m…ôlumatƒ±m yoxdur bizd…ô olmur m…ônc…ô amma siz deyirsiniz ki internetd…ô yazƒ±lƒ±b bura verilir b…ôli amma m…ônlik deyil y…ôni sistemd…ô baxmƒ±ram m…ôn indi s…ôn…ôdi hardan alƒ±m deyirsiniz he√ß bilmirsiniz z…ông edin sabah soru≈üun m…ôn d…ôqiq dey…ô bilmir…ôm\",\n",
    "    \"alo m…ôn v…ôr…ôs…ôlikl…ô baƒülƒ± s…ôn…ôd almaq ist…ôyirdim buyurun salam xanƒ±m bu bar…ôd…ô he√ß m…ôlumatƒ±m yoxdur v…ôr…ôs…ôlik dediniz notarius baxƒ±r bizd…ô olmur s…ôn…ôdi nec…ô ala bil…ôr…ôm deyirl…ôr bura yaxƒ±nla≈ümaq lazƒ±mdƒ±r yox xanƒ±m o prosedur bizlik deyil dey…ôs…ôn m…ôhk…ôm…ôy…ô aidiyyatƒ± var z…ông edin m…ôhk…ôm…ôy…ô bura bel…ô ≈üeyl…ôrl…ô m…ô≈üƒüul olmur\",\n",
    "    \"salam m…ônim adƒ±m d…ôyi≈üib ≈ü…ôxsiyy…ôt v…ôsiq…ôsini d…ôyi≈üm…ôk ist…ôyir…ôm hansƒ± s…ôn…ôdl…ôr lazƒ±mdƒ±r xanƒ±m siz y…ôqin qeydiyyat yerin…ô getm…ôlisiniz bizd…ô olmur s…ôn…ôd y…ôni m…ôlumatƒ±m yoxdur bel…ô ≈üeyl…ôrl…ô biz baxmƒ±rƒ±q …ôvv…ôlc…ô deyirdil…ôr burdan alƒ±nƒ±r indi yox notarius b…ôlk…ô bilir z…ôhm…ôt olmasa ora gedin m…ônlik deyil\",\n",
    "    \"salam m…ôn xaric…ô s…ôn…ôd g√∂nd…ôrm…ôk ist…ôyir…ôm v…ô s…ôn…ôdi t…ôsdiql…ôtm…ôliy…ôm buyurun amma xanƒ±m bu m…ôs…ôl…ô il…ô m…ô≈üƒüul olmuruq bel…ô t…ôsdiq √º√ß√ºn siz ba≈üqa yerd…ô t…ôsdiq alƒ±n b…ôlk…ô konsulluƒüa z…ông edin bu i≈ül…ôr bizlik deyil\",\n",
    "    \"salam m…ôn ≈ü…ôxsiyy…ôt v…ôsiq…ômi itirmi≈ü…ôm v…ô yeni s…ôn…ôd almaq ist…ôyir…ôm bu m√ºmk√ºnm√º xanƒ±m itkin s…ôn…ôdl…ô baƒülƒ± biz m…ôlumat vermirik polis…ô m√ºraci…ôt edin o s…ôn…ôd bizlik deyil y…ôni siz he√ß n…ô ed…ô bilmirsiniz yox xanƒ±m ke√ßin b√∂lm…ôy…ô\",\n",
    "    \"alo salam m…ôn ail…ô v…ôziyy…ôtiml…ô baƒülƒ± s…ôn…ôd ist…ôyir…ôm deyibl…ôr asan verir onu xanƒ±m m…ôn bilmir…ôm bel…ô ≈üeyl…ôr √º√ß√ºn notarius var dey…ôs…ôn ail…ô arayƒ±≈ülarƒ± biz vermirik valla m…ôlumatƒ±m yoxdur z…ông edin ba≈üqa yer…ô\",\n",
    "    \"salam m…ônim u≈üaƒüƒ±m √º√ß√ºn doƒüum ≈ü…ôhad…ôtnam…ôsini b…ôrpa etdirm…ôk ist…ôyir…ôm sistemd…ô g√∂r√ºnm√ºr xanƒ±m biz bel…ô m…ôlumatlara baxmƒ±rƒ±q siz qeydiyyat ≈ü√∂b…ôsin…ô getm…ôlisiniz bizd…ô olmur s…ôn…ôd m…ôn d…ô buradan n…ô ed…ô bil…ôr…ôm\",\n",
    "    \"salam m…ôn ev s…ôn…ôdimi t…ôsdiql…ôtm…ôk ist…ôyir…ôm y…ôni alqƒ±-satqƒ± √º√ß√ºn lazƒ±mdƒ±r buyurun amma xanƒ±m bizd…ô bel…ô ≈üeyl…ôr olmur o s…ôn…ôdi siz ya notariusdan alƒ±n ya da≈üƒ±nmaz …ômlak ofisin…ô yaxƒ±nla≈üƒ±n asan baxmƒ±r bu tip ≈üeyl…ôr…ô\",\n",
    "    \"salam m…ôn √∂z√ºm…ô vasiq…ô √ßƒ±xartmaq ist…ôyir…ôm amma deyibl…ôr b…ôzi s…ôn…ôdl…ôr lazƒ±mdƒ±r n…ô lazƒ±mdƒ±r dey…ô bil…ôrsiniz xanƒ±m bu m…ôs…ôl…ôy…ô m…ôn baxmƒ±ram √ºmumiyy…ôtl…ô nadir i≈üdir dey…ôs…ôn siz ba≈üqa yer…ô gedin\",\n",
    "    \"salam m…ôn m√ºv…ôqq…ôti qeydiyyat √º√ß√ºn s…ôn…ôd almaq ist…ôyir…ôm √ß√ºnki evimi d…ôyi≈ümi≈ü…ôm xanƒ±m bu qeydiyyat m…ôs…ôl…ôsi bizlik deyil icra hakimiyy…ôtin…ô getm…ôlisiniz biz he√ß n…ô etmirik bel…ô m…ôs…ôl…ôd…ô\",\n",
    "    \"salam m…ôn ≈ü…ôxsiyy…ôt v…ôsiq…ômi itirmi≈ü…ôm ona g√∂r…ô bank s…ôn…ôdi ala bilmir…ôm n…ô etm…ôliy…ôm buyurun bu bar…ôd…ô he√ß n…ô dey…ô bilm…ôr…ôm xanƒ±m b…ôlk…ô banka z…ông edin biz burda bel…ô s…ôn…ôdl…ô k√∂m…ôk ed…ô bilmirik\",\n",
    "    \"salam m…ôn mehkum olmamaq arayƒ±≈üƒ± almaq ist…ôyir…ôm deyibl…ôr s…ôn…ôd hazƒ±rdƒ±r amma h…ôl…ô z…ông g…ôlm…ôyib xanƒ±m biz burdan bel…ô m…ôlumatlarƒ± vermirik g√∂zl…ôyin z…ông ed…ôrl…ôr m…ônlik deyil\",\n",
    "    \"salam m…ôn ail…ô v…ôziyy…ôtiml…ô baƒülƒ± s…ôn…ôd t…ôqdim etm…ôliy…ôm …ôcn…ôbi v…ôt…ônda≈ü √º√ß√ºn bu asan xidm…ôtd…ô olurmu xanƒ±m bilmir…ôm valla …ôcn…ôbil…ôrl…ô baƒülƒ± √ßox ≈üey d…ôyi≈üib siz miqrasiya xidm…ôtin…ô z…ông edin bu bizlik deyil\",\n",
    "    \"salam ≈ü…ôxsiyy…ôt v…ôsiq…ômi d…ôyi≈üdirm…ôliy…ôm √ß√ºnki evl…ônmi≈ü…ôm v…ô soyadƒ±m d…ôyi≈üib xanƒ±m onu biz etmirik dey…ôs…ôn evlilik aktƒ± lazƒ±m olacaq notarius ya qeydiyyat yerin…ô yaxƒ±nla≈üƒ±n bizd…ô olmur\",\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the synthetic data\n",
    "df_negatives = pd.DataFrame({\n",
    "    \"Transcription\": negative_samples,\n",
    "    \"Label\": 0\n",
    "})\n",
    "\n",
    "# Append to your original DataFrame\n",
    "df_combined = pd.concat([df_transcripts, df_negatives], ignore_index=True)\n",
    "\n",
    "# Optional: Check class balance\n",
    "print(df_combined['Label'].value_counts())\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df_combined.to_csv(\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\".csv\")\n",
    "display(df)\n",
    "print()\n",
    "display(df['Transcription'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Transcription'].iloc[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "input_texts = ['query: Qadƒ±nlar g√ºnd…ô n…ô q…ôd…ôr protein q…ôbul etm…ôlidir?',\n",
    "                'query: Balqabaƒüƒ±n ev ≈ü…ôraitind…ô hazƒ±rlanmasƒ± √ºsullarƒ±',\n",
    "               \"passage: √úmumi olaraq, 19-70 ya≈ü arasƒ± qadƒ±nlar √º√ß√ºn g√ºnd…ôlik protein t…ôl…ôbatƒ± t…ôxmin…ôn 46 qramdƒ±r. ∆èg…ôr hamil…ôdirsinizs…ô v…ô ya idmanla m…ô≈üƒüul olursunuzsa, bu miqdar daha y√ºks…ôk ola bil…ôr.\",\n",
    "               \"passage: 1. Sƒ±yƒ±q balqabaq: Balqabaƒüƒ± soyub r…ônd…ôl…ôyin, azca yaƒüda qƒ±zardƒ±n, duz v…ô ≈ü…ôk…ôr …ôlav…ô edib bi≈üirin. 2. Soƒüanlƒ± balqabaq qovurmasƒ±: Balqabaq diliml…ôrini yaƒüda soƒüanla birlikd…ô qovurun, dadƒ±na duz vurun.\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ‚Äî the output looks exactly as expected, and here's how to **interpret the results** of the similarity matrix:\n",
    "\n",
    "```python\n",
    "[[86.88, 74.45],\n",
    " [75.53, 84.32]]\n",
    "```\n",
    "\n",
    "Each row corresponds to a **query**, and each column to a **passage**. So, the matrix shows:\n",
    "\n",
    "|                   | Passage 1 (protein) | Passage 2 (pumpkin) |\n",
    "| ----------------- | ------------------- | ------------------- |\n",
    "| Query 1 (protein) | **86.88**           | 74.45               |\n",
    "| Query 2 (pumpkin) | 75.53               | **84.32**           |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Interpretation:\n",
    "\n",
    "* **Query 1:** ‚ÄúQadƒ±nlar g√ºnd…ô n…ô q…ôd…ôr protein q…ôbul etm…ôlidir?‚Äù\n",
    "\n",
    "  * Highest match is **Passage 1**, with score **86.88** ‚Äî ‚úîÔ∏è Correct (it's the protein info).\n",
    "* **Query 2:** ‚ÄúBalqabaƒüƒ±n ev ≈ü…ôraitind…ô hazƒ±rlanmasƒ± √ºsullarƒ±‚Äù\n",
    "\n",
    "  * Highest match is **Passage 2**, with score **84.32** ‚Äî ‚úîÔ∏è Correct (it's the pumpkin recipe).\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Conclusion:\n",
    "\n",
    "* The model **correctly matched Azerbaijani queries** with relevant Azerbaijani passages based on semantic meaning.\n",
    "* This validates that `intfloat/multilingual-e5-large` can perform **cross-language and in-language semantic retrieval** very well ‚Äî even for **low-resource languages** like Azerbaijani."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Model size: 561M params; Tensor type: F32\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "# prepare input\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# forward pass\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Model size: 168M params; Tensor type: F32\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "text = df['Transcription'].iloc[0]\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = output.pooler_output.detach().numpy().squeeze()\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\".csv\")\n",
    "display(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "display(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased').to(device)\n",
    "model.eval()\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for text in df['Transcription']:\n",
    "    # Tokenize and move to device\n",
    "    encoded = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "        cls_embedding = output.pooler_output.detach().cpu().numpy().squeeze()  # move back to CPU for numpy\n",
    "        embeddings.append(cls_embedding)\n",
    "\n",
    "X = np.vstack(embeddings)  # Feature matrix\n",
    "y = df['Label'].values      # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Combine embeddings and labels into a DataFrame\n",
    "df_embeddings = pd.DataFrame(X)\n",
    "df_embeddings[\"Label\"] = y  # Append label column\n",
    "\n",
    "# Define output path\n",
    "output_path = r\".csv\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_embeddings.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Embeddings saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\".csv\")\n",
    "display(df)\n",
    "print()\n",
    "#display(df['Transcription'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\".csv\")\n",
    "#display(df)\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\".csv\")\n",
    "\n",
    "# Features and labels\n",
    "X = df.drop(columns=[\"Label\"]).values\n",
    "y = df[\"Label\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=1_000, random_state=42))\n",
    "    ]),\n",
    "    \"SVM (RBF)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(random_state=42))\n",
    "    ]),\n",
    "    \"K-NN\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "    ]),\n",
    "    # RandomForest, GradientBoosting and GaussianNB often work fine without scaling\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç Evaluating: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Access your trained model from the pipeline\n",
    "knn_model = models[\"K-NN\"]\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(knn_model, \"/knn_model.pkl\")\n",
    "print(\"‚úÖ K-NN model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# === Load tokenizer and model ===\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "model.eval()\n",
    "\n",
    "# === Load your trained K-NN model pipeline ===\n",
    "knn_model = joblib.load(\"../knn_model.pkl\")\n",
    "\n",
    "# === Your custom input text ===\n",
    "input_text = \"alo salam m…ôn s…ôn…ôd almaq ist…ôyir…ôm xaric…ô getm…ôk √º√ß√ºn m…ôhk√ºm olmamaƒüƒ±m bar…ôd…ô\"\n",
    "\n",
    "# === Step 1: Lowercase (same as training) ===\n",
    "input_text = input_text.lower()\n",
    "\n",
    "# === Step 2: Generate BERT [CLS] embedding ===\n",
    "encoded = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded)\n",
    "    embedding = output.pooler_output.detach().numpy().squeeze()\n",
    "\n",
    "# === Step 3: Predict using your trained model ===\n",
    "predicted_label = knn_model.predict([embedding])[0]\n",
    "\n",
    "print(f\"üîç Prediction: {'Yax≈üƒ± cavab' if predicted_label == 1 else 'Pis cavab'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import joblib\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "from stable_whisper import load_model as load_sw_model\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import gradio as gr\n",
    "\n",
    "# ========== Device Setup ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== Denoising Model ==========\n",
    "denoise_model = pretrained.dns64().to(device)\n",
    "DEBUG_DIR = \"debug/\"\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "def denoise_audio(audio_path):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = convert_audio(wav, sr, denoise_model.sample_rate, denoise_model.chin)\n",
    "    with torch.no_grad():\n",
    "        enhanced = denoise_model(wav.to(device))\n",
    "    enhanced = enhanced.squeeze(0).cpu()\n",
    "    out_path = os.path.join(DEBUG_DIR, f\"denoised_{uuid.uuid4().hex}.wav\")\n",
    "    torchaudio.save(out_path, enhanced, denoise_model.sample_rate)\n",
    "    return out_path\n",
    "\n",
    "# ========== Whisper Model ==========\n",
    "sw_model = load_sw_model(\"large-v3\", device=device)\n",
    "\n",
    "# ========== BERT + K-NN Setup ==========\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model.eval()\n",
    "\n",
    "knn_model = joblib.load(\"../knn_model.pkl\")\n",
    "\n",
    "def classify_transcription(text):\n",
    "    text = text.lower()\n",
    "    encoded = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()} # work on both GPU and CPU.\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(**encoded)\n",
    "        embedding = output.pooler_output.detach().numpy().squeeze()\n",
    "    \n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(**encoded)\n",
    "\n",
    "    \n",
    "    prediction = knn_model.predict([embedding])[0]\n",
    "    label = \"Yax≈üƒ± cavab ‚úÖ\" if prediction == 1 else \"Pis cavab ‚ùå\"\n",
    "    return label\n",
    "\n",
    "# ========== Full Processing Pipeline ==========\n",
    "def process_audio_and_classify(audio_path):\n",
    "    # 1. Save original\n",
    "    raw_copy = os.path.join(DEBUG_DIR, f\"original_{uuid.uuid4().hex}.wav\")\n",
    "    shutil.copy(audio_path, raw_copy)\n",
    "\n",
    "    # 2. Denoise\n",
    "    denoised_path = denoise_audio(audio_path)\n",
    "\n",
    "    # 3. Transcribe\n",
    "    result = sw_model.transcribe(denoised_path, language=\"azerbaijani\", word_timestamps=False)\n",
    "    full_text = result.text.strip()\n",
    "\n",
    "    # 4. Classify\n",
    "    label = classify_transcription(full_text)\n",
    "\n",
    "    # 5. Build HTML output\n",
    "    html = f\"\"\"\n",
    "    <h3>üîä Denoised Audio</h3>\n",
    "    <audio controls src='{denoised_path}' style='width:100%; margin-bottom:16px;'></audio>\n",
    "    <h3>üìÑ Transcription</h3>\n",
    "    <div style='white-space: pre-wrap; border:1px solid #ccc; padding:8px;'>{full_text}</div>\n",
    "    <h3>ü§ñ Model Prediction</h3>\n",
    "    <div style='font-size: 1.2em; font-weight: bold; color: {\"green\" if \"Yax≈üƒ±\" in label else \"red\"}'>{label}</div>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# ========== Gradio UI ==========\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## PoC: Evaluating Call Center Operator Performance via Call Analysis\")\n",
    "    audio_input = gr.Audio(type=\"filepath\", label=\"Upload WAV audio\")\n",
    "    output_html = gr.HTML()\n",
    "    run_button = gr.Button(\"Process and Classify\")\n",
    "\n",
    "    run_button.click(\n",
    "        fn=process_audio_and_classify,\n",
    "        inputs=audio_input,\n",
    "        outputs=output_html\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3CX Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import joblib\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "from stable_whisper import load_model as load_sw_model\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import gradio as gr\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import threading\n",
    "import time\n",
    "import webbrowser\n",
    "\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = str(os.cpu_count())\n",
    "\n",
    "# ========== Device Setup ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== Folders Setup ==========\n",
    "DEBUG_DIR = \"debug/\"\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "RECORDER_DIR = \"records\"\n",
    "os.makedirs(RECORDER_DIR, exist_ok=True)\n",
    "PROCESSED_DIR = os.path.join(RECORDER_DIR, \"processed\")\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ========== Denoising Model ==========\n",
    "denoise_model = pretrained.dns64().to(device)\n",
    "def denoise_audio(audio_path):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = convert_audio(wav, sr, denoise_model.sample_rate, denoise_model.chin)\n",
    "    with torch.no_grad():\n",
    "        enhanced = denoise_model(wav.to(device))\n",
    "    enhanced = enhanced.squeeze(0).cpu()\n",
    "    out_path = os.path.join(DEBUG_DIR, f\"denoised_{uuid.uuid4().hex}.wav\")\n",
    "    torchaudio.save(out_path, enhanced, denoise_model.sample_rate)\n",
    "    return out_path\n",
    "\n",
    "# ========== Whisper Model ==========\n",
    "sw_model = load_sw_model(\"large-v3\", device=device)\n",
    "\n",
    "# ========== BERT + K-NN Setup ==========\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model.eval()\n",
    "bert_model.to(device)\n",
    "\n",
    "knn_model = joblib.load(\"C:/Pasha-PoC/knn_model.pkl\")\n",
    "\n",
    "def classify_transcription(text):\n",
    "    text = text.lower()\n",
    "    encoded = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(**encoded.to(device))\n",
    "        embedding = output.pooler_output.detach().cpu().numpy().squeeze()\n",
    "    prediction = knn_model.predict([embedding])[0]\n",
    "    label = \"Yax≈üƒ± cavab ‚úÖ\" if prediction == 1 else \"Pis cavab ‚ùå\"\n",
    "    return label\n",
    "\n",
    "# ========== Full Processing Pipeline ==========\n",
    "def process_audio_and_classify(audio_path):\n",
    "    # 1. Save original\n",
    "    raw_copy = os.path.join(DEBUG_DIR, f\"original_{uuid.uuid4().hex}.wav\")\n",
    "    shutil.copy(audio_path, raw_copy)\n",
    "\n",
    "    # 2. Denoise\n",
    "    denoised_path = denoise_audio(audio_path)\n",
    "\n",
    "    # 3. Transcribe\n",
    "    result = sw_model.transcribe(denoised_path, language=\"azerbaijani\", word_timestamps=False)\n",
    "    full_text = result.text.strip()\n",
    "\n",
    "    # 4. Classify\n",
    "    label = classify_transcription(full_text)\n",
    "\n",
    "    # 5. Build HTML output\n",
    "    html = f\"\"\"\n",
    "    <h3>üîä Denoised Audio</h3>\n",
    "    <audio controls src='{denoised_path}' style='width:100%; margin-bottom:16px;'></audio>\n",
    "    <h3>üìÑ Transcription</h3>\n",
    "    <div style='white-space: pre-wrap; border:1px solid #ccc; padding:8px;'>{full_text}</div>\n",
    "    <h3>ü§ñ Model Prediction</h3>\n",
    "    <div style='font-size: 1.2em; font-weight: bold; color: {\"green\" if \"Yax≈üƒ±\" in label else \"red\"}'>{label}</div>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# === File Watcher Setup ===\n",
    "class NewWavHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event):\n",
    "        if event.is_directory or not event.src_path.endswith(\".wav\"):\n",
    "            return\n",
    "        if \"processed\" in os.path.normpath(event.src_path).split(os.sep):\n",
    "            return  # ‚úÖ Skip files inside any \"processed\" folder\n",
    "    \n",
    "        print(f\"[Watcher] Detected new file: {event.src_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Run full processing\n",
    "            result_html = process_audio_and_classify(event.src_path)\n",
    "            print(\"[Watcher] Processing complete.\")\n",
    "\n",
    "            # Save result to HTML\n",
    "            result_filename = f\"result_{uuid.uuid4().hex}.html\"\n",
    "            result_path = os.path.join(DEBUG_DIR, result_filename)\n",
    "            with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(result_html)\n",
    "            print(f\"[Watcher] Result saved to: {result_path}\")\n",
    "\n",
    "            # Open in browser\n",
    "            webbrowser.open(f\"file://{os.path.abspath(result_path)}\")\n",
    "\n",
    "            # Move original file to processed/\n",
    "            relative_path = os.path.relpath(event.src_path, RECORDER_DIR)\n",
    "            processed_path = os.path.join(PROCESSED_DIR, relative_path)\n",
    "\n",
    "            os.makedirs(os.path.dirname(processed_path), exist_ok=True)\n",
    "            shutil.move(event.src_path, processed_path)\n",
    "            print(f\"[Watcher] Moved file to: {processed_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Watcher] Error processing file: {e}\")\n",
    "\n",
    "def start_file_watcher():\n",
    "    observer = Observer()\n",
    "    event_handler = NewWavHandler()\n",
    "    observer.schedule(event_handler, path=RECORDER_DIR, recursive=True)\n",
    "    observer.start()\n",
    "    print(\"[Watcher] Monitoring started.\")\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n",
    "\n",
    "# Start the watcher in a background thread\n",
    "watcher_thread = threading.Thread(target=start_file_watcher, daemon=True)\n",
    "watcher_thread.start()\n",
    "\n",
    "# ========== Gradio UI ==========\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## PoC: Evaluating Call Center Operator Performance via Call Analysis\")\n",
    "    audio_input = gr.Audio(type=\"filepath\", label=\"Upload WAV audio\")\n",
    "    output_html = gr.HTML()\n",
    "    run_button = gr.Button(\"Process and Classify\")\n",
    "\n",
    "    run_button.click(\n",
    "        fn=process_audio_and_classify,\n",
    "        inputs=audio_input,\n",
    "        outputs=output_html\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "article_text = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=84,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    "    # early_stopping=True  # Optional: stop when EOS token is reached\n",
    "    # length_penalty=\n",
    ")[0]\n",
    "\n",
    "summary = tokenizer.decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For input**: \"*Salam! Xo≈ü g√ºn√ºn√ºz olsun, ‚ÄúX Bank‚Äù m√º≈üt…ôri xidm…ôtin…ô z…ông etdiyiniz √º√ß√ºn t…ô≈ü…ôkk√ºr edirik. M…ôn Aysel…ôm. \n",
    "Siz…ô nec…ô k√∂m…ôk ed…ô bil…ôr…ôm? Salam, Aysel xanƒ±m. M…ôn kartƒ±mƒ±n balansƒ±nƒ± √∂yr…ônm…ôk ist…ôyir…ôm. ∆èlb…ôtt…ô, siz…ô k√∂m…ôk etm…ôk √º√ß√ºn \n",
    "…ôvv…ôlc…ô ≈ü…ôxsiyy…ôtinizi t…ôsdiql…ôm…ôliy…ôm. Z…ôhm…ôt olmasa, adƒ±nƒ±zƒ±, soyadƒ±nƒ±zƒ± v…ô doƒüum tarixinizi qeyd edin. \n",
    "Adƒ±m Kamran M…ômm…ôdov, 12 iyun 1987-ci il. T…ô≈ü…ôkk√ºr edir…ôm, Kamran b…ôy. Sistem…ô baxƒ±ram... \n",
    "B…ôli, sizi tapdƒ±m. Hal-hazƒ±rda kartƒ±nƒ±zƒ±n balansƒ± 524 manat 80 q…ôpik t…ô≈ükil edir. √áox saƒü olun. \n",
    "Bir sualƒ±m da var. Kartƒ±mda h…ôr ay niy…ô 2 manat tutulur? Bu kartƒ±n aylƒ±q xidm…ôt haqqƒ±dƒ±r. \n",
    "∆èg…ôr ist…ôs…ôniz, komissiyasƒ±z kart n√∂v√º il…ô …ôv…ôz ed…ô bil…ôrik. B…ôli, maraqlƒ±dƒ±r. \n",
    "Z…ôhm…ôt olmasa, …ôtraflƒ± m…ôlumat verin. ∆èlb…ôtt…ô! Yeni kartla baƒülƒ± siz…ô m…ôlumat g√∂nd…ôr…ôc…ôyik v…ô filialƒ±mƒ±za yaxƒ±nla≈üaraq \n",
    "d…ôyi≈üiklik ed…ô bil…ôrsiniz. Oldu, t…ô≈ü…ôkk√ºr edir…ôm. Siz…ô k√∂m…ôk ed…ô bildiyim √º√ß√ºn m…ômnunam. G√∂z…ôl g√ºnl…ôr arzulayƒ±ram!*\"\n",
    "\n",
    "**Output is**: \"*X Bankƒ±n m√º≈üt…ôri xidm…ôtin…ô z…ông etdiyiniz √º√ß√ºn t…ô≈ü…ôkk√ºr edirik.*\"\n",
    "\n",
    "**For input**: \"*Rusiya Prezidenti Putin s√ºlh danƒ±≈üƒ±qlarƒ± √º√ß√ºn ƒ∞stanbula getm…ôyib. \n",
    "Onu Ukrayna il…ô danƒ±≈üƒ±qlarda k√∂m…ôk√ßisi t…ômsil edir, Kreml a√ßƒ±qlayƒ±b.\n",
    "Putin ƒ∞stanbula g…ôls…ôydi, AB≈û Prezidenti Donald Trump da bu g√∂r√º≈ü…ô qo≈üulacaƒüƒ±nƒ± demi≈üdi.\n",
    "Mayƒ±n 11-d…ô Putin √∂z√º Zelenski il…ô s√ºlh danƒ±≈üƒ±qlarƒ±na √ßaƒüƒ±rƒ±≈ü etmi≈üdi.\n",
    "Zelenski d…ô ƒ∞stanbulda onu ≈ü…ôxs…ôn g√∂zl…ôy…ôc…ôyini demi≈üdi.*\"\n",
    "\n",
    "**Output is**: \"*Rusiya Prezidenti Vladimir Putin Ukrayna prezidenti Volodimir Zelenski il…ô s√ºlh danƒ±≈üƒ±qlarƒ± √º√ß√ºn ƒ∞stanbula g…ôlm…ôyib.*\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Conclusion**\n",
    "\n",
    "The `csebuetnlp/mT5_multilingual_XLSum` model demonstrates strong baseline performance for Azerbaijani text summarization, especially in the context of news articles, as evidenced by accurate summaries of political news input. However, its performance on domain-specific data‚Äîsuch as bank call center conversations‚Äîshows limitations due to the conversational and transactional nature of the content, which the model wasn't explicitly trained on.\n",
    "\n",
    "This model serves as a **good starting point**, but for effective deployment in the banking customer service domain, **fine-tuning is essential**. A fine-tuning strategy can begin with the **\"LocalDoc/summarization\\_azerbaijan\"** dataset to bridge the domain gap, followed by incorporation of **bank-provided audio recordings**. These recordings can be transcribed using **Whisper-Large-V3**, post-processed and cleaned with a **large language model (LLM)**, **manually reviewed a sample set** to *validate the LLM corrections* and then used to fine-tune the model for **dialogue-based summarization**.\n",
    "\n",
    "These transcriptions can be used in two complementary ways:\n",
    "\n",
    "* **As-is (noisy STT output):** Fine-tune the summarizer to handle real-world, imperfect input, improving robustness.\n",
    "* **After correction (cleaned by LLM):** Fine-tune on clean data for higher precision and clarity in summaries.\n",
    "\n",
    "Alternatively, **Whisper itself can be fine-tuned** (if needed and feasible) on domain-specific audio to produce **higher-quality transcriptions**, reducing (or eliminating) the need for post-processing before summarization fine-tuning.\n",
    "\n",
    "This approach ensures adaptability to both **raw conversational input** and **high-quality cleaned text**, supporting a production-grade summarization system for banking customer interactions in Azerbaijani. This pipeline ensures the summarization model will learn the structure, terminology, and conversational nuances specific to Azerbaijani banking customer service contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 (CUDA GPU)",
   "language": "python",
   "name": "cuda_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
