{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96898e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9367f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_in_data = np.array([int(i.split(\".\")[0]) for i in os.listdir(\"data/data\")])\n",
    "pages_not_in_data = np.array([i for i in range(1, 56557) if i not in pages_in_data])\n",
    "pages = np.array([f\"https://e-qanun.az/framework/{i}\" for i in pages_not_in_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fdaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "def run_selenium_task(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CLASS_NAME, \"WordSection1\")))\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.CLASS_NAME, \"Section1\")))\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "        html = driver.page_source\n",
    "\n",
    "        with open(f\"data/pages/{url.split('/')[-1]}.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html)\n",
    "            file.close()\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        try:\n",
    "            if soup.find(\"div\", class_=\"WordSection1\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"WordSection1\")\n",
    "            elif soup.find(\"div\", class_=\"Section1\").find(\"div\", class_=\"Section1\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section1\").find(\"div\", class_=\"Section1\")\n",
    "            elif soup.find(\"div\", class_=\"Section1\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section1\")\n",
    "            elif soup.find(\"div\", class_=\"WordSection2\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"WordSection2\")\n",
    "            elif soup.find(\"div\", class_=\"Section3\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section3\")\n",
    "            elif soup.find(\"div\", class_=\"Section4\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section4\")\n",
    "            elif soup.find(\"div\", class_=\"WordSection3\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"WordSection3\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        content = []\n",
    "        for _content in content_part:\n",
    "            try:\n",
    "                _class = content.get(\"class\")\n",
    "            except:\n",
    "                _class = None\n",
    "\n",
    "            if _class != \"Bottomima\" and _class != \"BottomNo\" and _class != \"MsoNormal\":\n",
    "                if _content.text not in content:\n",
    "                    content.append(_content.text)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "        data[\"text\"] = content\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x: x.strip())\n",
    "        data[\"len\"] = data[\"text\"].apply(lambda x: len(x))\n",
    "        data = data.loc[data[\"len\"] > 0].reset_index(drop=True)\n",
    "        data[\"document_name\"] = data[\"text\"].loc[0]\n",
    "        data[\"document_type\"] = data[\"text\"].loc[1]\n",
    "\n",
    "        data.to_csv(f\"data/data/{url.split('/')[-1]}.csv\", index=False)\n",
    "\n",
    "        driver.close()\n",
    "        driver.quit()\n",
    "\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"{ex}: {url}\")\n",
    "\n",
    "Parallel(n_jobs=-1)(delayed(run_selenium_task)(url) for url in tqdm(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393aabc3",
   "metadata": {},
   "source": [
    "## Handling Table Content Also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selenium_task(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.CLASS_NAME, \"WordSection1\")))\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.CLASS_NAME, \"Section1\")))\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        # Initialize an empty list to store all content.\n",
    "        content = []\n",
    "\n",
    "        try:\n",
    "            if soup.find(\"div\", class_=\"WordSection1\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"WordSection1\")\n",
    "            elif soup.find(\"div\", class_=\"Section1\").find(\"div\", class_=\"Section1\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section1\").find(\"div\", class_=\"Section1\")\n",
    "            elif soup.find(\"div\", class_=\"Section1\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section1\")\n",
    "            elif soup.find(\"div\", class_=\"WordSection2\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"WordSection2\")\n",
    "            elif soup.find(\"div\", class_=\"Section3\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section3\")\n",
    "            elif soup.find(\"div\", class_=\"Section4\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"Section4\")\n",
    "            elif soup.find(\"div\", class_=\"WordSection3\") is not None:\n",
    "                content_part = soup.find(\"div\", class_=\"WordSection3\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # If 'content_part' exists, find tables within it.\n",
    "        tables = content_part.find_all('table', class_='MsoTableGrid') if content_part else []\n",
    "        for table in tables:\n",
    "            # Process each table as explained previously\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "                content.append(' | '.join(row_data))  # You can customize the delimiter as needed.\n",
    "\n",
    "        # Now process other content that is not part of the tables.\n",
    "        # Note that this part depends on the structure of your HTML and may need adjustments.\n",
    "        non_table_content = [element.get_text(strip=True) for element in content_part.find_all(recursive=False)\n",
    "                             if element.name != 'table']\n",
    "        content.extend(non_table_content)\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "        data[\"text\"] = content\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x: x.strip())\n",
    "        data[\"len\"] = data[\"text\"].apply(lambda x: len(x))\n",
    "        data = data.loc[data[\"len\"] > 0].reset_index(drop=True)\n",
    "        if len(data) >= 2:\n",
    "            data[\"document_name\"] = data[\"text\"].iloc[0]\n",
    "            data[\"document_type\"] = data[\"text\"].iloc[1]\n",
    "        else:\n",
    "            data[\"document_name\"] = data[\"document_type\"] = None\n",
    "    \n",
    "        # Save the DataFrame to CSV\n",
    "        data.to_csv(f\"data/data/{url.split('/')[-1]}.csv\", index=False)\n",
    "\n",
    "        # Close driver and other clean-up\n",
    "        driver.close()\n",
    "        driver.quit()\n",
    "\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"{ex}: {url}\")\n",
    "\n",
    "Parallel(n_jobs=-1)(delayed(run_selenium_task)(url) for url in tqdm(pages))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
